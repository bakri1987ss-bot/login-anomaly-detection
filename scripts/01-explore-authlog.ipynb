{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7799c4e-b5a2-46a1-ac56-73d37e8a161f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/auth_parsed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/auth_parsed.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m display(df.head())\n\u001b[32m      6\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m'\u001b[39m].value_counts()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sec-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sec-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sec-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sec-env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sec-env/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/auth_parsed.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "display(df.head())\n",
    "df['result'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d223cf-b20b-434c-838c-d79cd016cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('timestamp', inplace=True)\n",
    "df.resample('1H')['result'].count().plot(title='Events per hour')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of events')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07a2ae-0ea7-4a37-bbab-76c11dff7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv('data/auth_features.csv', parse_dates=['timestamp'])\n",
    "display(feat.head())\n",
    "feat[['cnt_last_1m','cnt_last_5m','cnt_last_15m']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3af623-3e38-4f3d-9235-24669007f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "feat['cnt_last_5m'].hist(bins=50)\n",
    "plt.title('Distribution of events per IP in last 5 minutes')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8a1b6-f0be-41ad-a70c-5cc80bb9dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# تبسيط: سنعتبر الحدث مشبوه إذا كان event_result == 'failed'\n",
    "feat = feat.dropna(subset=['event_result'])\n",
    "feat['label'] = feat['event_result'].apply(lambda x: 1 if x=='failed' else 0)\n",
    "\n",
    "X = feat[['cnt_last_1m','cnt_last_5m','cnt_last_15m','fail_rate']].values\n",
    "y = feat['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f1c75-24ac-49aa-960e-3409c20838d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'models/logreg_baseline.joblib')\n",
    "print(\"Saved model to models/logreg_baseline.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec301236-de11-44f5-be46-1cc6f9842794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# تحميل البيانات\n",
    "df = pd.read_csv('../data/auth_features.csv')\n",
    "\n",
    "# عرض أول 5 صفوف للتأكد\n",
    "display(df.head())\n",
    "\n",
    "# نفترض أن لدينا عمود target = 1 (مشبوه) أو 0 (عادي)\n",
    "# إذا لم يكن موجودًا، سننشئ عمودًا بسيطًا مؤقتًا\n",
    "if 'target' not in df.columns:\n",
    "    # قاعدة مؤقتة: إذا فشل الدخول أكثر من 3 مرات في الدقيقة = مشبوه\n",
    "    df['target'] = (df['fail_count'] > 3).astype(int)\n",
    "\n",
    "# اختيار الميزات للتدريب\n",
    "features = ['fail_count', 'success_count', 'unique_users', 'hour']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# تقسيم البيانات إلى تدريب واختبار\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# إنشاء النموذج\n",
    "model = LogisticRegression()\n",
    "\n",
    "# تدريب النموذج\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# التقييم\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# حفظ النموذج\n",
    "joblib.dump(model, '../models/logreg_baseline.joblib')\n",
    "print(\"✅ تم حفظ النموذج في ../models/logreg_baseline.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e18ea-ae34-4504-90d0-cfb68c801bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "print(df.isnull().mean())            # نسبة القيم الفارغة لكل عمود\n",
    "print(df['result'].value_counts())   # توازن النتائج\n",
    "feat = pd.read_csv('data/auth_features.csv', parse_dates=['timestamp'])\n",
    "print(feat.describe())               # إحصاءات عددية\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3449c-7a07-4656-b027-1f3320b08c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# قراءة ملفات CSV\n",
    "df = pd.read_csv('data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features.csv', parse_dates=['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e669f4d5-8676-467e-9925-ad721ae200b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# نسبة القيم المفقودة لكل عمود\n",
    "print(\"Missing values in auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nMissing values in auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f07ad-4a44-4fb9-9914-b9a434d1b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# نسبة القيم المفقودة لكل عمود\n",
    "print(\"Missing values in auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nMissing values in auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a1f9d-5177-47c7-9777-fdc00a857981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a0684-8bf5-4784-8b94-ac9cc5305c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# نسبة القيم المفقودة لكل عمود\n",
    "print(\"Missing values in auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nMissing values in auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cca516-4d85-4687-8bd8-ca330454a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features.csv', parse_dates=['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da7c75-36fd-4b3f-9740-18ddb26070e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات وقراءة البيانات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features.csv', parse_dates=['timestamp'])\n",
    "print(\"البيانات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"نسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\nنسبة كل فئة:\")\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص الإحصاءات العددية\n",
    "# ------------------------------\n",
    "print(\"\\nالإحصاءات العددية في auth_features.csv:\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: إزالة الصفوف الغير صالحة\n",
    "# ------------------------------\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "print(\"\\nتم تنظيف البيانات: الصفوف التي تحتوي على timestamp أو ip مفقود تم استبعادها.\")\n",
    "print(f\"auth_parsed: {len(df)} -> {len(df_clean)} صفوف\")\n",
    "print(f\"auth_features: {len(feat)} -> {len(feat_clean)} صفوف\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: فحص القيم المتطرفة (Boxplot سريع)\n",
    "# ------------------------------\n",
    "feat_clean[['duration', 'num_failures']].boxplot()\n",
    "plt.title(\"Boxplot للقيم العددية\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26b8e8-a074-41e0-a938-3b55228a61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات وقراءة البيانات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features.csv', parse_dates=['timestamp'])\n",
    "print(\"البيانات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"نسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\nنسبة كل فئة:\")\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص الإحصاءات العددية\n",
    "# ------------------------------\n",
    "print(\"\\nالإحصاءات العددية في auth_features.csv:\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: إزالة الصفوف الغير صالحة\n",
    "# ------------------------------\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "print(\"\\nتم تنظيف البيانات: الصفوف التي تحتوي على timestamp أو ip مفقود تم استبعادها.\")\n",
    "print(f\"auth_parsed: {len(df)} -> {len(df_clean)} صفوف\")\n",
    "print(f\"auth_features: {len(feat)} -> {len(feat_clean)} صفوف\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: فحص القيم المتطرفة (Boxplot سريع)\n",
    "# ------------------------------\n",
    "feat_clean[['duration', 'num_failures']].boxplot()\n",
    "plt.title(\"Boxplot للقيم العددية\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84a441-302a-488d-9f74-02f668f13a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Bakri/Documents/csv_files/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('C:/Users/Bakri/Documents/csv_files/auth_features.csv', parse_dates=['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fd4a8-09e8-4ff0-bd35-3ae48d2ded4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# مسارات الملفات\n",
    "parsed_path = 'data/auth_parsed.csv'\n",
    "features_path = 'data/auth_features.csv'\n",
    "\n",
    "# التحقق من وجود الملفات\n",
    "if os.path.exists(parsed_path) and os.path.exists(features_path):\n",
    "    # قراءة الملفات\n",
    "    df = pd.read_csv(parsed_path, parse_dates=['timestamp'])\n",
    "    feat = pd.read_csv(features_path, parse_dates=['timestamp'])\n",
    "    print(\"الملفات تم تحميلها بنجاح!\")\n",
    "else:\n",
    "    # إعلام المستخدم بالملفات المفقودة\n",
    "    if not os.path.exists(parsed_path):\n",
    "        print(f\"ملف مفقود: {parsed_path}\")\n",
    "    if not os.path.exists(features_path):\n",
    "        print(f\"ملف مفقود: {features_path}\")\n",
    "    print(\"تأكد من أن الملفات موجودة في المسار الصحيح قبل المتابعة.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a699035-34de-4a0a-98b6-1c4127598737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# عرض الملفات الموجودة داخل مجلد data\n",
    "print(os.listdir('data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72398f57-aed6-4fc3-823c-c2de361ad59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# إظهار المسار الحالي للـ Notebook\n",
    "print(\"المجلد الحالي للـ Notebook:\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a6d56-2158-4661-bb73-e8f5540c0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('../data/auth_features.csv', parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465438e-9f56-4993-9a49-863f1b8b16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات والتأكد من الملفات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار الصحيح من scripts إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات الموجودة داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\nنسبة كل فئة:\")\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: فحص الإحصاءات العددية\n",
    "# ------------------------------\n",
    "print(\"\\nالإحصاءات العددية في auth_features.csv:\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: إزالة الصفوف الغير صالحة\n",
    "# ------------------------------\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "print(\"\\nتم تنظيف البيانات: الصفوف التي تحتوي على timestamp أو ip مفقود تم استبعادها.\")\n",
    "print(f\"auth_parsed: {len(df)} -> {len(df_clean)} صفوف\")\n",
    "print(f\"auth_features: {len(feat)} -> {len(feat_clean)} صفوف\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 7: فحص القيم المتطرفة (Boxplot سريع)\n",
    "# ------------------------------\n",
    "feat_clean[['duration', 'num_failures']].boxplot()\n",
    "plt.title(\"Boxplot للقيم العددية\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a1e9b-4e84-40ed-9e74-0a17ef07ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"أسماء الأعمدة في auth_features.csv:\")\n",
    "print(feat_clean.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3d41c-dee1-4fb1-8a81-82e38e84339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# رسم Boxplot لجميع الأعمدة العددية في auth_features\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "feat_clean[numeric_cols].boxplot(figsize=(10,6))\n",
    "plt.title(\"Boxplot للقيم العددية في auth_features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562eeb2-2755-4326-87c6-1cac3c9f4446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات والتأكد من الملفات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار الصحيح من scripts إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات الموجودة داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['res]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b76ea-a8f1-46a4-860a-1d5570479138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات والتأكد من الملفات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار الصحيح من scripts إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات الموجودة داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result']()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84502f20-9614-4beb-bbe1-829fc97bd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات والتأكد من الملفات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار الصحيح من scripts إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات الموجودة داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result'].value_counts())  # تصحيح الاسم والأقواس\n",
    "\n",
    "print(\"\\nنسبة كل فئة:\")\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: فحص الإحصاءات العددية\n",
    "# ------------------------------\n",
    "print(\"\\nالإحصاءات العددية في auth_features.csv:\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: إزالة الصفوف الغير صالحة\n",
    "# ------------------------------\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "print(\"\\nتم تنظيف البيانات: الصفوف التي تحتوي على timestamp أو ip مفقود تم استبعادها.\")\n",
    "print(f\"auth_parsed: {len(df)} -> {len(df_clean)} صفوف\")\n",
    "print(f\"auth_features: {len(feat)} -> {len(feat_clean)} صفوف\")\n",
    "\n",
    "# -----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866d7b8-b658-4c14-91d4-4b24cc3092b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات والتأكد من الملفات + تفعيل الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline  # لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار الصحيح من scripts إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات الموجودة داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ec00a-7949-4261-a0c5-b0d3dc721a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات والتأكد من الملفات + تفعيل الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline  # لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار الصحيح من scripts إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات الموجودة داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_featur_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91439e55-336c-412f-925a-1e671e9a93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات والتأكد من الملفات + تفعيل الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline  # لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار الصحيح من scripts إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات الموجودة داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result']()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b98c77-9d54-4efb-b205-152940ba5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات والتأكد من الملفات + تفعيل الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline  # لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار الصحيح من scripts إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات الموجودة داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c29f7-4b2f-44bb-92dd-3f16ca28dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات والتأكد من الملفات + تفعيل الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline  # لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار الصحيح من scripts إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات الموجودة داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result'].value_counts())  # صحيح بدون أقواس إضافية\n",
    "\n",
    "print(\"\\nنسبة كل فئة:\")\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: فحص الإحصاءات العددية\n",
    "# ------------------------------\n",
    "print(\"\\nالإحصاءات العددية في auth_features.csv:\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: إزالة الصفوف الغير صالحة\n",
    "# ------------------------------\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "print(\"\\nتم تنظيف البيانات: الصفوف التي تحتوي على timestamp أو ip مفقود تم استبعادها.\")\n",
    "print(f\"auth_parsed: {len(df)} -> {len(df_clean)} صفوف\")\n",
    "print(f\"auth_features: {len(feat)} -> {len(feat_clean)} صفوف\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 7: فحص القيم المتطرفة (Boxplot للأعمدة العددية)\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "# تحقق من وجود الأعمدة فعلياً\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in feat_clean.columns]\n",
    "\n",
    "# رسم Boxplot\n",
    "feat_clean[existing_numeric_cols].boxplot(figsize=(10,6))\n",
    "plt.title(\"Boxplot للقيم العددية في auth_features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45b70f-d0a8-443b-9c22-1638d546964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline  # لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار النسبي إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())  # تصحيح الأقواس\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\nنسبة كل فئة:\")\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: فحص الإحصاءات العددية\n",
    "# ------------------------------\n",
    "print(\"\\nالإحصاءات العددية في auth_features.csv:\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: إزالة الصفوف الغير صالحة\n",
    "# ------------------------------\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean =_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bbe92-4c06-4b59-a83a-d5d569a22f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline\n",
    "# لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار النسبي إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870a069-6a64-48f9-8315-0a5aea076310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline\n",
    "# لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار النسبي إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\nنسبة كل فئة:\")\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: فحص الإحصاءات العددية\n",
    "# ------------------------------\n",
    "print(\"\\nالإحصاءات العددية في auth_features.csv:\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: إزالة الصفوف الغير صالحة\n",
    "# ------------------------------\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "print(\"\\nتم تنظيف البيانات: الصفوف التي تحتوي على timestamp أو ip مفقود تم استبعادها.\")\n",
    "print(f\"auth_parsed: {len(df)} -> {len(df_clean)} صفوف\")\n",
    "print(f\"auth_features: {len(feat)} -> {len(feat_clean)} صفوف\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 7: فحص القيم المتطرفة (Boxplot للأعمدة العددية)\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "# التأكد من وجود الأعمدة قبل الرسم\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in feat_clean.columns]\n",
    "\n",
    "# رسم Boxplot\n",
    "feat_clean[existing_numeric_cols].boxplot(figsize=(10,6))\n",
    "plt.title(\"Boxplot للقيم العددية في auth_features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3fb00-f6aa-4c43-ab18-3972991b389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline\n",
    "# لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار النسبي إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\nنسبة كل فئة:\")\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: فحص الإحصاءات العددية\n",
    "# ------------------------------\n",
    "print(\"\\nالإحصاءات العددية في auth_features.csv:\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: إزالة الصفوف الغير صالحة\n",
    "# ------------------------------\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "print(\"\\nتم تنظيف البيانات: الصفوف التي تحتوي على timestamp أو ip مفقود تم استبعادها.\")\n",
    "print(f\"auth_parsed: {len(df)} -> {len(df_clean)} صفوف\")\n",
    "print(f\"auth_features: {len(feat)} -> {len(feat_clean)} صفوف\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 7: فحص القيم المتطرفة (Boxplot للأعمدة العددية)\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "# التأكد من وجود الأعمدة قبل الرسم\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in feat_clean.columns]\n",
    "\n",
    "# رسم Boxplot\n",
    "feat_clean[existing_numeric_cols].boxplot(figsize=(10,6))\n",
    "plt.title(\"Boxplot للقيم العددية في auth_features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06554a7d-cb04-46a7-b211-13d134cc8dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline\n",
    "# لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# المسار النسبي إلى مجلد data\n",
    "data_path = '../data'\n",
    "\n",
    "# عرض الملفات داخل data للتأكد\n",
    "print(\"الملفات الموجودة في مجلد data:\")\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nنسبة القيم المفقودة في auth_parsed.csv:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nنسبة القيم المفقودة في auth_features.csv:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: فحص توازن الفئات\n",
    "# ------------------------------\n",
    "print(\"\\nتعداد فئات 'result' في auth_parsed.csv:\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\nنسبة كل فئة:\")\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: فحص الإحصاءات العددية\n",
    "# ------------------------------\n",
    "print(\"\\nالإحصاءات العددية في auth_features.csv:\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: إزالة الصفوف الغير صالحة\n",
    "# ------------------------------\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "print(\"\\nتم تنظيف البيانات: الصفوف التي تحتوي على timestamp أو ip مفقود تم استبعادها.\")\n",
    "print(f\"auth_parsed: {len(df)} -> {len(df_clean)} صفوف\")\n",
    "print(f\"auth_features: {len(feat)} -> {len(feat_clean)} صفوف\")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 7: فحص القيم المتطرفة (Boxplot للأعمدة العددية)\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "# التأكد من وجود الأعمدة قبل الرسم\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in feat_clean.columns]\n",
    "\n",
    "# رسم Boxplot\n",
    "feat_clean[existing_numeric_cols].boxplot(figsize=(10,6))\n",
    "plt.title(\"Boxplot للقيم العددية في auth_features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f079d-2e37-4829-aa4e-6366707e7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------\n",
    "# 1️⃣ توقع الاحتمالات على مجموعة الاختبار\n",
    "# --------------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:,1]  # احتمالية فئة failed\n",
    "y_true = y_test\n",
    "\n",
    "# --------------------------------------\n",
    "# 2️⃣ حساب Precision, Recall, F1 مع threshold افتراضي 0.5\n",
    "# --------------------------------------\n",
    "threshold_default = 0.5\n",
    "y_pred = (y_prob >= threshold_default).astype(int)\n",
    "\n",
    "print(\"=== التقارير الافتراضية (threshold=0.5) ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=['success','failed']))\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3️⃣ رسم ROC curve\n",
    "# --------------------------------------\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC={roc_auc:.2f})')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------\n",
    "# 4️⃣ رسم Precision-Recall curve\n",
    "# --------------------------------------\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(rec, prec, label=f'PR curve (AUC={pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------\n",
    "# 5️⃣ ضبط threshold للوصول إلى Recall ≥ 0.8\n",
    "# --------------------------------------\n",
    "target_recall = 0.8\n",
    "\n",
    "# إيجاد أول threshold يحقق الهدف\n",
    "thresholds = np.linspace(0,1,101)\n",
    "best_threshold = 0.5  # افتراضي\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_prob >= t).astype(int)\n",
    "    tp = ((y_true==1) & (y_pred_t==1)).sum()\n",
    "    fn = ((y_true==1) & (y_pred_t==0)).sum()\n",
    "    recall_t = tp / (tp + fn) if (tp+fn)>0 else 0\n",
    "    if recall_t >= target_recall:\n",
    "        best_threshold = t\n",
    "        break\n",
    "\n",
    "print(f\"\\nأفضل threshold لتحقيق Recall ≥ {target_recall}: {best_threshold:.2f}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 6️⃣ إعادة الحساب مع threshold الجديد\n",
    "# --------------------------------------\n",
    "y_pred_best = (y_prob >= best_threshold).astype(int)\n",
    "print(\"\\n=== التقارير بعد ضبط threshold ===\")\n",
    "print(classification_report(y_true, y_pred_best, target_names=['success','failed']))\n",
    "\n",
    "# رسم مصفوفة الالتباس\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred_best, display_labels=['success','failed'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0f027-7756-472d-aa6b-cf82ff642179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------\n",
    "# 1️⃣ توقع الاحتمالات على مجموعة الاختبار\n",
    "# --------------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:,1]  # احتمالية فئة failed\n",
    "y_true = y_test\n",
    "\n",
    "# --------------------------------------\n",
    "# 2️⃣ حساب Precision, Recall, F1 مع threshold افتراضي 0.5\n",
    "# --------------------------------------\n",
    "threshold_default = 0.5\n",
    "y_pred = (y_prob >= threshold_default).astype(int)\n",
    "\n",
    "print(\"=== التقارير الافتراضية (threshold=0.5) ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=['success','failed']))\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3️⃣ رسم ROC curve\n",
    "# --------------------------------------\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC={roc_auc:.2f})')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------\n",
    "# 4️⃣ رسم Precision-Recall curve\n",
    "# --------------------------------------\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(rec, prec, label=f'PR curve (AUC={pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------\n",
    "# 5️⃣ ضبط threshold للوصول إلى Recall ≥ 0.8\n",
    "# --------------------------------------\n",
    "target_recall = 0.8\n",
    "\n",
    "# إيجاد أول threshold يحقق الهدف\n",
    "thresholds = np.linspace(0,1,101)\n",
    "best_threshold = 0.5  # افتراضي\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_prob >= t).astype(int)\n",
    "    tp = ((y_true==1) & (y_pred_t==1)).sum()\n",
    "    fn = ((y_true==1) & (y_pred_t==0)).sum()\n",
    "    recall_t = tp / (tp + fn) if (tp+fn)>0 else 0\n",
    "    if recall_t >= target_recall:\n",
    "        best_threshold = t\n",
    "        break\n",
    "\n",
    "print(f\"\\nأفضل threshold لتحقيق Recall ≥ {target_recall}: {best_threshold:.2f}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 6️⃣ إعادة الحساب مع threshold الجديد\n",
    "# --------------------------------------\n",
    "y_pred_best = (y_prob >= best_threshold).astype(int)\n",
    "print(\"\\n=== التقارير بعد ضبط threshold ===\")\n",
    "print(classification_report(y_true, y_pred_best, target_names=['success','failed']))\n",
    "\n",
    "# رسم مصفوفة الالتباس\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred_best, display_labels=['success','failed'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ceef1c-4892-4349-8ad4-d53cf1f8563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X: كل الأعمدة العددية أو الميزات المختارة\n",
    "# y: العمود المستهدف 'result' (0=success, 1=failed)\n",
    "X = feat_clean[existing_numeric_cols]  # مثال، اختياراتك للميزات\n",
    "y = df_clean['result'].map({'success':0, 'failed':1})  # تحويل إلى 0/1\n",
    "\n",
    "# تقسيم البيانات\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab54f8-861c-448e-a72f-d61a73da98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = '../data'\n",
    "\n",
    "# قراءة الملفات\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "\n",
    "# تنظيف البيانات: إزالة الصفوف المفقودة في timestamp أو ip\n",
    "df_clean = df.dropna(subset=['timestamp', 'ip'])\n",
    "feat_clean = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "# اختيار الأعمدة العددية الموجودة\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in feat_clean.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0ec72-2ad2-433b-8242-5ca5b4fc0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.shape)\n",
    "print(feat_clean.shape)\n",
    "print(existing_numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75bf65a-0c6e-4d66-b43b-da38b717fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X: كل الأعمدة العددية أو الميزات المختارة\n",
    "X = feat_clean[existing_numeric_cols]\n",
    "\n",
    "# y: العمود المستهدف 'result' من df_clean، تحويل success=0, failed=1\n",
    "y = df_clean['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "# تقسيم البيانات\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90858f-b664-4441-91f6-da2737d94eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# دمج البيانات حسب 'timestamp' و 'ip' أو أي عمود مشترك\n",
    "data_merged = pd.merge(df_clean, feat_clean, on=['timestamp', 'ip'], how='inner')\n",
    "\n",
    "# الآن الميزات X والأهداف y من نفس DataFrame\n",
    "X = data_merged[existing_numeric_cols]  # الميزات العددية\n",
    "y = data_merged['result'].map({'success':0, 'failed':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f6cb4-f181-46a7-bc96-9348a502bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab108e-4e22-4f31-a980-bb6da8f16571",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.shape)print(df_clean.shape)\n",
    "print(feat_clean.shape)\n",
    "print(feat_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38db7d5-7a19-4b11-bcf3-dda0f031b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.shape)\n",
    "print(feat_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8e4af-95a5-4fb8-9b52-6a34ed48ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline\n",
    "# لتفعيل عرض الرسومات داخل Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, ConfusionMatrixDisplay, roc_curve\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: تحميل البيانات\n",
    "# ------------------------------\n",
    "data_path = '../data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "print(\"أبعاد df:\", df.shape)\n",
    "print(\"أبعاد feat:\", feat.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: تنظيف البيانات (لا نفقد الصفوف)\n",
    "# ------------------------------\n",
    "df_clean = df.copy()\n",
    "feat_clean = feat.copy()\n",
    "\n",
    "# عرض الأعمدة العددية الموجودة\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in feat_clean.columns]\n",
    "print(\"الأعمدة العد\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c35d7-2d10-4124-a7eb-3f101a1d8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, ConfusionMatrixDisplay, roc_curve\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: تحميل البيانات\n",
    "# ------------------------------\n",
    "data_path = '../data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "print(\"أبعاد df:\", df.shape)\n",
    "print(\"أبعاد feat:\", feat.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: تنظيف البيانات (لا نفقد الصفوف)\n",
    "# ------------------------------\n",
    "df_clean = df.copy()\n",
    "feat_clean = feat.copy()\n",
    "\n",
    "# عرض الأعمدة العددية الموجودة\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in feat_clean.columns]\n",
    "print(\"الأعمدة العددية المستخدمة:\", existing_numeric_cols)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: دمج البيانات مع الحفاظ على أكبر عدد ممكن من الصفوف\n",
    "# ------------------------------\n",
    "data_merged = pd.merge(df_clean, feat_clean, on=['timestamp', 'ip'], how='left')\n",
    "\n",
    "# ملء القيم المفقودة للأعمدة العددية بـ 0\n",
    "for col in existing_numeric_cols:\n",
    "    data_merged[col] = data_merged[col].fillna(0)\n",
    "\n",
    "print(\"أبعاد البيانات بعد الدمج:\", data_merged.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: بناء X و y\n",
    "# ------------------------------\n",
    "X = data_merged[existing_numeric_cols]\n",
    "y = data_merged['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 7: تدريب موديل RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 8: تقييم الموديل\n",
    "# ------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "y_true = y_test\n",
    "\n",
    "# Threshold افتراضي 0.5\n",
    "threshold_default = 0.5\n",
    "y_pred = (y_prob >= threshold_default).astype(int)\n",
    "\n",
    "print(\"=== التقارير الافتراضية (threshold=0.5) ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=['success','failed']))\n",
    "\n",
    "roc_auc = roc_auc_score(y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5f68e-c86c-4a43-b893-f33991b5f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, ConfusionMatrixDisplay, roc_curve\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: تحميل البيانات\n",
    "# ------------------------------\n",
    "data_path = '../data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "print(\"أبعاد df:\", df.shape)\n",
    "print(\"أبعاد feat:\", feat.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: تنظيف البيانات (لا نفقد الصفوف)\n",
    "# ------------------------------\n",
    "df_clean = df.copy()\n",
    "feat_clean = feat.copy()\n",
    "\n",
    "# عرض الأعمدة العددية الموجودة\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09665d1d-87b6-4017-a3ee-f6a0541f1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, ConfusionMatrixDisplay, roc_curve\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: تحميل البيانات\n",
    "# ------------------------------\n",
    "data_path = '../data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "print(\"أبعاد df:\", df.shape)\n",
    "print(\"أبعاد feat:\", feat.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: تنظيف البيانات (لا نفقد الصفوف)\n",
    "# ------------------------------\n",
    "df_clean = df.copy()\n",
    "feat_clean = feat.copy()\n",
    "\n",
    "# عرض الأعمدة العددية الموجودة\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in feat_clean.columns]\n",
    "print(\"الأعمدة العددية المستخدمة:\", existing_numeric_cols)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: دمج البيانات مع الحفاظ على أكبر عدد ممكن من الصفوف\n",
    "# ------------------------------\n",
    "data_merged = pd.merge(df_clean, feat_clean, on=['timestamp', 'ip'], how='left')\n",
    "\n",
    "# ملء القيم المفقودة للأعمدة العددية بـ 0\n",
    "for col in existing_numeric_cols:\n",
    "    data_merged[col] = data_merged[col].fillna(0)\n",
    "\n",
    "print(\"أبعاد البيانات بعد الدمج:\", data_merged.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: بناء X و y مع إزالة أي NaN في y\n",
    "# ------------------------------\n",
    "data_final = data_merged.dropna(subset=['result'])  # حذف الصفوف التي فيها y مفقود\n",
    "\n",
    "X = data_final[existing_numeric_cols]\n",
    "y = data_final['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "print(\"X shape بعد إزالة NaN في y:\", X.shape)\n",
    "print(\"y shape بعد إزالة NaN في y:\", y.shape)\n",
    "print(\"عدد الصفوف لكل فئة:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 7: تدريب موديل RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 8: تقييم الموديل\n",
    "# ------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "y_true = y_test\n",
    "\n",
    "# Threshold افتراضي 0.5\n",
    "threshold_default = 0.5\n",
    "y_pred = (y_prob >= threshold_default).astype(int)\n",
    "\n",
    "print(\"=== التقارير الافتراضية (threshold=0.5) ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=['success','failed']))\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# رسم ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC={roc_auc:.2f})')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# رسم Precision-Recall curve\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(rec, prec, label=f'PR curve (AUC={pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 9: ضبط threshold للوصول إلى Recall ≥ 0.8\n",
    "# ------------------------------\n",
    "target_recall = 0.8\n",
    "thresholds = np.linspace(0,1,101)\n",
    "best_threshold = 0.5\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_prob >= t).astype(int)\n",
    "    tp = ((y_true==1) & (y_pred_t==1)).sum()\n",
    "    fn = ((y_true==1) & (y_pred_t==0)).sum()\n",
    "    recall_t = tp / (tp + fn) if (tp+fn)>0 else 0\n",
    "    if recall_t >= target_recall:\n",
    "        best_threshold = t\n",
    "        break\n",
    "\n",
    "print(f\"\\nأفضل threshold لتحقيق Recall ≥ {target_recall}: {best_threshold:.2f}\")\n",
    "\n",
    "y_pred_best = (y_prob >= best_threshold).astype(int)\n",
    "print(\"\\n=== التقارير بعد ضبط threshold ===\")\n",
    "print(classification_report(y_true, y_pred_best, target_names=['success','failed']))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred_best, display_labels=['success','failed'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5bba8-6295-4667-8de9-6916d3fe87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# خلية 1: استيراد المكتبات + تفعيل عرض الرسومات\n",
    "# ------------------------------\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, ConfusionMatrixDisplay, roc_curve\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 2: تحميل البيانات\n",
    "# ------------------------------\n",
    "data_path = '../data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_path, 'auth_parsed.csv'), parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(os.path.join(data_path, 'auth_features.csv'), parse_dates=['timestamp'])\n",
    "\n",
    "print(\"الملفات تم تحميلها بنجاح!\")\n",
    "print(\"أبعاد df:\", df.shape)\n",
    "print(\"أبعاد feat:\", feat.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 3: تنظيف البيانات\n",
    "# ------------------------------\n",
    "df_clean = df.copy()\n",
    "feat_clean = feat.copy()\n",
    "\n",
    "# الأعمدة العددية\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', \n",
    "                'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in feat_clean.columns]\n",
    "print(\"الأعمدة العددية المستخدمة:\", existing_numeric_cols)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 4: دمج البيانات\n",
    "# ------------------------------\n",
    "data_merged = pd.merge(df_clean, feat_clean, on=['timestamp', 'ip'], how='left')\n",
    "\n",
    "# ملء القيم المفقودة للأعمدة العددية بـ 0\n",
    "for col in existing_numeric_cols:\n",
    "    data_merged[col] = data_merged[col].fillna(0)\n",
    "\n",
    "print(\"أبعاد البيانات بعد الدمج:\", data_merged.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 5: تنظيف العمود result\n",
    "# ------------------------------\n",
    "# تحويل القيم إلى نصوص وإزالة الفراغات\n",
    "data_merged['result'] = data_merged['result'].astype(str).str.strip()\n",
    "\n",
    "# الاحتفاظ بالقيم الصحيحة فقط\n",
    "data_merged = data_merged[data_merged['result'].isin(['success','failed'])]\n",
    "\n",
    "# التحقق بعد التنظيف\n",
    "print(\"عدد الصفوف بعد تنظيف result:\", len(data_merged))\n",
    "print(\"القيم الفريدة في result:\", data_merged['result'].unique())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 6: بناء X و y\n",
    "# ------------------------------\n",
    "X = data_merged[existing_numeric_cols]\n",
    "y = data_merged['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "print(\"عدد القيم الفارغة في y:\", y.isnull().sum())\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"عدد الصفوف لكل فئة:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 7: تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 8: تدريب موديل RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 9: تقييم الموديل\n",
    "# ------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "y_true = y_test\n",
    "\n",
    "# Threshold افتراضي 0.5\n",
    "threshold_default = 0.5\n",
    "y_pred = (y_prob >= threshold_default).astype(int)\n",
    "\n",
    "print(\"=== التقارير الافتراضية (threshold=0.5) ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=['success','failed']))\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# رسم ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC={roc_auc:.2f})')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# رسم Precision-Recall curve\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(rec, prec, label=f'PR curve (AUC={pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# خلية 10: ضبط threshold للوصول إلى Recall ≥ 0.8\n",
    "# ------------------------------\n",
    "target_recall = 0.8\n",
    "thresholds = np.linspace(0,1,101)\n",
    "best_threshold = 0.5\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_prob >= t).astype(int)\n",
    "    tp = ((y_true==1) & (y_pred_t==1)).sum()\n",
    "    fn = ((y_true==1) & (y_pred_t==0)).sum()\n",
    "    recall_t = tp / (tp + fn) if (tp+fn)>0 else 0\n",
    "    if recall_t >= target_recall:\n",
    "        best_threshold = t\n",
    "        break\n",
    "\n",
    "print(f\"\\nأفضل threshold لتحقيق Recall ≥ {target_recall}: {best_threshold:.2f}\")\n",
    "\n",
    "y_pred_best = (y_prob >= best_threshold).astype(int)\n",
    "print(\"\\n=== التقارير بعد ضبط threshold ===\")\n",
    "print(classification_report(y_true, y_pred_best, target_names=['success','failed']))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred_best, display_labels=['success','failed'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcafa80e-4ed0-4c6c-9d39-1ab1796fb422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# عدد الصفوف الاصطناعية\n",
    "n_rows = 1000\n",
    "\n",
    "# إنشاء timestamps متسلسلة\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='T')\n",
    "\n",
    "# إنشاء بيانات auth_parsed\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "\n",
    "# حفظ الملف\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "print(\"تم إنشاء auth_parsed_large.csv بعدد صفوف:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b578e-0fe7-45f4-8003-45d8fd6b4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# عدد الصفوف الاصطناعية\n",
    "n_rows = 1000\n",
    "\n",
    "# إنشاء timestamps متسلسلة\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='T')\n",
    "\n",
    "# إنشاء بيانات auth_parsed\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "\n",
    "# حفظ الملف\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "print(\"تم إنشاء auth_parsed_large.csv بعدد صفوف:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd37d4f-0caf-4bc1-b5e7-5142198b651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# التأكد من وجود المجلد data، إذا لم يكن موجودًا، إنشاءه\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print(\"تم التأكد من وجود مجلد data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708027d-8f7d-4252-89f9-2109050e38d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "feat.to_csv('data/auth_features_large.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf7a88-e7ac-47a8-8f21-df22c11f5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# عدد الصفوف الاصطناعية\n",
    "n_rows = 1000\n",
    "\n",
    "# إنشاء timestamps متسلسلة\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='T')\n",
    "\n",
    "# إنشاء بيانات auth_parsed\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "\n",
    "# حفظ الملف\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "print(\"تم إنشاء auth_parsed_large.csv بعدد صفوف:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61154812-c1ee-431c-ae2a-28c0db08dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ إنشاء مجلد data إذا لم يكن موجود\n",
    "# ------------------------------\n",
    "os.makedirs('data', exist_ok=True)\n",
    "print(\"✅ تم التأكد من وجود مجلد data\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ إعداد البيانات الاصطناعية\n",
    "# ------------------------------\n",
    "np.random.seed(42)\n",
    "n_rows = 1000  # عدد الصفوف\n",
    "\n",
    "# timestamps بالدقائق\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "# بيانات auth_parsed\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "\n",
    "# حفظ auth_parsed_large.csv\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "print(\"✅ تم إنشاء auth_parsed_large.csv بعدد صفوف:\", len(df))\n",
    "\n",
    "# بيانات auth_features\n",
    "feat = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': df['ip'],\n",
    "    'cnt_last_1m': np.random.randint(0,10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0,50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0,150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0,10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0,5, size=n_rows),\n",
    "    'total_count': np.random.randint(1,15, size=n_rows),\n",
    "    'fail_rate': np.random.rand(n_rows),\n",
    "    'event_user': df['user'],\n",
    "    'event_result': df['result']\n",
    "})\n",
    "\n",
    "# حفظ auth_features_large.csv\n",
    "feat.to_csv('data/auth_features_large.csv', index=False)\n",
    "print(\"✅ تم إنشاء auth_features_large.csv بعدد صفوف:\", len(feat))\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ قراءة الملفات الجديدة للتأكد\n",
    "# ------------------------------\n",
    "df_check = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat_check = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "print(\"\\nSample auth_parsed:\")\n",
    "print(df_check.head())\n",
    "\n",
    "print(\"\\nSample auth_features:\")\n",
    "print(feat_check.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00d83b-58f8-4467-b663-7b738e61a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# اختيار الأعمدة العددية من auth_features_large.csv\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat_check[numeric_cols]\n",
    "\n",
    "# العمود المستهدف: result (0=success, 1=failed)\n",
    "y = df_check['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y value counts:\\n\", y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871456b-0dbd-4051-a56a-3af267bdf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0], \"Test size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f043791-682f-4f01-b47c-63cefb9ef437",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862fe0f-c99a-443d-9775-718e0105fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# احتمالات فئة failed\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# تقرير classification\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC\n",
    "roc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC:\", roc)\n",
    "\n",
    "# PR-AUC\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(\"PR AUC:\", pr_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa94b2-1697-4ffe-b709-d18318ff7377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_rows = 5000  # عدد الصفوف\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='T')\n",
    "\n",
    "df_large = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['10.0.0.1','10.0.0.2','192.168.0.1','192.168.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.7,0.3])  # 30% failed\n",
    "})\n",
    "\n",
    "df_large.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "print(\"تم إنشاء auth_parsed_large.csv بعدد صفوف:\", len(df_large))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da622655-264f-487b-b9a7-0186ecb1219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_rows = 5000  # عدد الصفوف\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')  # استخدم 'min' بدل 'T'\n",
    "\n",
    "# --------------------------\n",
    "# إنشاء auth_parsed_large.csv\n",
    "# --------------------------\n",
    "df_large = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['10.0.0.1','10.0.0.2','192.168.0.1','192.168.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.7,0.3])  # 30% failed\n",
    "})\n",
    "df_large.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "print(\"✅ تم إنشاء auth_parsed_large.csv بعدد صفوف:\", len(df_large))\n",
    "\n",
    "# --------------------------\n",
    "# إنشاء auth_features_large.csv\n",
    "# --------------------------\n",
    "feat_large = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['10.0.0.1','10.0.0.2','192.168.0.1','192.168.0.2'], size=n_rows),\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d34ac-8bb2-4e19-9997-18eb757cd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_rows = 5000  # عدد الصفوف\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')  # استخدم 'min' بدل 'T'\n",
    "\n",
    "# --------------------------\n",
    "# إنشاء auth_parsed_large.csv\n",
    "# --------------------------\n",
    "df_large = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['10.0.0.1','10.0.0.2','192.168.0.1','192.168.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.7,0.3])\n",
    "})\n",
    "df_large.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "print(\"✅ تم إنشاء auth_parsed_large.csv بعدد صفوف:\", len(df_large))\n",
    "\n",
    "# --------------------------\n",
    "# إنشاء auth_features_large.csv\n",
    "# --------------------------\n",
    "feat_large = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['10.0.0.1','10.0.0.2','192.168.0.1','192.168.0.2'], size=n_rows),\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n",
    "    'total_count': np.random.randint(1, 15, size=n_rows),\n",
    "    'fail_rate': np.random.rand(n_rows),\n",
    "    'event_user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'event_result': np.random.choice(['success','failed'], size=n_rows)\n",
    "})\n",
    "feat_large.to_csv('data/auth_features_large.csv', index=False)\n",
    "print(\"✅ تم إنشاء auth_features_large.csv بعدد صفوف:\", len(feat_large))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e28c64-376c-42e3-8a11-da9e308f9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# قراءة الملفات الكبيرة\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "print(\"أبعاد auth_parsed:\", df.shape)\n",
    "print(\"أبعاد auth_features:\", feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b778c517-6749-4ee0-a554-75289679f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# إزالة الصفوف التي بها قيم مفقودة\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "\n",
    "# تحويل العمود المستهدف إلى 0/1\n",
    "df_clean['result_bin'] = df_clean['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "print(\"عدد الصفوف بعد التنظيف:\", len(df_clean))\n",
    "print(df_clean['result_bin'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174a2a4-cb7d-44e8-b4c5-d25eb55728d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# نختار الأعمدة العددية من auth_features\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "\n",
    "# العمود المستهدف\n",
    "y = df_clean['result_bin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841307e9-7fab-439a-b994-b7c7740e3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405748be-ad17-4bee-814d-0c99262359d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca63de48-18b5-40f6-8793-7a76ce6e58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"PR AUC:\", pr_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abca6b5-b4e5-4011-8dae-3203200ef9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ استيراد المكتبات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "\n",
    "# لتفعيل عرض الرسومات داخل Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "print(\"أبعاد auth_parsed:\", df.shape)\n",
    "print(\"أبعاد auth_features:\", feat.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تنظيف البيانات\n",
    "# ------------------------------\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "\n",
    "# تحويل العمود المستهدف إلى 0/1\n",
    "df_clean['result_bin'] = df_clean['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "print(\"عدد الصفوف بعد التنظيف:\", len(df_clean))\n",
    "print(df_clean['result_bin'].value_counts())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ اختيار الميزات والأهداف\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "y = df_clean['result_bin']\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ تقسيم البيانات إلى تدريب واختبار\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تدريب نموذج RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ تقييم الأداء\n",
    "# ------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]  # احتمالية فئة failed\n",
    "\n",
    "# تقرير Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# حساب ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# حساب Precision-Recall AUC\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ رسم منحنى ROC و Precision-Recall\n",
    "# ------------------------------\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC curve\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.2f}')\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall curve\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(rec, prec, label=f'PR AUC = {pr_auc:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01df692-5800-49ef-a98e-b1e5c8619494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ استيراد المكتبات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ قراءة الملفات\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تنظيف البيانات\n",
    "# ------------------------------\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "df_clean['result_bin'] = df_clean['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ اختيار الميزات والأهداف\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "y = df_clean['result_bin']\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تدريب نموذج RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ توقع الاحتمالات\n",
    "# ------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:,1]  # احتمالية فئة failed\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ اختيار threshold تلقائيًا\n",
    "# ------------------------------\n",
    "def select_threshold(y_true, y_prob, min_recall=0.8, min_precision=0.3):\n",
    "    prec, rec, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    # اختر أول threshold يحقق الشروط\n",
    "    candidates = np.where((rec >= min_recall) & (prec >= min_precision))[0]\n",
    "    if len(candidates) > 0:\n",
    "        return thresholds[candidates[0]]\n",
    "    # إذا لم نجد threshold يلبي الشرط، اختر أعلى recall ممكن\n",
    "    best_idx = np.argmax(rec)\n",
    "    return thresholds[best_idx]\n",
    "\n",
    "best_threshold = select_threshold(y_test, y_prob)\n",
    "print(f\"Threshold تلقائي محسّن: {best_threshold:.2f}\")\n",
    "\n",
    "# تطبيق threshold\n",
    "y_pred_thresh = (y_prob >= best_threshold).astype(int)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ تقييم الأداء\n",
    "# ------------------------------\n",
    "print(\"\\nClassification Report (باستخدام threshold محسّن):\")\n",
    "print(classification_report(y_test, y_pred_thresh))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ رسم المنحنيات\n",
    "# ------------------------------\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.2f}')\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(rec, prec, label=f'PR AUC = {pr_auc:.2f}')\n",
    "plt.axhline(0.3, color='red', linestyle='--', label='Precision ≥ 0.3')\n",
    "plt.axvline(0.8, color='green', linestyle='--', label='Recall ≥ 0.8')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6af47-749b-4a42-b68e-adf83b3d0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 11️⃣ دالة للتنبؤ على أي بيانات جديدة\n",
    "# ------------------------------\n",
    "def predict_new_data(new_feat_df, model, threshold, numeric_cols):\n",
    "    \"\"\"\n",
    "    new_feat_df: بيانات Features جديدة (DataFrame)\n",
    "    model: النموذج المدرب\n",
    "    threshold: threshold محسّن\n",
    "    numeric_cols: أسماء الأعمدة العددية المستخدمة في التدريب\n",
    "    \"\"\"\n",
    "    # تنظيف البيانات الجديدة\n",
    "    new_feat_df_clean = new_feat_df.dropna(subset=numeric_cols)\n",
    "    \n",
    "    # استخراج الميزات العددية\n",
    "    X_new = new_feat_df_clean[numeric_cols]\n",
    "    \n",
    "    # التنبؤ بالاحتمالات\n",
    "    y_prob_new = model.predict_proba(X_new)[:,1]\n",
    "    \n",
    "    # تطبيق threshold\n",
    "    y_pred_new = (y_prob_new >= threshold).astype(int)\n",
    "    \n",
    "    # إضافة الأعمدة للـ DataFrame\n",
    "    new_feat_df_clean['pred_failed_prob'] = y_prob_new\n",
    "    new_feat_df_clean['pred_failed'] = y_pred_new\n",
    "    \n",
    "    return new_feat_df_clean\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ مثال على استخدام الدالة\n",
    "# ------------------------------\n",
    "# نفترض أن لدينا بيانات Features جديدة باسم new_features.csv\n",
    "new_feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "predicted_df = predict_new_data(new_feat, model, best_threshold, numeric_cols)\n",
    "\n",
    "# عرض بعض الصفوف للتأكد\n",
    "predicted_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd518d-c60b-4472-a3fe-9739b76b530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "def evaluate_new_data(predicted_df, true_col='event_result', pred_col='pred_failed', prob_col='pred_failed_prob'):\n",
    "    \"\"\"\n",
    "    predicted_df: DataFrame بعد التنبؤ\n",
    "    true_col: اسم العمود الحقيقي (0=success, 1=failed أو 'success'/'failed')\n",
    "    pred_col: اسم العمود المقدر من الدالة السابقة\n",
    "    prob_col: عمود الاحتمالية\n",
    "    \"\"\"\n",
    "    # التأكد من وجود العمود الحقيقي\n",
    "    if true_col not in predicted_df.columns:\n",
    "        print(f\"لا يوجد العمود الحقيقي '{true_col}' في البيانات الجديدة، لا يمكن حساب الأداء.\")\n",
    "        return\n",
    "    \n",
    "    # تحويل القيم إلى 0/1\n",
    "    y_true = predicted_df[true_col].map({'success':0, 'failed':1})\n",
    "    y_pred = predicted_df[pred_col]\n",
    "    y_prob = predicted_df[prob_col]\n",
    "    \n",
    "    # تقرير Classification\n",
    "    print(\"------ Classification Report ------\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    \n",
    "    # ROC AUC\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"ROC AUC: {roc:.3f}\")\n",
    "    \n",
    "    # PR AUC\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(f\"PR AUC: {pr_auc:.3f}\")\n",
    "    \n",
    "    return roc, pr_auc\n",
    "\n",
    "# ------------------------------\n",
    "# مثال على الاستخدام\n",
    "# ------------------------------\n",
    "roc, pr_auc = evaluate_new_data(predicted_df, true_col='event_result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2addc6b-93ec-43a4-a6ad-76cb5fb03930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# ✅ Jupyter Notebook: كامل من الصفر حتى التقييم مع الرسومات\n",
    "# ======================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, RocCurveDisplay, PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ إنشاء مجلد البيانات إذا لم يكن موجودًا\n",
    "# ------------------------------\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ إنشاء بيانات auth_parsed_large.csv و auth_features_large.csv\n",
    "# ------------------------------\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "# auth_parsed\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "\n",
    "# auth_features\n",
    "feat = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 100, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n",
    "    'total_count': np.random.randint(1, 15, size=n_rows),\n",
    "    'fail_rate': np.random.rand(n_rows),\n",
    "    'event_user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'event_result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "feat.to_csv('data/auth_features_large.csv', index=False)\n",
    "\n",
    "print(\"✅ الملفات تم إنشاؤها بنجاح:\")\n",
    "print(\"auth_parsed_large.csv:\", df.shape)\n",
    "print(\"auth_features_large.csv:\", feat.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تحميل البيانات\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ تنظيف البيانات\n",
    "# ------------------------------\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ اختيار الميزات والهدف\n",
    "# ------------------------------\n",
    "y = df_clean['result'].map({'success':0, 'failed':1})\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', 'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ تدريب نموذج RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ التنبؤ\n",
    "# ------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]  # احتمالية فئة failed\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ التقييم\n",
    "# ------------------------------\n",
    "print(\"\\n------ Classification Report ------\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "roc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC AUC: {roc:.3f}\")\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(f\"PR AUC: {pr_auc:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 🔹 رسم ROC Curve و Precision-Recall Curve\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1,2,1)\n",
    "RocCurveDisplay.from_predictions(y_test, y_prob)\n",
    "plt.title(\"ROC Curve\")\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1,2,2)\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_prob)\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51aaf6-dd97-4368-ac43-16bcb83f8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# ✅ كامل الكود مع اختيار Threshold لتحسين Recall لفئة failed\n",
    "# ======================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, RocCurveDisplay, PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ إنشاء مجلد البيانات إذا لم يكن موجودًا\n",
    "# ------------------------------\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ إنشاء بيانات auth_parsed_large.csv و auth_features_large.csv\n",
    "# ------------------------------\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "# auth_parsed\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "\n",
    "# auth_features\n",
    "feat = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 100, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n",
    "    'total_count': np.random.randint(1, 15, size=n_rows),\n",
    "    'fail_rate': np.random.rand(n_rows),\n",
    "    'event_user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'event_result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "feat.to_csv('data/auth_features_large.csv', index=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تحميل البيانات\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ تنظيف البيانات\n",
    "# ------------------------------\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ اختيار الميزات والهدف\n",
    "# ------------------------------\n",
    "y = df_clean['result'].map({'success':0, 'failed':1})\n",
    "numeric_cols = ['cnt_last_1m', 'cnt_last_5m', 'cnt_last_15m', 'succ_count', 'fail_count', 'total_count', 'fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ تدريب نموذج RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ التنبؤ بالاحتمالات\n",
    "# ------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:,1]  # احتمالية فئة failed\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ اختيار Threshold لتحسين Recall لفئة failed\n",
    "# ------------------------------\n",
    "prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# هدفنا: recall >= 0.8\n",
    "desired_recall = 0.8\n",
    "# اختر أصغر threshold يحقق هذا الـ recall\n",
    "threshold_opt = thresholds[np.argmax(rec >= desired_recall)]\n",
    "print(f\"Threshold الأمثل لتحقيق recall ≥ {desired_recall}: {threshold_opt:.3f}\")\n",
    "\n",
    "# تطبيق الـ threshold الجديد\n",
    "y_pred_opt = (y_prob >= threshold_opt).astype(int)\n",
    "\n",
    "# ------------------------------\n",
    "# 🔹 التقييم بعد تعديل Threshold\n",
    "# ------------------------------\n",
    "print(\"\\n------ Classification Report (بعد ضبط Threshold) ------\")\n",
    "print(classification_report(y_test, y_pred_opt, zero_division=0))\n",
    "\n",
    "roc = roc_auc_score(y_test, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "print(f\"ROC AUC: {roc:.3f}\")\n",
    "print(f\"PR AUC: {pr_auc:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 🔹 رسم ROC Curve و Precision-Recall Curve\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1,2,1)\n",
    "RocCurveDisplay.from_predictions(y_test, y_prob)\n",
    "plt.title(\"ROC Curve\")\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1,2,2)\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_prob)\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c64024-6af9-40d4-94fd-aa2ea1048a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# 🔹 مكتبات\n",
    "# ----------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 0️⃣ توليد بيانات وهمية كبيرة\n",
    "# ----------------------------------------------\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "\n",
    "feat = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': df['ip'],\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n",
    "    'total_count': np.random.randint(1, 15, size=n_rows),\n",
    "    'fail_rate': np.random.rand(n_rows),\n",
    "    'event_user': df['user'],\n",
    "    'event_result': df['result']\n",
    "})\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1️⃣ حفظ البيانات (اختياري)\n",
    "# ----------------------------------------------\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "feat.to_csv('data/auth_features_large.csv', index=False)\n",
    "print(\"✔️ تم إنشاء الملفات وهمية كبيرة\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2️⃣ تنظيف البيانات\n",
    "# ----------------------------------------------\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 3️⃣ إعداد X و y\n",
    "# ----------------------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "y = df_clean['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "print(\"\\nتوزيع الهدف y:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 4️⃣ تقسيم البيانات\n",
    "# ----------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5️⃣ تدريب RandomForest\n",
    "# ----------------------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 6️⃣ توقع الاحتمالات\n",
    "# ----------------------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 7️⃣ اختيار Threshold تلقائي لتحقيق Recall ≥ 0.8 و Precision ≥ 0.3\n",
    "# ----------------------------------------------\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# إيجاد أفضل threshold\n",
    "best_threshold = 0.5\n",
    "for p, r, t in zip(precisions, recalls, np.append(thresholds,1.0)):\n",
    "    if r >= 0.8 and p >= 0.3:\n",
    "        best_threshold = t\n",
    "        break\n",
    "\n",
    "y_pred_custom = (y_prob >= best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✔️ أفضل Threshold: {best_threshold:.3f}\")\n",
    "print(\"\\n------ Classification Report (custom threshold) ------\")\n",
    "print(classification_report(y_test, y_pred_custom, zero_division=0))\n",
    "\n",
    "roc = roc_auc_score(y_test, y_prob)\n",
    "pr_auc = auc(recalls, precisions)\n",
    "print(f\"ROC-AUC: {roc:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 8️⃣ Feature Importance\n",
    "# ----------------------------------------------\n",
    "importances = model.feature_importances_\n",
    "feat_imp_df = pd.DataFrame({'feature': numeric_cols, 'importance': importances})\n",
    "feat_imp_df = feat_imp_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(feat_imp_df['feature'], feat_imp_df['importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance - RandomForest\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 9️⃣ Precision-Recall Curve\n",
    "# ----------------------------------------------\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(recalls, precisions, label=f'PR Curve (AUC={pr_auc:.4f})')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb267a15-f8d4-4e74-b217-8772be9ac8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# 🔹 مكتبات\n",
    "# ----------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score\n",
    "import joblib  # لحفظ وتحميل الموديل\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 0️⃣ توليد بيانات وهمية كبيرة\n",
    "# ----------------------------------------------\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "\n",
    "feat = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': df['ip'],\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n",
    "    'total_count': np.random.randint(1, 15, size=n_rows),\n",
    "    'fail_rate': np.random.rand(n_rows),\n",
    "    'event_user': df['user'],\n",
    "    'event_result': df['result']\n",
    "})\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1️⃣ حفظ البيانات (اختياري)\n",
    "# ----------------------------------------------\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "feat.to_csv('data/auth_features_large.csv', index=False)\n",
    "print(\"✔️ تم إنشاء الملفات وهمية كبيرة\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2️⃣ تنظيف البيانات\n",
    "# ----------------------------------------------\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 3️⃣ إعداد X و y\n",
    "# ----------------------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "y = df_clean['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "print(\"\\nتوزيع الهدف y:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 4️⃣ تقسيم البيانات\n",
    "# ----------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5️⃣ تدريب RandomForest\n",
    "# ----------------------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 6️⃣ حفظ النموذج المدرب\n",
    "# ----------------------------------------------\n",
    "joblib.dump(model, 'data/random_forest_model.joblib')\n",
    "print(\"✔️ تم حفظ الموديل في: data/random_forest_model.joblib\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 7️⃣ توقع الاحتمالات\n",
    "# ----------------------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 8️⃣ اختيار Threshold تلقائي لتحقيق Recall ≥ 0.8 و Precision ≥ 0.3\n",
    "# ----------------------------------------------\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "best_threshold = 0.5\n",
    "for p, r, t in zip(precisions, recalls, np.append(thresholds,1.0)):\n",
    "    if r >= 0.8 and p >= 0.3:\n",
    "        best_threshold = t\n",
    "        break\n",
    "\n",
    "y_pred_custom = (y_prob >= best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✔️ أفضل Threshold: {best_threshold:.3f}\")\n",
    "print(\"\\n------ Classification Report (custom threshold) ------\")\n",
    "print(classification_report(y_test, y_pred_custom, zero_division=0))\n",
    "\n",
    "roc = roc_auc_score(y_test, y_prob)\n",
    "pr_auc = auc(recalls, precisions)\n",
    "print(f\"ROC-AUC: {roc:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 9️⃣ Feature Importance\n",
    "# ----------------------------------------------\n",
    "importances = model.feature_importances_\n",
    "feat_imp_df = pd.DataFrame({'feature': numeric_cols, 'importance': importances})\n",
    "feat_imp_df = feat_imp_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(feat_imp_df['feature'], feat_imp_df['importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance - RandomForest\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 🔟 Precision-Recall Curve\n",
    "# ----------------------------------------------\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(recalls, precisions, label=f'PR Curve (AUC={pr_auc:.4f})')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1️⃣1️⃣ مثال لإعادة تحميل النموذج لاحقًا\n",
    "# ----------------------------------------------\n",
    "# loaded_model = joblib.load('data/random_forest_model.joblib')\n",
    "# y_prob_loaded = loaded_model.predict_proba(X_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152bd79-2bea-4241-a626-a69996319eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# استيراد المكتبات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ توليد بيانات وهمية كبيرة\n",
    "# ------------------------------\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "# auth_parsed_large.csv\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "\n",
    "# auth_features_large.csv\n",
    "feat = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n",
    "    'total_count': np.random.randint(1, 15, size=n_rows),\n",
    "    'fail_rate': np.random.rand(n_rows),\n",
    "    'event_user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'event_result': np.random.choice(['success','failed'], size=n_rows)\n",
    "})\n",
    "feat.to_csv('data/auth_features_large.csv', index=False)\n",
    "\n",
    "print(\"✅ تم إنشاء البيانات بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تنظيف البيانات وفحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nMissing values in auth_parsed:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nMissing values in auth_features:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# إزالة أي صفوف مفقودة\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ إعداد الميزات والهدف\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "y = df_clean['result'].map({'success':0, 'failed':1})  # تحويل إلى 0/1\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تدريب موديل RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ التوقع والتقييم\n",
    "# ------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]  # احتمالية فئة failed\n",
    "\n",
    "print(\"\\n------ Classification Report ------\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "\n",
    "precisions, recalls, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(recalls, precisions)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Feature Importance\n",
    "# ------------------------------\n",
    "importances = model.feature_importances_\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(numeric_cols, importances)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Precision/Recall لكل Threshold\n",
    "# ------------------------------\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "thresholds = np.append(thresholds, 1.0)\n",
    "\n",
    "pr_df = pd.DataFrame({'threshold': thresholds, 'precision': precisions, 'recall': recalls})\n",
    "print(\"\\n------ جدول Precision/Recall لكل Threshold ------\")\n",
    "print(pr_df.head(20))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(pr_df['threshold'], pr_df['precision'], label='Precision', color='blue')\n",
    "plt.plot(pr_df['threshold'], pr_df['recall'], label='Recall', color='green')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision & Recall مقابل Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# العثور على أفضل Threshold يحقق Recall >= 0.8 و Precision >= 0.3\n",
    "best_threshold = 0.5\n",
    "for p, r, t in zip(precisions, recalls, thresholds):\n",
    "    if r >= 0.8 and p >= 0.3:\n",
    "        best_threshold = t\n",
    "        break\n",
    "\n",
    "print(f\"\\n✔️ أفضل Threshold لتحقيق Recall>=0.8 و Precision>=0.3: {best_threshold:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361a207-a585-4eb5-95c1-3b47ddf01e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# استيراد المكتبات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ توليد بيانات وهمية كبيرة\n",
    "# ------------------------------\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "# auth_parsed_large.csv\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "\n",
    "# auth_features_large.csv\n",
    "feat = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n",
    "    'total_count': np.random.randint(1, 15, size=n_rows),\n",
    "    'fail_rate': np.random.rand(n_rows),\n",
    "    'event_user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'event_result': np.random.choice(['success','failed'], size=n_rows)\n",
    "})\n",
    "feat.to_csv('data/auth_features_large.csv', index=False)\n",
    "\n",
    "print(\"✅ تم إنشاء البيانات بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تنظيف البيانات وفحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nMissing values in auth_parsed:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nMissing values in auth_features:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# إزالة أي صفوف مفقودة\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ إعداد الميزات والهدف\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "y = df_clean['result'].map({'success':0, 'failed':1})  # تحويل إلى 0/1\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تدريب موديل RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ التوقع والتقييم\n",
    "# ------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]  # احتمالية فئة failed\n",
    "\n",
    "print(\"\\n------ Classification Report ------\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "\n",
    "precisions, recalls, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(recalls, precisions)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Feature Importance\n",
    "# ------------------------------\n",
    "importances = model.feature_importances_\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(numeric_cols, importances)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Precision/Recall لكل Threshold\n",
    "# ------------------------------\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "thresholds = np.append(thresholds, 1.0)\n",
    "\n",
    "pr_df = pd.DataFrame({'threshold': thresholds, 'precision': precisions, 'recall': recalls})\n",
    "print(\"\\n------ جدول Precision/Recall لكل Threshold ------\")\n",
    "print(pr_df.head(20))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(pr_df['threshold'], pr_df['precision'], label='Precision', color='blue')\n",
    "plt.plot(pr_df['threshold'], pr_df['recall'], label='Recall', color='green')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision & Recall مقابل Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# العثور على أفضل Threshold يحقق Recall >= 0.8 و Precision >= 0.3\n",
    "best_threshold = 0.5\n",
    "for p, r, t in zip(precisions, recalls, thresholds):\n",
    "    if r >= 0.8 and p >= 0.3:\n",
    "        best_threshold = t\n",
    "        break\n",
    "\n",
    "print(f\"\\n✔️ أفضل Threshold لتحقيق Recall>=0.8 و Precision>=0.3: {best_threshold:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1b050-b194-4bcf-a2b4-e97fd9d768c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# استيراد المكتبات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ توليد بيانات وهمية كبيرة\n",
    "# ------------------------------\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "# auth_parsed_large.csv\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "df.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "\n",
    "# auth_features_large.csv\n",
    "feat = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n",
    "    'total_count': np.random.randint(1, 15, size=n_rows),\n",
    "    'fail_rate': np.random.rand(n_rows),\n",
    "    'event_user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'event_result': np.random.choice(['success','failed'], size=n_rows)\n",
    "})\n",
    "feat.to_csv('data/auth_features_large.csv', index=False)\n",
    "\n",
    "print(\"✅ تم إنشاء البيانات بنجاح!\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تنظيف البيانات وفحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\nMissing values in auth_parsed:\")\n",
    "print(df.isnull().mean())\n",
    "\n",
    "print(\"\\nMissing values in auth_features:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# إزالة أي صفوف مفقودة\n",
    "df_clean = df.dropna()\n",
    "feat_clean = feat.dropna()\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ إعداد الميزات والهدف\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat_clean[numeric_cols]\n",
    "y = df_clean['result'].map({'success':0, 'failed':1})  # تحويل إلى 0/1\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تدريب موديل RandomForest\n",
    "# ------------------------------\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ التوقع والتقييم\n",
    "# ------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]  # احتمالية فئة failed\n",
    "\n",
    "print(\"\\n------ Classification Report ------\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "\n",
    "precisions, recalls, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(recalls, precisions)\n",
    "print(\"PR AUC:\", pr_auc)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Feature Importance\n",
    "# ------------------------------\n",
    "importances = model.feature_importances_\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(numeric_cols, importances)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Precision/Recall لكل Threshold\n",
    "# ------------------------------\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "thresholds = np.append(thresholds, 1.0)\n",
    "\n",
    "pr_df = pd.DataFrame({'threshold': thresholds, 'precision': precisions, 'recall': recalls})\n",
    "print(\"\\n------ جدول Precision/Recall لكل Threshold ------\")\n",
    "print(pr_df.head(20))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(pr_df['threshold'], pr_df['precision'], label='Precision', color='blue')\n",
    "plt.plot(pr_df['threshold'], pr_df['recall'], label='Recall', color='green')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision & Recall مقابل Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# العثور على أفضل Threshold يحقق Recall >= 0.8 و Precision >= 0.3\n",
    "best_threshold = 0.5\n",
    "for p, r, t in zip(precisions, recalls, thresholds):\n",
    "    if r >= 0.8 and p >= 0.3:\n",
    "        best_threshold = t\n",
    "        break\n",
    "\n",
    "print(f\"\\n✔️ أفضل Threshold لتحقيق Recall>=0.8 و Precision>=0.3: {best_threshold:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52068cfe-edf2-4ac5-9cb4-2cfa48cf1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline (English comments) — Synthetic data -> features -> train -> eval -> plots -> save model\n",
    "# Run this in a Jupyter cell.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Config / randomness\n",
    "# -------------------------------\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Generate synthetic datasets\n",
    "#    - auth_parsed (events with result)\n",
    "#    - auth_features (precomputed rolling counts / rates)\n",
    "# -------------------------------\n",
    "n_rows = 5000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')  # use 'min' (no FutureWarning)\n",
    "\n",
    "# auth_parsed: event-level (timestamp, ip, user, result)\n",
    "df_parsed = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    # make failed less frequent but not too rare\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.75,0.25])\n",
    "})\n",
    "\n",
    "# auth_features: features aligned (timestamp, ip, counts, rates, event_user, event_result)\n",
    "df_features = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    # make ip sometimes different to simulate imperfect joins\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'cnt_last_1m': np.random.randint(0, 10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0, 50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0, 150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0, 10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0, 5, size=n_rows),\n",
    "    'total_count': np.random.randint(1, 15, size=n_rows),\n",
    "    'fail_rate': np.random.random(size=n_rows),\n",
    "    # event_user / event_result for optional sanity-check/eval\n",
    "    'event_user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'event_result': np.random.choice(['success','failed'], size=n_rows, p=[0.75,0.25])\n",
    "})\n",
    "\n",
    "# Save (optional, helpful to inspect files on disk)\n",
    "df_parsed.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "df_features.to_csv('data/auth_features_large.csv', index=False)\n",
    "print(\"Data files saved to ./data/ (auth_parsed_large.csv, auth_features_large.csv)\")\n",
    "print(\"Sizes:\", df_parsed.shape, df_features.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Merge parsed + features to align labels and features\n",
    "#    Use a left join on timestamp + ip where we assume features may or may not match exactly.\n",
    "# -------------------------------\n",
    "# If your real data uses other keys, change merge keys accordingly.\n",
    "merged = pd.merge(df_parsed, df_features, on=['timestamp', 'ip'], how='left', suffixes=('_parsed','_feat'))\n",
    "\n",
    "# Show how many parsed rows lost features\n",
    "print(\"\\nAfter merge shapes:\", merged.shape)\n",
    "missing_features = merged[['cnt_last_1m','cnt_last_5m','cnt_last_15m']].isnull().any(axis=1).sum()\n",
    "print(f\"Rows with missing features after merge: {missing_features} (out of {len(merged)})\")\n",
    "\n",
    "# For rows with missing numeric features fill with 0 (or other imputation strategy)\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "for c in numeric_cols:\n",
    "    if c in merged.columns:\n",
    "        merged[c] = merged[c].fillna(0)\n",
    "\n",
    "# Clean label column: ensure 'result' exists and valid\n",
    "print(\"\\nUnique values in parsed.result before cleaning:\", merged['result'].unique()[:10])\n",
    "merged['result'] = merged['result'].astype(str).str.strip().str.lower()\n",
    "# keep only 'success'/'failed' rows\n",
    "merged = merged[merged['result'].isin(['success','failed'])].copy()\n",
    "# map to numeric\n",
    "merged['label'] = merged['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "print(\"After cleaning labels, rows:\", merged.shape[0])\n",
    "print(\"Label distribution:\\n\", merged['label'].value_counts())\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Build feature matrix X and target y\n",
    "# -------------------------------\n",
    "X = merged[numeric_cols].copy()\n",
    "y = merged['label'].copy()\n",
    "\n",
    "# Quick sanity checks\n",
    "assert len(X) == len(y), \"Feature/target length mismatch\"\n",
    "\n",
    "# If dataset too small for stratify splitting, handle that:\n",
    "if len(y) < 10 or y.value_counts().min() < 2:\n",
    "    raise ValueError(\"Not enough samples or too-rare positive class to train reliably. Increase data.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Train/test split\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "print(\"\\nSplit sizes — train:\", X_train.shape, \"test:\", X_test.shape)\n",
    "print(\"Train label dist:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Test label dist:\\n\", y_test.value_counts(normalize=True))\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Train a RandomForest (class_weight='balanced' to help imbalance)\n",
    "# -------------------------------\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, class_weight='balanced', n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"\\nModel trained (RandomForest).\")\n",
    "\n",
    "# Optionally save the model to disk\n",
    "model_path = 'data/random_forest_model.joblib'\n",
    "joblib.dump(model, model_path)\n",
    "print(\"Saved trained model to:\", model_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Baseline evaluation with default threshold 0.5\n",
    "# -------------------------------\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # probability for class 'failed' (1)\n",
    "y_pred_default = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== Evaluation (default threshold=0.5) ===\")\n",
    "print(classification_report(y_test, y_pred_default, digits=4, zero_division=0))\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC: {:.4f}\".format(roc_auc))\n",
    "precisions, recalls, pr_thresholds = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(recalls, precisions)\n",
    "print(\"PR AUC: {:.4f}\".format(pr_auc))\n",
    "\n",
    "# -------------------------------\n",
    "# 7) Find threshold automatically:\n",
    "#    prefer thresholds that satisfy recall >= 0.8 and precision >= 0.3;\n",
    "#    if none exist, pick threshold that maximizes F1 score.\n",
    "# -------------------------------\n",
    "# Build arrays for thresholds aligned with precisions/recalls:\n",
    "# precision_recall_curve returns arrays: precision[i], recall[i] for threshold = thresholds[i-1]\n",
    "# we'll compute F1 for corresponding thresholds by evaluating predictions.\n",
    "candidate_thresholds = np.linspace(0.0, 1.0, 101)\n",
    "\n",
    "best_thresh = None\n",
    "best_choice = None  # store tuple (precision, recall, f1)\n",
    "\n",
    "# compute precision/recall/f1 for each candidate threshold\n",
    "for t in candidate_thresholds:\n",
    "    y_pred_t = (y_prob >= t).astype(int)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred_t, average='binary', zero_division=0)\n",
    "    # prefer thresholds meeting constraints first\n",
    "    if (r >= 0.8) and (p >= 0.3):\n",
    "        best_thresh = t\n",
    "        best_choice = (p, r, f1)\n",
    "        break\n",
    "\n",
    "# if not found, pick threshold that maximizes F1\n",
    "if best_thresh is None:\n",
    "    best_f1 = -1\n",
    "    best_t = 0.5\n",
    "    for t in candidate_thresholds:\n",
    "        y_pred_t = (y_prob >= t).astype(int)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred_t, average='binary', zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_t = t\n",
    "            best_choice = (p, r, f1)\n",
    "    best_thresh = best_t\n",
    "\n",
    "print(\"\\nSelected threshold:\", best_thresh)\n",
    "print(\"Selected threshold metrics: precision={:.4f}, recall={:.4f}, f1={:.4f}\".format(*best_choice))\n",
    "\n",
    "# Apply selected threshold to produce final predictions\n",
    "y_pred_best = (y_prob >= best_thresh).astype(int)\n",
    "\n",
    "# Final classification report\n",
    "print(\"\\n=== Final Evaluation (selected threshold) ===\")\n",
    "print(classification_report(y_test, y_pred_best, digits=4, zero_division=0))\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC AUC (probabilities): {:.4f}\".format(roc_auc))\n",
    "precisions_full, recalls_full, thresholds_full = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc_full = auc(recalls_full, precisions_full)\n",
    "print(\"PR AUC: {:.4f}\".format(pr_auc_full))\n",
    "\n",
    "# -------------------------------\n",
    "# 8) Plots: ROC curve, Precision-Recall, Confusion matrix, Feature importance\n",
    "# -------------------------------\n",
    "# ROC curve\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "RocCurveDisplay.from_predictions(y_test, y_prob, name=\"RandomForest\", ax=plt.gca())\n",
    "plt.title(\"ROC Curve\")\n",
    "\n",
    "# Precision-Recall curve\n",
    "plt.subplot(1,3,2)\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_prob, name=\"RandomForest\", ax=plt.gca())\n",
    "plt.axhline(0.3, color='red', linestyle='--', label='Precision target=0.3')\n",
    "plt.axvline(0.8, color='green', linestyle='--', label='Recall target=0.8')\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "\n",
    "# Confusion matrix at selected threshold\n",
    "plt.subplot(1,3,3)\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['success','failed'])\n",
    "disp.plot(ax=plt.gca(), cmap='Blues', values_format='d')\n",
    "plt.title(f\"Confusion Matrix (threshold={best_thresh:.2f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': numeric_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.barh(feat_imp['feature'], feat_imp['importance'])\n",
    "plt.title(\"Feature importance (RandomForest)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 9) Precision & Recall table by threshold (top entries)\n",
    "# -------------------------------\n",
    "# Build PR table using candidate_thresholds\n",
    "rows = []\n",
    "for t in candidate_thresholds:\n",
    "    y_pred_t = (y_prob >= t).astype(int)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred_t, average='binary', zero_division=0)\n",
    "    rows.append((t, p, r, f1))\n",
    "pr_table = pd.DataFrame(rows, columns=['threshold','precision','recall','f1'])\n",
    "print(\"\\nTop 20 threshold rows (precision/recall/f1):\")\n",
    "print(pr_table.sort_values(by='recall', ascending=False).head(20))\n",
    "\n",
    "# -------------------------------\n",
    "# 10) Example: predict on new feature rows (use some rows from features as \"new\")\n",
    "# -------------------------------\n",
    "new_features = df_features.sample(10, random_state=RANDOM_SEED).copy()\n",
    "# ensure numeric cols exist and match order\n",
    "X_new = new_features[numeric_cols].fillna(0)\n",
    "new_probs = model.predict_proba(X_new)[:,1]\n",
    "new_preds = (new_probs >= best_thresh).astype(int)\n",
    "new_features = new_features.assign(pred_failed_prob=new_probs, pred_failed=new_preds)\n",
    "\n",
    "print(\"\\nSample predictions on new data:\")\n",
    "print(new_features[['timestamp','ip'] + numeric_cols + ['pred_failed_prob','pred_failed']].head())\n",
    "\n",
    "# If 'event_result' exists, evaluate predictions for those rows\n",
    "if 'event_result' in new_features.columns:\n",
    "    y_true_new = new_features['event_result'].map({'success':0,'failed':1})\n",
    "    # only evaluate rows where mapped values not NA\n",
    "    valid_mask = y_true_new.notna()\n",
    "    if valid_mask.any():\n",
    "        p_new, r_new, f1_new, _ = precision_recall_fscore_support(y_true_new[valid_mask], new_features.loc[valid_mask,'pred_failed'], average='binary', zero_division=0)\n",
    "        print(f\"\\nNew-data evaluation (sample): precision={p_new:.4f}, recall={r_new:.4f}, f1={f1_new:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 11) Save final artifacts: model and PR table to csv\n",
    "# -------------------------------\n",
    "joblib.dump(model, 'data/final_random_forest_model.joblib')\n",
    "pr_table.to_csv('data/precision_recall_by_threshold.csv', index=False)\n",
    "print(\"\\nSaved model -> data/final_random_forest_model.joblib\")\n",
    "print(\"Saved precision/recall table -> data/precision_recall_by_threshold.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# 12) Summary print\n",
    "# -------------------------------\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(f\" - Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\" - Selected threshold: {best_thresh:.3f}\")\n",
    "p_final, r_final, f1_final, _ = precision_recall_fscore_support(y_test, y_pred_best, average='binary', zero_division=0)\n",
    "print(f\" - Final metrics at selected threshold: precision={p_final:.4f}, recall={r_final:.4f}, f1={f1_final:.4f}, ROC AUC={roc_auc:.4f}, PR AUC={pr_auc_full:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618afa0-80b0-43b6-80c4-5ccd065c71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 4: Handle Class Imbalance with SMOTE ======\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# ✅ Apply SMOTE to training data only (to avoid data leakage)\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", y_train.value_counts().to_dict())\n",
    "print(\"After SMOTE:\",  pd.Series(y_res).value_counts().to_dict())\n",
    "\n",
    "# ✅ Train a Logistic Regression model on balanced data\n",
    "model_smote = LogisticRegression(max_iter=1000, class_weight=None)\n",
    "model_smote.fit(X_res, y_res)\n",
    "\n",
    "# ✅ Evaluate performance on test data\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "y_prob_smote = model_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n--- Classification Report (SMOTE + Logistic Regression) ---\")\n",
    "print(classification_report(y_test, y_pred_smote))\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_smote))\n",
    "\n",
    "# ✅ Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_smote)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91bca8e-c01a-4c67-8a84-8df5691f8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 5: Try Stronger Models + Cross-Validation ======\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Random Forest Base Model ---\n",
    "rf = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "rf.fit(X_res, y_res)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Random Forest Results ===\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_rf))\n",
    "\n",
    "# --- Hyperparameter Search for Random Forest ---\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rs_rf = RandomizedSearchCV(\n",
    "    rf, param_distributions=param_dist, n_iter=10, cv=cv,\n",
    "    scoring='recall', n_jobs=-1, random_state=42\n",
    ")\n",
    "rs_rf.fit(X_res, y_res)\n",
    "print(\"\\nBest Random Forest Params:\", rs_rf.best_params_)\n",
    "\n",
    "# --- XGBoost Model ---\n",
    "xgb = XGBClassifier(\n",
    "    scale_pos_weight=1,  # we used SMOTE, so we can set this to 1\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb.fit(X_res, y_res)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_prob_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== XGBoost Results ===\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6cdca-da30-45c1-9e78-4d71169b2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Step 5: Try Stronger Models + Cross-Validation ======\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Random Forest Base Model ---\n",
    "rf = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "rf.fit(X_res, y_res)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Random Forest Results ===\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_rf))\n",
    "\n",
    "# --- Hyperparameter Search for Random Forest ---\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rs_rf = RandomizedSearchCV(\n",
    "    rf, param_distributions=param_dist, n_iter=10, cv=cv,\n",
    "    scoring='recall', n_jobs=-1, random_state=42\n",
    ")\n",
    "rs_rf.fit(X_res, y_res)\n",
    "print(\"\\nBest Random Forest Params:\", rs_rf.best_params_)\n",
    "\n",
    "# --- XGBoost Model ---\n",
    "xgb = XGBClassifier(\n",
    "    scale_pos_weight=1,  # we used SMOTE, so we can set this to 1\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb.fit(X_res, y_res)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_prob_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== XGBoost Results ===\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156aece-4a78-4697-8c0d-e4fd8ca6dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 0️⃣ استيراد المكتبات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ قراءة البيانات الكبيرة\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "# ساعة اليوم ووقت الليل\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# تحويل بعض العدادات باللوجاريتم لتقليل القيم المتطرفة\n",
    "for col in ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count']:\n",
    "    feat[col+'_log'] = np.log1p(feat[col])\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تجهيز X و y\n",
    "# ------------------------------\n",
    "# دمج العمود المستهدف من df\n",
    "df['result_binary'] = df['result'].map({'success':0, 'failed':1})\n",
    "y = df['result_binary']\n",
    "\n",
    "# اختيار ميزات عددية\n",
    "numeric_cols = ['cnt_last_1m_log','cnt_last_5m_log','cnt_last_15m_log',\n",
    "                'succ_count_log','fail_count_log','total_count_log',\n",
    "                'fail_rate','hour','is_night']\n",
    "\n",
    "X = feat[numeric_cols]\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ مواجهة عدم التوازن\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تدريب نموذج RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ تدريب نموذج XGBoost\n",
    "# ------------------------------\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, scale_pos_weight=1)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ تقييم الأداء على مجموعة الاختبار\n",
    "# ------------------------------\n",
    "for model_name, model in [('RandomForest', rf), ('XGBoost', xgb)]:\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ حفظ أفضل نموذج\n",
    "# ------------------------------\n",
    "joblib.dump({'model_rf': rf, 'model_xgb': xgb, 'features': numeric_cols}, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9cec67-8d56-4b14-8298-ea1995e8cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 0️⃣ استيراد المكتبات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ قراءة البيانات الكبيرة\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "# ساعة اليوم ووقت الليل\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# تحويل بعض العدادات باللوجاريتم لتقليل القيم المتطرفة\n",
    "for col in ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count']:\n",
    "    feat[col+'_log'] = np.log1p(feat[col])\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تجهيز X و y\n",
    "# ------------------------------\n",
    "# دمج العمود المستهدف من df\n",
    "df['result_binary'] = df['result'].map({'success':0, 'failed':1})\n",
    "y = df['result_binary']\n",
    "\n",
    "# اختيار ميزات عددية\n",
    "numeric_cols = ['cnt_last_1m_log','cnt_last_5m_log','cnt_last_15m_log',\n",
    "                'succ_count_log','fail_count_log','total_count_log',\n",
    "                'fail_rate','hour','is_night']\n",
    "\n",
    "X = feat[numeric_cols]\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ مواجهة عدم التوازن\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تدريب نموذج RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ تدريب نموذج XGBoost\n",
    "# ------------------------------\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, scale_pos_weight=1)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ تقييم الأداء على مجموعة الاختبار\n",
    "# ------------------------------\n",
    "for model_name, model in [('RandomForest', rf), ('XGBoost', xgb)]:\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ حفظ أفضل نموذج\n",
    "# ------------------------------\n",
    "joblib.dump({'model_rf': rf, 'model_xgb': xgb, 'features': numeric_cols}, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7094a-4b2f-4cc0-8399-9e58325f7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 0️⃣ استيراد المكتبات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ قراءة البيانات الكبيرة\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "# ساعة اليوم ووقت الليل\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# تحويل بعض العدادات باللوجاريتم لتقليل القيم المتطرفة\n",
    "for col in ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count']:\n",
    "    feat[col+'_log'] = np.log1p(feat[col])\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تجهيز X و y\n",
    "# ------------------------------\n",
    "# دمج العمود المستهدف من df\n",
    "df['result_binary'] = df['result'].map({'success':0, 'failed':1})\n",
    "y = df['result_binary']\n",
    "\n",
    "# اختيار ميزات عددية\n",
    "numeric_cols = ['cnt_last_1m_log','cnt_last_5m_log','cnt_last_15m_log',\n",
    "                'succ_count_log','fail_count_log','total_count_log',\n",
    "                'fail_rate','hour','is_night']\n",
    "\n",
    "X = feat[numeric_cols]\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ مواجهة عدم التوازن\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تدريب نموذج RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ تدريب نموذج XGBoost\n",
    "# ------------------------------\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, scale_pos_weight=1)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ تقييم الأداء على مجموعة الاختبار\n",
    "# ------------------------------\n",
    "for model_name, model in [('RandomForest', rf), ('XGBoost', xgb)]:\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ حفظ أفضل نموذج\n",
    "# ------------------------------\n",
    "joblib.dump({'model_rf': rf, 'model_xgb': xgb, 'features': numeric_cols}, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840e87b-a49a-4bdd-a2c1-52958ebaaa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "import joblib\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ تأكد من وجود مجلدات data و models\n",
    "# ------------------------------\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ تنظيف البيانات الأساسية\n",
    "# ------------------------------\n",
    "df.dropna(subset=['result', 'ip', 'user', 'timestamp'], inplace=True)\n",
    "feat.dropna(subset=['timestamp', 'ip'], inplace=True)\n",
    "\n",
    "# تحويل النتائج إلى 0/1\n",
    "y = df['result'].map({'success':0, 'failed':1})\n",
    "\n",
    "# اختيار الميزات العددية\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate']\n",
    "X = feat[numeric_cols]\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ تدريب نماذج\n",
    "# ------------------------------\n",
    "# RandomForest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier(n_estimators=100, random_state=42, scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ تقييم النماذج\n",
    "# ------------------------------\n",
    "for name, model in [('RandomForest', rf), ('XGBoost', xgb)]:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ حفظ النماذج\n",
    "# ------------------------------\n",
    "joblib.dump({'model_rf': rf, 'model_xgb': xgb, 'features': numeric_cols}, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af5c5c-fbbb-441a-a2fe-7ac3f185c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Create Sample Data\n",
    "# ------------------------------\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "# auth_parsed.csv\n",
    "df_parsed = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "df_parsed.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "\n",
    "# auth_features.csv\n",
    "df_features = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': df_parsed['ip'],\n",
    "    'cnt_last_1m': np.random.randint(0,10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0,50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0,150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0,10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0,5, size=n_rows),\n",
    "    'total_count': np.random.randint(1,15, size=n_rows),\n",
    "    'fail_rate': np.random.random(size=n_rows),\n",
    "    'event_user': df_parsed['user'],\n",
    "    'event_result': df_parsed['result']\n",
    "})\n",
    "df_features.to_csv('data/auth_features_large.csv', index=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Load and Clean Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Basic check\n",
    "print(\"Missing values in auth_parsed:\", df.isnull().mean())\n",
    "print(\"Result counts:\", df['result'].value_counts())\n",
    "print(\"Feature stats:\", feat.describe())\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Use only numeric features for modeling\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate','hour','is_night']\n",
    "X = feat[numeric_cols]\n",
    "y = df['result_bin']\n",
    "\n",
    "# Handle missing values if any\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Default threshold 0.5\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    # Precision-Recall AUC\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    # Find threshold for recall>=0.8 and precision>=0.3\n",
    "    threshold_opt = 0.5  # default\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Simple Alert System (Example)\n",
    "# ------------------------------\n",
    "# Use RandomForest as main model\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]  # indices of predicted \"failed\" events\n",
    "\n",
    "print(f\"\\nTotal alerts (failed events predicted above threshold {threshold:.3f}): {len(alerts)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Models and Scaler\n",
    "# ------------------------------\n",
    "os.makedirs('models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db2750-8828-43a8-ae82-16cd159eaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Create Sample Data\n",
    "# ------------------------------\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "# auth_parsed.csv\n",
    "df_parsed = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "df_parsed.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "\n",
    "# auth_features.csv\n",
    "df_features = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': df_parsed['ip'],\n",
    "    'cnt_last_1m': np.random.randint(0,10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0,50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0,150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0,10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0,5, size=n_rows),\n",
    "    'total_count': np.random.randint(1,15, size=n_rows),\n",
    "    'fail_rate': np.random.random(size=n_rows),\n",
    "    'event_user': df_parsed['user'],\n",
    "    'event_result': df_parsed['result']\n",
    "})\n",
    "df_features.to_csv('data/auth_features_large.csv', index=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Load and Clean Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Basic check\n",
    "print(\"Missing values in auth_parsed:\", df.isnull().mean())\n",
    "print(\"Result counts:\", df['result'].value_counts())\n",
    "print(\"Feature stats:\", feat.describe())\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Use only numeric features for modeling\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate','hour','is_night']\n",
    "X = feat[numeric_cols]\n",
    "y = df['result_bin']\n",
    "\n",
    "# Handle missing values if any\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Default threshold 0.5\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    # Precision-Recall AUC\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    # Find threshold for recall>=0.8 and precision>=0.3\n",
    "    threshold_opt = 0.5  # default\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Simple Alert System (Example)\n",
    "# ------------------------------\n",
    "# Use RandomForest as main model\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]  # indices of predicted \"failed\" events\n",
    "\n",
    "print(f\"\\nTotal alerts (failed events predicted above threshold {threshold:.3f}): {len(alerts)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Models and Scaler\n",
    "# ------------------------------\n",
    "os.makedirs('models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d453f6c-e02f-49ef-a1e4-5d50eaa5769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Create Sample Data\n",
    "# ------------------------------\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "n_rows = 1000\n",
    "timestamps = pd.date_range(start='2025-10-01', periods=n_rows, freq='min')\n",
    "\n",
    "# auth_parsed.csv\n",
    "df_parsed = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': np.random.choice(['192.168.0.1','192.168.0.2','10.0.0.1','10.0.0.2'], size=n_rows),\n",
    "    'user': np.random.choice(['alice','bob','charlie','dave'], size=n_rows),\n",
    "    'result': np.random.choice(['success','failed'], size=n_rows, p=[0.85,0.15])\n",
    "})\n",
    "df_parsed.to_csv('data/auth_parsed_large.csv', index=False)\n",
    "\n",
    "# auth_features.csv\n",
    "df_features = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'ip': df_parsed['ip'],\n",
    "    'cnt_last_1m': np.random.randint(0,10, size=n_rows),\n",
    "    'cnt_last_5m': np.random.randint(0,50, size=n_rows),\n",
    "    'cnt_last_15m': np.random.randint(0,150, size=n_rows),\n",
    "    'succ_count': np.random.randint(0,10, size=n_rows),\n",
    "    'fail_count': np.random.randint(0,5, size=n_rows),\n",
    "    'total_count': np.random.randint(1,15, size=n_rows),\n",
    "    'fail_rate': np.random.random(size=n_rows),\n",
    "    'event_user': df_parsed['user'],\n",
    "    'event_result': df_parsed['result']\n",
    "})\n",
    "df_features.to_csv('data/auth_features_large.csv', index=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Load and Clean Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Basic check\n",
    "print(\"Missing values in auth_parsed:\", df.isnull().mean())\n",
    "print(\"Result counts:\", df['result'].value_counts())\n",
    "print(\"Feature stats:\", feat.describe())\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Use only numeric features for modeling\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count','fail_rate','hour','is_night']\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Default threshold 0.5\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    # Precision-Recall AUC\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    # Find threshold for recall>=0.8 and precision>=0.3\n",
    "    threshold_opt = 0.5  # default\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Simple Alert System\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]  # indices of predicted \"failed\" events\n",
    "\n",
    "print(f\"\\nTotal alerts (failed events predicted above threshold {threshold:.3f}): {len(alerts)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Models and Scaler\n",
    "# ------------------------------\n",
    "os.makedirs('models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db83c49-5645-4a4b-a333-debc8ebbe994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering (Full)\n",
    "# ------------------------------\n",
    "# --- ميزات الوقت ---\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- فشل متسلسل (failed_streak) ---\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "# --- عدد المستخدمين الفريدين خلال آخر 5 دقائق لكل IP ---\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "def unique_users_last_5m(group):\n",
    "    return group['event_user'].rolling('5min', on='timestamp').apply(lambda x: x.nunique(), raw=False)\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(unique_users_last_5m)\n",
    "\n",
    "# --- متوسط الفاصل الزمني بين المحاولات (avg_interarrival_seconds) ---\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "# --- تحويلات لوغاريتمية لتخفيف القيم المتطرفة ---\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# --- تنظيف الأعمدة المؤقتة ---\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "print(\"✅ Feature Engineering completed.\")\n",
    "print(feat.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds'] + \\\n",
    "               [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Default threshold 0.5\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    # Precision-Recall AUC\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    # Find threshold for recall>=0.8 and precision>=0.3\n",
    "    threshold_opt = 0.5  # default\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Simple Alert System\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "print(f\"\\nTotal alerts (failed events predicted above threshold {threshold:.3f}): {len(alerts)}\")\n",
    "\n",
    "# Export alerts to CSV\n",
    "alert_df = feat.iloc[alerts].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts]\n",
    "alert_df['pred_label'] = 1\n",
    "\n",
    "os.makedirs('alerts', exist_ok=True)\n",
    "alert_file = 'alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported to CSV: {alert_file}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Models and Scaler\n",
    "# ------------------------------\n",
    "os.makedirs('models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054e8e4-6aca-4009-a5d9-7fe4da8c0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering (Full)\n",
    "# ------------------------------\n",
    "# --- ميزات الوقت ---\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- فشل متسلسل (failed_streak) ---\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "# --- عدد المستخدمين الفريدين خلال آخر 5 دقائق لكل IP ---\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "\n",
    "def unique_users_last_5m(group):\n",
    "    group = group.set_index('timestamp')  # تحويل timestamp إلى Index\n",
    "    return group['event_user'].rolling('5min').apply(lambda x: x.nunique(), raw=False)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(unique_users_last_5m)\n",
    "\n",
    "# --- متوسط الفاصل الزمني بين المحاولات (avg_interarrival_seconds) ---\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "# --- تحويلات لوغاريتمية لتخفيف القيم المتطرفة ---\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# --- تنظيف الأعمدة المؤقتة ---\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "print(\"✅ Feature Engineering completed.\")\n",
    "print(feat.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds'] + \\\n",
    "               [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Default threshold 0.5\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    # Precision-Recall AUC\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    # Find threshold for recall>=0.8 and precision>=0.3\n",
    "    threshold_opt = 0.5  # default\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Simple Alert System\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "print(f\"\\nTotal alerts (failed events predicted above threshold {threshold:.3f}): {len(alerts)}\")\n",
    "\n",
    "# Export alerts to CSV\n",
    "alert_df = feat.iloc[alerts].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts]\n",
    "alert_df['pred_label'] = 1\n",
    "\n",
    "os.makedirs('alerts', exist_ok=True)\n",
    "alert_file = 'alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported to CSV: {alert_file}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Models and Scaler\n",
    "# ------------------------------\n",
    "os.makedirs('models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96dabd3-8652-466a-bc98-7f5c80451957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering (Full)\n",
    "# ------------------------------\n",
    "# --- ميزات الوقت ---\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- فشل متسلسل (failed_streak) ---\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "# --- عدد المستخدمين الفريدين خلال آخر 5 دقائق لكل IP (آمن للنصوص) ---\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5, 'm')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask, 'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "# --- متوسط الفاصل الزمني بين المحاولات (avg_interarrival_seconds) ---\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "# --- تحويلات لوغاريتمية لتخفيف القيم المتطرفة ---\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# --- تنظيف الأعمدة المؤقتة ---\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "print(\"✅ Feature Engineering completed.\")\n",
    "print(feat.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds'] + \\\n",
    "               [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Default threshold 0.5\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    # Precision-Recall AUC\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    # Find threshold for recall>=0.8 and precision>=0.3\n",
    "    threshold_opt = 0.5  # default\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Simple Alert System\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "print(f\"\\nTotal alerts (failed events predicted above threshold {threshold:.3f}): {len(alerts)}\")\n",
    "\n",
    "# Export alerts to CSV\n",
    "alert_df = feat.iloc[alerts].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts]\n",
    "alert_df['pred_label'] = 1\n",
    "\n",
    "os.makedirs('alerts', exist_ok=True)\n",
    "alert_file = 'alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported to CSV: {alert_file}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Models and Scaler\n",
    "# ------------------------------\n",
    "os.makedirs('models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37876d-59d1-4daa-a3dc-2c8e5e54a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GeoIP\n",
    "import geoip2.database\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering (Full)\n",
    "# ------------------------------\n",
    "# --- ميزات الوقت ---\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- فشل متسلسل (failed_streak) ---\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "# --- عدد المستخدمين الفريدين خلال آخر 5 دقائق لكل IP ---\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5, 'm')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask, 'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "# --- متوسط الفاصل الزمني بين المحاولات (avg_interarrival_seconds) ---\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "# --- تحويلات لوغاريتمية لتخفيف القيم المتطرفة ---\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader('data/GeoLite2-City.mmdb')\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        country = response.country.iso_code\n",
    "        city = response.city.name\n",
    "        return pd.Series([country, city])\n",
    "    except:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "feat[['geo_country','geo_city']] = feat['ip'].apply(geoip_features)\n",
    "reader.close()\n",
    "\n",
    "# Encode GeoIP categorical features\n",
    "for col in ['geo_country','geo_city']:\n",
    "    le = LabelEncoder()\n",
    "    feat[col] = le.fit_transform(feat[col].astype(str))\n",
    "\n",
    "# --- تنظيف الأعمدة المؤقتة ---\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "print(\"✅ Feature Engineering completed.\")\n",
    "print(feat.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds',\n",
    "                'geo_country','geo_city'] + [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Unsupervised Anomaly Detection\n",
    "# ------------------------------\n",
    "iso = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "iso.fit(X_scaled)\n",
    "feat['anomaly_score'] = iso.score_samples(X_scaled)  # أقل = أكثر شذوذية\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Default threshold 0.5\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    # Precision-Recall AUC\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    # Find threshold for recall>=0.8 and precision>=0.3\n",
    "    threshold_opt = 0.5\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Simple Alert System with anomaly_score\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts_idx = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "alert_df = feat.iloc[alerts_idx].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts_idx]\n",
    "alert_df['pred_label'] = 1\n",
    "\n",
    "# Flag high-risk anomalies\n",
    "alert_df['high_risk'] = alert_df['anomaly_score'] < -0.5\n",
    "\n",
    "os.makedirs('alerts', exist_ok=True)\n",
    "alert_file = 'alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported to CSV: {alert_file}\")\n",
    "print(f\"High-risk alerts count: {alert_df['high_risk'].sum()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 13️⃣ Save Models, Scaler, and Features\n",
    "# ------------------------------\n",
    "os.makedirs('models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "save_dict['iso_model'] = iso  # إضافة نموذج anomaly\n",
    "\n",
    "joblib.dump(save_dict, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884505c-08e2-498c-8f06-52c669172fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoip2.database\n",
    "import os\n",
    "\n",
    "# مسار GeoIP بالنسبة لمجلد scripts\n",
    "geoip_path = os.path.join('..', 'data', 'GeoLite2-City.mmdb')\n",
    "reader = geoip2.database.Reader(geoip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd958ea-9c89-4e73-b131-c37fa1a05af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GeoIP\n",
    "import geoip2.database\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('../data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('../data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering (Full)\n",
    "# ------------------------------\n",
    "# --- ميزات الوقت ---\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- فشل متسلسل (failed_streak) ---\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "# --- عدد المستخدمين الفريدين خلال آخر 5 دقائق لكل IP ---\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5, 'm')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask, 'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "# --- متوسط الفاصل الزمني بين المحاولات (avg_interarrival_seconds) ---\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "# --- تحويلات لوغاريتمية لتخفيف القيم المتطرفة ---\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# --- GeoIP Features (مسار آمن) ---\n",
    "geoip_path = os.path.join('..', 'data', 'GeoLite2-City.mmdb')\n",
    "reader = geoip2.database.Reader(geoip_path)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        country = response.country.iso_code\n",
    "        city = response.city.name\n",
    "        return pd.Series([country, city])\n",
    "    except:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "feat[['geo_country','geo_city']] = feat['ip'].apply(geoip_features)\n",
    "reader.close()\n",
    "\n",
    "# Encode GeoIP categorical features\n",
    "for col in ['geo_country','geo_city']:\n",
    "    le = LabelEncoder()\n",
    "    feat[col] = le.fit_transform(feat[col].astype(str))\n",
    "\n",
    "# --- تنظيف الأعمدة المؤقتة ---\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "print(\"✅ Feature Engineering completed.\")\n",
    "print(feat.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds',\n",
    "                'geo_country','geo_city'] + [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Unsupervised Anomaly Detection\n",
    "# ------------------------------\n",
    "iso = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "iso.fit(X_scaled)\n",
    "feat['anomaly_score'] = iso.score_samples(X_scaled)  # أقل = أكثر شذوذية\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    threshold_opt = 0.5\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Alerts with anomaly_score\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts_idx = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "alert_df = feat.iloc[alerts_idx].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts_idx]\n",
    "alert_df['pred_label'] = 1\n",
    "alert_df['high_risk'] = alert_df['anomaly_score'] < -0.5\n",
    "\n",
    "os.makedirs('../alerts', exist_ok=True)\n",
    "alert_file = '../alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported to CSV: {alert_file}\")\n",
    "print(f\"High-risk alerts count: {alert_df['high_risk'].sum()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 13️⃣ Save Models, Scaler, Features\n",
    "# ------------------------------\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "save_dict['iso_model'] = iso\n",
    "\n",
    "joblib.dump(save_dict, '../models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd79ed-aa11-4483-a5e4-0b7ab285fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('../data/auth_features.csv', parse_dates=['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984bf665-37ba-45e8-8a4e-08b431dd498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GeoIP\n",
    "import geoip2.database\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data (مسارات صحيحة)\n",
    "# ------------------------------\n",
    "df = pd.read_csv('../data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('../data/auth_features.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering (Full)\n",
    "# ------------------------------\n",
    "# --- ميزات الوقت ---\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- فشل متسلسل (failed_streak) ---\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "# --- عدد المستخدمين الفريدين خلال آخر 5 دقائق لكل IP ---\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5, 'm')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask, 'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "# --- متوسط الفاصل الزمني بين المحاولات (avg_interarrival_seconds) ---\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "# --- تحويلات لوغاريتمية لتخفيف القيم المتطرفة ---\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# --- GeoIP Features (مسار آمن) ---\n",
    "geoip_path = os.path.join('..', 'data', 'GeoLite2-City.mmdb')\n",
    "reader = geoip2.database.Reader(geoip_path)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        country = response.country.iso_code\n",
    "        city = response.city.name\n",
    "        return pd.Series([country, city])\n",
    "    except:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "feat[['geo_country','geo_city']] = feat['ip'].apply(geoip_features)\n",
    "reader.close()\n",
    "\n",
    "# Encode GeoIP categorical features\n",
    "for col in ['geo_country','geo_city']:\n",
    "    le = LabelEncoder()\n",
    "    feat[col] = le.fit_transform(feat[col].astype(str))\n",
    "\n",
    "# --- تنظيف الأعمدة المؤقتة ---\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "print(\"✅ Feature Engineering completed.\")\n",
    "print(feat.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds',\n",
    "                'geo_country','geo_city'] + [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Unsupervised Anomaly Detection\n",
    "# ------------------------------\n",
    "iso = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "iso.fit(X_scaled)\n",
    "feat['anomaly_score'] = iso.score_samples(X_scaled)  # أقل = أكثر شذوذية\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    threshold_opt = 0.5\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Alerts with anomaly_score\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts_idx = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "alert_df = feat.iloc[alerts_idx].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts_idx]\n",
    "alert_df['pred_label'] = 1\n",
    "alert_df['high_risk'] = alert_df['anomaly_score'] < -0.5\n",
    "\n",
    "os.makedirs('../alerts', exist_ok=True)\n",
    "alert_file = '../alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported to CSV: {alert_file}\")\n",
    "print(f\"High-risk alerts count: {alert_df['high_risk'].sum()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 13️⃣ Save Models, Scaler, Features\n",
    "# ------------------------------\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "save_dict['iso_model'] = iso\n",
    "\n",
    "joblib.dump(save_dict, '../models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a86f5e-ce8e-41fb-aee1-af84d7336233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['result_bin'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79982f07-62ee-4505-a6d9-2ad9df63114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# إزالة الصفوف التي فيها result_bin فارغ\n",
    "valid_idx = df['result_bin'].notna()\n",
    "X_scaled = X_scaled[valid_idx]\n",
    "y = df['result_bin'][valid_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ba24e-f744-4544-b1eb-ad25bd3df36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# إزالة الصفوف التي فيها y فارغ\n",
    "valid_idx = df['result_bin'].notna()\n",
    "\n",
    "# فلترة y و X معًا بناءً على valid_idx\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n",
    "X = X.loc[valid_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488db8e-ddfa-47b0-a4a4-64b14c850716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import geoip2.database\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('../data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('../data/auth_features.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5, 'm')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask, 'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# GeoIP Features\n",
    "geoip_path = os.path.join('..', 'data', 'GeoLite2-City.mmdb')\n",
    "reader = geoip2.database.Reader(geoip_path)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        country = response.country.iso_code\n",
    "        city = response.city.name\n",
    "        return pd.Series([country, city])\n",
    "    except:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "feat[['geo_country','geo_city']] = feat['ip'].apply(geoip_features)\n",
    "reader.close()\n",
    "\n",
    "for col in ['geo_country','geo_city']:\n",
    "    le = LabelEncoder()\n",
    "    feat[col] = le.fit_transform(feat[col].astype(str))\n",
    "\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds',\n",
    "                'geo_country','geo_city'] + [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# محاذاة X و y\n",
    "common_idx = X.index.intersection(df.index)\n",
    "X = X.loc[common_idx]\n",
    "y = y.loc[common_idx]\n",
    "\n",
    "# إزالة أي NaN\n",
    "mask = y.notna()\n",
    "X = X.loc[mask]\n",
    "y = y.loc[mask].astype(int)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ IsolationForest\n",
    "# ------------------------------\n",
    "iso = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "iso.fit(X_scaled)\n",
    "feat['anomaly_score'] = iso.score_samples(X_scaled)\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Evaluate Models\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    threshold_opt = 0.5\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name}: {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Alerts\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts_idx = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "alert_df = feat.iloc[alerts_idx].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts_idx]\n",
    "alert_df['pred_label'] = 1\n",
    "alert_df['high_risk'] = alert_df['anomaly_score'] < -0.5\n",
    "\n",
    "os.makedirs('../alerts', exist_ok=True)\n",
    "alert_file = '../alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported: {alert_file}\")\n",
    "print(f\"High-risk alerts count: {alert_df['high_risk'].sum()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Plot ROC & PR\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 13️⃣ Save Models\n",
    "# ------------------------------\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols, 'iso_model': iso}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, '../models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a786c751-7e34-4956-b371-5787d0a40ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import geoip2.database\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGBoost model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('../data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('../data/auth_features.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5, 'm')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask, 'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# GeoIP Features\n",
    "geoip_path = os.path.join('..', 'data', 'GeoLite2-City.mmdb')\n",
    "reader = geoip2.database.Reader(geoip_path)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        country = response.country.iso_code\n",
    "        city = response.city.name\n",
    "        return pd.Series([country, city])\n",
    "    except:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "feat[['geo_country','geo_city']] = feat['ip'].apply(geoip_features)\n",
    "reader.close()\n",
    "\n",
    "for col in ['geo_country','geo_city']:\n",
    "    le = LabelEncoder()\n",
    "    feat[col] = le.fit_transform(feat[col].astype(str))\n",
    "\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds',\n",
    "                'geo_country','geo_city'] + [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Align X and y\n",
    "common_idx = X.index.intersection(df.index)\n",
    "X = X.loc[common_idx]\n",
    "y = y.loc[common_idx]\n",
    "\n",
    "# Fill NaN in X and y\n",
    "X.fillna(X.median(), inplace=True)\n",
    "y.fillna(0, inplace=True)\n",
    "y = y.astype(int)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ IsolationForest\n",
    "# ------------------------------\n",
    "iso = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "iso.fit(X_scaled)\n",
    "feat['anomaly_score'] = iso.score_samples(X_scaled)\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Evaluate Models\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    threshold_opt = 0.5\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name}: {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Alerts\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts_idx = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "alert_df = feat.iloc[alerts_idx].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts_idx]\n",
    "alert_df['pred_label'] = 1\n",
    "alert_df['high_risk'] = alert_df['anomaly_score'] < -0.5\n",
    "\n",
    "os.makedirs('../alerts', exist_ok=True)\n",
    "alert_file = '../alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported: {alert_file}\")\n",
    "print(f\"High-risk alerts count: {alert_df['high_risk'].sum()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Plot ROC & PR\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 13️⃣ Save Models\n",
    "# ------------------------------\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols, 'iso_model': iso}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, '../models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0f91b-900b-42df-a576-7dfabe3f27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Align X و y بعد Feature Engineering\n",
    "# ------------------------------\n",
    "common_idx = X.index.intersection(df.index)\n",
    "X = X.loc[common_idx]\n",
    "y = df.loc[common_idx, 'result_bin']\n",
    "\n",
    "# تأكد من عدم وجود NaN\n",
    "X = X.fillna(X.median())\n",
    "y = y.fillna(0).astype(int)\n",
    "\n",
    "# تحويل إلى مصفوفة numpy مع Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# التأكد قبل SMOTE\n",
    "print(\"✅ Check NaN in X_scaled:\", np.isnan(X_scaled).any())\n",
    "print(\"✅ Check NaN in y:\", y.isna().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6e693-ff43-4a03-ac0c-825a3298c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ملء القيم المفقودة بالقيمة المتوسطة لكل عمود\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# استبدال القيم Inf أو -Inf بأقصى/أدنى قيمة ممكنة\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X = X.fillna(X.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0047882-2f21-4680-a674-0112bec5c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# إزالة الأعمدة التي لها نفس القيمة في كل الصفوف\n",
    "X = X.loc[:, X.nunique() > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c21ce-2ac9-4b6b-9d37-7b6668e08898",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375fd87-7173-4b67-81c8-f2f94f40b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import geoip2.database\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "\n",
    "# --- Time Features ---\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Failed streak ---\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "# --- Unique users last 5 minutes (safe for text) ---\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5, 'm')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask, 'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "# --- Average interarrival time ---\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "\n",
    "# --- Log1p transformations ---\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader('data/GeoLite2-City.mmdb')\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        city = response.city.name or \"unknown\"\n",
    "        country = response.country.iso_code or \"unknown\"\n",
    "        return pd.Series([city, country])\n",
    "    except:\n",
    "        return pd.Series([\"unknown\",\"unknown\"])\n",
    "\n",
    "feat[['geo_city','geo_country']] = feat['ip'].apply(geoip_features)\n",
    "\n",
    "# --- Clean temporary columns ---\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds'] + \\\n",
    "               [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Align indices and handle missing values\n",
    "common_idx = X.index.intersection(df.index)\n",
    "X = X.loc[common_idx].fillna(X.median())\n",
    "y = y.loc[common_idx].fillna(0).astype(int)\n",
    "\n",
    "# Remove constant columns\n",
    "X = X.loc[:, X.nunique() > 1]\n",
    "\n",
    "# Replace Inf\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.fillna(X.median(), inplace=True)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Default threshold 0.5\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    \n",
    "    # Precision-Recall AUC\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    \n",
    "    # Find threshold for recall>=0.8 and precision>=0.3\n",
    "    threshold_opt = 0.5\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Simple Alert System\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "alert_df = feat.iloc[alerts].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts]\n",
    "alert_df['pred_label'] = 1\n",
    "\n",
    "os.makedirs('alerts', exist_ok=True)\n",
    "alert_file = 'alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported to CSV: {alert_file}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Models and Scaler\n",
    "# ------------------------------\n",
    "os.makedirs('models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': X.columns.tolist()}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, 'models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72a265-3090-42dc-9e35-a64c3d267d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geoip2.database\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv('../data/auth_parsed_large.csv', parse_dates=['timestamp'])\n",
    "feat = pd.read_csv('../data/auth_features_large.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering (Full)\n",
    "# ------------------------------\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5, 'm')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask, 'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# GeoIP Features\n",
    "# ------------------------------\n",
    "reader = geoip2.database.Reader('../data/GeoLite2-City.mmdb')\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        resp = reader.city(ip)\n",
    "        country = resp.country.iso_code if resp.country.iso_code else 'XX'\n",
    "        city = resp.city.name if resp.city.name else 'Unknown'\n",
    "        return pd.Series([country, city])\n",
    "    except:\n",
    "        return pd.Series(['XX','Unknown'])\n",
    "\n",
    "feat[['geo_country','geo_city']] = feat['ip'].apply(geoip_features)\n",
    "\n",
    "# Encode categorical GeoIP features\n",
    "feat = pd.get_dummies(feat, columns=['geo_country','geo_city'], drop_first=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds'] + \\\n",
    "               [f'log1p_{col}' for col in log_cols]\n",
    "\n",
    "# Include new dummy columns\n",
    "numeric_cols += [c for c in feat.columns if 'geo_' in c]\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# Remove rows with NaN in X or y\n",
    "valid_idx = X.notna().all(axis=1) & y.notna()\n",
    "X = X.loc[valid_idx]\n",
    "y = y.loc[valid_idx]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    # Threshold optimization\n",
    "    threshold_opt = 0.5\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold: {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ IsolationForest for unusual behavior\n",
    "# ------------------------------\n",
    "iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso.fit(X_train)\n",
    "iso_scores = iso.decision_function(X_test)\n",
    "iso_anomalies = np.where(iso_scores < 0)[0]\n",
    "print(f\"⚠️ IsolationForest detected {len(iso_anomalies)} anomalous events.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Alerts\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "alert_df = feat.iloc[alerts].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts]\n",
    "alert_df['pred_label'] = 1\n",
    "\n",
    "os.makedirs('../alerts', exist_ok=True)\n",
    "alert_file = '../alerts/failed_events_alerts.csv'\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported: {alert_file}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Models and Scaler\n",
    "# ------------------------------\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "save_dict = {'model_rf': rf, 'scaler': scaler, 'features': numeric_cols}\n",
    "if xgb_available:\n",
    "    save_dict['model_xgb'] = xgb\n",
    "\n",
    "joblib.dump(save_dict, '../models/models_pipeline.joblib')\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a6f30-58c8-48fa-9d83-6e3c2835f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 0️⃣ إعداد المسارات الديناميكية\n",
    "# ------------------------------\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # لو تشغيل من .py\n",
    "DATA_DIR = os.path.join(BASE_DIR, '../data')\n",
    "ALERTS_DIR = os.path.join(BASE_DIR, '../alerts')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, '../models')\n",
    "\n",
    "os.makedirs(ALERTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import geoip2.database\n",
    "\n",
    "# Try to import XGBoost if installed\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering (Full)\n",
    "# ------------------------------\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "feat['failed'] = (feat['event_result'] == 'failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5, 'm')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask, 'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5, min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        return pd.Series({\n",
    "            'geo_city': response.city.name or 'Unknown',\n",
    "            'geo_country': response.country.iso_code or 'Unknown',\n",
    "            'geo_lat': response.location.latitude or 0,\n",
    "            'geo_lon': response.location.longitude or 0\n",
    "        })\n",
    "    except:\n",
    "        return pd.Series({\n",
    "            'geo_city': 'Unknown',\n",
    "            'geo_country': 'Unknown',\n",
    "            'geo_lat': 0,\n",
    "            'geo_lon': 0\n",
    "        })\n",
    "\n",
    "geo_df = feat['ip'].apply(geoip_features)\n",
    "feat = pd.concat([feat, geo_df], axis=1)\n",
    "\n",
    "print(\"✅ Feature Engineering completed.\")\n",
    "print(feat.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds'] + \\\n",
    "               [f'log1p_{col}' for col in log_cols] + ['geo_lat','geo_lon']\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin']\n",
    "\n",
    "# حذف الصفوف التي فيها NaN في X أو y\n",
    "valid_idx = X.notna().all(axis=1) & y.notna()\n",
    "X = X.loc[valid_idx]\n",
    "y = y.loc[valid_idx].astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models and Find Best Threshold\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    threshold_opt = 0.5\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name} (recall>=0.8, precision>=0.3): {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Simple Alert System\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "alert_df = feat.iloc[alerts].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts]\n",
    "alert_df['pred_label'] = 1\n",
    "alert_file = os.path.join(ALERTS_DIR, 'failed_events_alerts.csv')\n",
    "alert_df.to_csv(alert_file, index=False)\n",
    "print(f\"✅ Alerts exported to CSV: {alert_file}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Plot ROC & PR Curves\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Model & Scaler\n",
    "# ------------------------------\n",
    "joblib.dump(rf, os.path.join(MODELS_DIR, 'rf_model.joblib'))\n",
    "joblib.dump(scaler, os.path.join(MODELS_DIR, 'scaler.joblib'))\n",
    "if xgb_available:\n",
    "    joblib.dump(xgb, os.path.join(MODELS_DIR, 'xgb_model.joblib'))\n",
    "print(\"✅ Models and scaler saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d492a-8d82-4bab-a002-52a76fe27de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 0️⃣ إعداد المسارات الديناميكية\n",
    "# ------------------------------\n",
    "import os\n",
    "BASE_DIR = os.getcwd()      # مجلد العمل الحالي\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "ALERTS_DIR = os.path.join(BASE_DIR, 'alerts')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "os.makedirs(ALERTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Import Libraries\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GeoIP\n",
    "import geoip2.database\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except ModuleNotFoundError:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not installed, skipping XGB model.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# Map target to 0/1\n",
    "df['result_bin'] = df['result'].map({'success':0,'failed':1})\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Feature Engineering\n",
    "# ------------------------------\n",
    "# وقت وليل\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# فشل متسلسل\n",
    "feat['failed'] = (feat['event_result']=='failed').astype(int)\n",
    "feat['failed_streak'] = feat.groupby('ip')['failed'].cumsum() - feat['failed'].cumsum().where(feat['failed']==0).fillna(0)\n",
    "\n",
    "# عدد المستخدمين الفريدين آخر 5 دقائق لكل IP (آمن للنصوص)\n",
    "feat = feat.sort_values(['ip','timestamp'])\n",
    "def compute_unique_users_last_5m(group):\n",
    "    users = []\n",
    "    ts = group['timestamp'].values\n",
    "    for i in range(len(group)):\n",
    "        start_time = ts[i] - np.timedelta64(5,'m')\n",
    "        mask = (ts >= start_time) & (ts <= ts[i])\n",
    "        users.append(group.loc[mask,'event_user'].nunique())\n",
    "    return pd.Series(users, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = feat.groupby('ip', group_keys=False).apply(compute_unique_users_last_5m)\n",
    "\n",
    "# متوسط الفاصل بين المحاولات\n",
    "feat['prev_timestamp'] = feat.groupby('ip')['timestamp'].shift(1)\n",
    "feat['interarrival'] = (feat['timestamp'] - feat['prev_timestamp']).dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = feat.groupby('ip')['interarrival'].transform(lambda x: x.rolling(5,min_periods=1).mean())\n",
    "feat['avg_interarrival_seconds'].fillna(feat['avg_interarrival_seconds'].median(), inplace=True)\n",
    "\n",
    "# تحويلات لوغاريتمية لتخفيف القيم المتطرفة\n",
    "log_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count',\n",
    "            'total_count','unique_users_last_5m','avg_interarrival_seconds']\n",
    "\n",
    "for col in log_cols:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        resp = reader.city(ip)\n",
    "        return pd.Series({\n",
    "            'geo_lat': resp.location.latitude or 0.0,\n",
    "            'geo_lon': resp.location.longitude or 0.0,\n",
    "            'geo_country': hash(resp.country.iso_code) % 1000,  # simple encoding\n",
    "        })\n",
    "    except:\n",
    "        return pd.Series({'geo_lat':0.0,'geo_lon':0.0,'geo_country':0})\n",
    "geo_feat = feat['ip'].apply(geoip_features)\n",
    "feat = pd.concat([feat, geo_feat], axis=1)\n",
    "\n",
    "# تنظيف أعمدة مؤقتة\n",
    "feat.drop(columns=['failed','prev_timestamp','interarrival'], inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Prepare Data for Modeling\n",
    "# ------------------------------\n",
    "numeric_cols = ['cnt_last_1m','cnt_last_5m','cnt_last_15m','succ_count','fail_count','total_count',\n",
    "                'fail_rate','hour','is_night','failed_streak','unique_users_last_5m','avg_interarrival_seconds'] + \\\n",
    "               [f'log1p_{col}' for col in log_cols] + ['geo_lat','geo_lon','geo_country']\n",
    "\n",
    "X = feat[numeric_cols].astype(float)\n",
    "y = df['result_bin'].astype(float)\n",
    "\n",
    "# Handle missing values\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Handle Imbalanced Classes\n",
    "# ------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Train/Test Split\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Train RandomForest\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Train XGBoost (optional)\n",
    "# ------------------------------\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                        scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Evaluate Models\n",
    "# ------------------------------\n",
    "models = {'RandomForest': rf}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = xgb\n",
    "\n",
    "best_thresholds = {}\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    print(f\"\\n{name} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    prec, rec, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    threshold_opt = 0.5\n",
    "    for t, p, r in zip(thresholds, prec[:-1], rec[:-1]):\n",
    "        if r >= 0.8 and p >= 0.3:\n",
    "            threshold_opt = t\n",
    "            break\n",
    "    best_thresholds[name] = threshold_opt\n",
    "    print(f\"✔️ Best Threshold for {name}: {threshold_opt:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Alerts\n",
    "# ------------------------------\n",
    "threshold = best_thresholds['RandomForest']\n",
    "y_prob_test = rf.predict_proba(X_test)[:,1]\n",
    "alerts = np.where(y_prob_test >= threshold)[0]\n",
    "\n",
    "alert_df = feat.iloc[alerts].copy()\n",
    "alert_df['pred_prob'] = y_prob_test[alerts]\n",
    "alert_df['pred_label'] = 1\n",
    "alert_df.to_csv(os.path.join(ALERTS_DIR, 'failed_events_alerts.csv'), index=False)\n",
    "print(f\"✅ Alerts saved: {ALERTS_DIR}/failed_events_alerts.csv\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11️⃣ Plot ROC & PR\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve'); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=name)\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curve'); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 12️⃣ Save Models\n",
    "# ------------------------------\n",
    "joblib.dump({'model_rf': rf, 'scaler': scaler, 'features': numeric_cols,\n",
    "             **({'model_xgb': xgb} if xgb_available else {})},\n",
    "            os.path.join(MODELS_DIR, 'models_pipeline.joblib'))\n",
    "print(\"✅ Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a669dd-2725-4364-a499-be55455f4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# المسار الحالي للنوتبوك أو السكريبت\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))  # .. للرجوع لمجلد المشروع الرئيسي\n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "ALERTS_DIR = os.path.join(BASE_DIR, 'alerts')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "# ملفاتك\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# تحقق من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e25c8e-8e33-44c2-8095-fbbaef48b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTH_PARSED_FILE = 'data/auth_parsed_large.csv'\n",
    "AUTH_FEATURES_FILE = 'data/auth_features_large.csv'\n",
    "GEOIP_FILE = 'data/GeoLite2-City.mmdb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9bec9-8bbe-4a22-a26f-ca560388ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c904ba6-b59c-4147-a8b7-707071a5b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1️⃣ استيراد المكتبات\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# للـ GeoIP\n",
    "import geoip2.database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc364b-db8a-4c8e-a9f5-a9daca104f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa862a-c6cb-46be-aea0-8180e268bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOIP_FILE = 'data/GeoLite2-City.mmdb'\n",
    "AUTH_PARSED_FILE = 'data/auth_parsed_large.csv'\n",
    "AUTH_FEATURES_FILE = 'data/auth_features_large.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b401109-12e8-4c7c-929a-09b7d3274119",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaba68-00b1-4b4c-9f8c-bdd17561d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOIP_FILE = 'data/GeoLite2-City.mmdb'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92863e-cc4a-4bf1-91d4-fb8b520ae901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoip2.database\n",
    "\n",
    "GEOIP_FILE = 'data/GeoLite2-City.mmdb'\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"GeoIP database loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140fd78-7fcd-4eeb-8bae-39817e0a508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geoip2.database\n",
    "\n",
    "# الحصول على مسار المجلد الحالي للنوتبوك\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "# بناء المسار الكامل لقاعدة GeoIP\n",
    "GEOIP_FILE = os.path.join(CURRENT_DIR, 'data', 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملف\n",
    "if not os.path.exists(GEOIP_FILE):\n",
    "    raise FileNotFoundError(f\"❌ الملف غير موجود: {GEOIP_FILE}\")\n",
    "\n",
    "# فتح القاعدة\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"✅ GeoIP database loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81962815-f7b3-4706-b0ac-e8c8991ee954",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTH_PARSED_FILE = 'data/auth_parsed_large.csv'\n",
    "AUTH_FEATURES_FILE = 'data/auth_features_large.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247264a2-548d-4e57-b550-19f5751deff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد البيئة والمسارات\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# مجلد المشروع الرئيسي\n",
    "BASE_DIR = os.getcwd()  # نفترض أنك فتحت النوتبوك من ~/projects/login-anomaly/\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "ALERTS_DIR = os.path.join(BASE_DIR, 'alerts')\n",
    "\n",
    "# مسارات الملفات\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "print(\"✅ البيانات تم تحميلها بنجاح!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ تحميل قاعدة GeoIP\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"✅ قاعدة GeoIP تم تحميلها بنجاح!\")\n",
    "\n",
    "# مثال: دالة لإضافة ميزات GeoIP من عنوان IP\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        return {\n",
    "            'geo_country': response.country.iso_code or 'NA',\n",
    "            'geo_city': response.city.name or 'NA',\n",
    "            'geo_latitude': response.location.latitude or 0,\n",
    "            'geo_longitude': response.location.longitude or 0\n",
    "        }\n",
    "    except:\n",
    "        return {'geo_country': 'NA', 'geo_city': 'NA', 'geo_latitude': 0, 'geo_longitude': 0}\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ معالجة البيانات\n",
    "# ==============================\n",
    "# مثال: إضافة أعمدة لوغاريتمية للميزات العددية\n",
    "for col in ['some_numeric_column1', 'some_numeric_column2']:  # عدل حسب الأعمدة لديك\n",
    "    if col in feat.columns:\n",
    "        feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ تحضير بيانات التدريب\n",
    "# ==============================\n",
    "# افترض أن X و y تم إنشاؤهما من df و feat\n",
    "X = feat.select_dtypes(include=[np.number])  # جميع الأعمدة العددية\n",
    "y = df['result_bin'].astype('Int64')  # عمود الهدف\n",
    "\n",
    "# إزالة أي صفوف فيها NaN\n",
    "valid_idx = y.notna() & X.notna().all(axis=1)\n",
    "X = X.loc[valid_idx]\n",
    "y = y.loc[valid_idx]\n",
    "\n",
    "print(f\"✅ بعد إزالة الصفوف الفارغة: {X.shape[0]} صفوف، {X.shape[1]} أعمدة.\")\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ تحجيم البيانات\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ تم تحجيم البيانات.\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ معالجة التوازن بين الفئات\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "print(\"✅ تم معالجة توازن الفئات.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ تقسيم البيانات إلى تدريب واختبار\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "print(f\"✅ تم تقسيم البيانات: تدريب {X_train.shape[0]}، اختبار {X_test.shape[0]}.\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ تدريب نموذج XGBoost\n",
    "# ==============================\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"✅ تم تدريب نموذج XGBoost.\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ حفظ النموذج المدرب\n",
    "# ==============================\n",
    "import joblib\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'models', 'xgb_model.joblib')\n",
    "os.makedirs(os.path.dirname(MODEL_FILE), exist_ok=True)\n",
    "joblib.dump(xgb_model, MODEL_FILE)\n",
    "print(f\"✅ تم حفظ النموذج في: {MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023db3ad-9b31-47dd-a2f4-da9c7f9585f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد المسارات الديناميكية\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"⚠️ XGBoost غير مثبت، سيتم تخطيه.\")\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "ALERTS_DIR = os.path.join(BASE_DIR, 'alerts')\n",
    "\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ فتح قاعدة GeoIP\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"✅ GeoIP database loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ استخراج ميزات GeoIP\n",
    "# ==============================\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        return pd.Series({\n",
    "            'geo_country': response.country.iso_code,\n",
    "            'geo_city': response.city.name,\n",
    "            'geo_latitude': response.location.latitude,\n",
    "            'geo_longitude': response.location.longitude\n",
    "        })\n",
    "    except:\n",
    "        return pd.Series({\n",
    "            'geo_country': None,\n",
    "            'geo_city': None,\n",
    "            'geo_latitude': np.nan,\n",
    "            'geo_longitude': np.nan\n",
    "        })\n",
    "\n",
    "# مثال: إضافة ميزات IP\n",
    "# df[['geo_country','geo_city','geo_latitude','geo_longitude']] = df['ip'].apply(geoip_features)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ تجهيز البيانات للتعلم الآلي\n",
    "# ==============================\n",
    "valid_idx = df['result_bin'].notna()\n",
    "df = df.loc[valid_idx]\n",
    "feat = feat.loc[valid_idx]\n",
    "\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns\n",
    "X = feat[numeric_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ التعامل مع البيانات غير المتوازنة\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ تقسيم البيانات للتدريب والاختبار\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "print(\"✅ البيانات جاهزة للتدريب!\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ تدريب نموذج Random Forest\n",
    "# ==============================\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# حفظ النموذج\n",
    "joblib.dump(rf_model, os.path.join(DATA_DIR, 'final_random_forest_model.joblib'))\n",
    "print(\"✅ Random Forest model trained and saved!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ تدريب نموذج XGBoost (اختياري)\n",
    "# ==============================\n",
    "if XGB_AVAILABLE:\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    joblib.dump(xgb_model, os.path.join(DATA_DIR, 'final_xgb_model.joblib'))\n",
    "    print(\"✅ XGBoost model trained and saved!\")\n",
    "else:\n",
    "    print(\"⚠️ تم تخطي تدريب XGBoost.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe6f85-efe0-422d-8ccb-7ca6dc4027ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# المسار الصحيح للملف من مجلد المشروع\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'data', 'GeoLite2-City.mmdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9340a-9ccf-45c2-a982-42587c87f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()  # لو النوتبوك مفتوح من مجلد المشروع الرئيسي\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818aab7c-50d7-4fcb-bbfb-04ed0b3342db",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'data', 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'data', 'auth_features_large.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277b8b2-19d1-422b-b95a-17799be577a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    print(f, \"->\", os.path.exists(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88deff9e-9114-4121-b61f-54115c30065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 إعداد بيئة العمل بشكل ديناميكي\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import geoip2.database\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# مجلد المشروع الحالي\n",
    "BASE_DIR = os.getcwd()  \n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')  # كل الملفات ستكون هنا\n",
    "\n",
    "# أسماء الملفات\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 تحميل البيانات\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# فتح قاعدة GeoIP\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"✅ GeoIP database loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 ميزات إضافية\n",
    "# ==============================\n",
    "for col in ['count', 'success_count', 'fail_count']:\n",
    "    if col in feat.columns:\n",
    "        feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "# ==============================\n",
    "# 🔹 ميزات GeoIP\n",
    "# ==============================\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        country = response.country.iso_code or 'NA'\n",
    "        city = response.city.name or 'NA'\n",
    "        return country, city\n",
    "    except:\n",
    "        return 'NA', 'NA'\n",
    "\n",
    "# ==============================\n",
    "# 🔹 إعداد X و y\n",
    "# ==============================\n",
    "valid_idx = df['result_bin'].notna()\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n",
    "\n",
    "# اختيار الأعمدة الرقمية فقط\n",
    "X = feat.select_dtypes(include=[np.number])\n",
    "X = X.loc[valid_idx]\n",
    "X = X.fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 StandardScaler\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 التعامل مع Imbalanced Classes باستخدام SMOTE\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "print(f\"✅ بعد SMOTE، عدد العينات: {X_res.shape[0]}\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 تقسيم البيانات Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "print(f\"✅ Train/Test Split: {X_train.shape[0]}/{X_test.shape[0]}\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 تدريب نموذج RandomForest\n",
    "# ==============================\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"✅ نموذج RandomForest تم تدريبه بنجاح\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ==============================\n",
    "# 🔹 إغلاق GeoIP\n",
    "# ==============================\n",
    "reader.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a19c1-82ab-465a-87a5-a50273ef227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/auth_parsed_large.csv')\n",
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d76a68-b02f-47ea-a481-fa8535f67477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# تحويل العمود result إلى result_bin\n",
    "df['result_bin'] = df['result'].map({'success': 0, 'fail': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26449dcb-8e13-48db-8051-e3a3a0f00811",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = df['result_bin'].notna()\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a328a-7c64-4932-ad8a-01a497c1f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد المسارات الديناميكية\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import geoip2.database\n",
    "\n",
    "# تحديد مجلد المشروع\n",
    "BASE_DIR = os.path.expanduser('~/projects/login-anomaly')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "ALERTS_DIR = os.path.join(BASE_DIR, 'alerts')\n",
    "\n",
    "# ملفات البيانات\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ تحويل العمود result إلى ثنائي\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].map({'success': 0, 'fail': 1})\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ فتح قاعدة GeoIP\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"✅ GeoIP database loaded!\")\n",
    "\n",
    "# دالة استخراج معلومات GeoIP\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        country = response.country.iso_code\n",
    "        city = response.city.name\n",
    "    except:\n",
    "        country = 'NA'\n",
    "        city = 'NA'\n",
    "    return country, city\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ إعداد X و y\n",
    "# ==============================\n",
    "valid_idx = df['result_bin'].notna()\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n",
    "\n",
    "# اختيار الأعمدة الرقمية فقط من feat\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns\n",
    "X = feat.loc[valid_idx, numeric_cols]\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ قياس القيم وتوحيدها\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ معالجة عدم التوازن\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ تقسيم البيانات للتدريب والاختبار\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "print(\"✅ البيانات جاهزة للتدريب!\")\n",
    "# ==============================\n",
    "# 8️⃣ تدريب النموذج Random Forest\n",
    "# ==============================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# إنشاء النموذج\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200, random_state=42, class_weight='balanced'\n",
    ")\n",
    "\n",
    "# تدريب النموذج\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# تقييم النموذج\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# حفظ النموذج\n",
    "RF_MODEL_FILE = os.path.join(BASE_DIR, 'models/random_forest_model.joblib')\n",
    "os.makedirs(os.path.dirname(RF_MODEL_FILE), exist_ok=True)\n",
    "joblib.dump(rf_model, RF_MODEL_FILE)\n",
    "print(f\"✅ نموذج Random Forest محفوظ في: {RF_MODEL_FILE}\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ تدريب XGBoost (اختياري)\n",
    "# ==============================\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=200, random_state=42, use_label_encoder=False, eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    print(\"XGBoost Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    \n",
    "    XGB_MODEL_FILE = os.path.join(BASE_DIR, 'models/xgb_model.joblib')\n",
    "    joblib.dump(xgb_model, XGB_MODEL_FILE)\n",
    "    print(f\"✅ نموذج XGBoost محفوظ في: {XGB_MODEL_FILE}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⚠️ XGBoost غير مثبت، تخطي هذه الخطوة.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 ملاحظات\n",
    "# ==============================\n",
    "# - تأكد أن المجلد \"models\" موجود في المشروع، أو سيُنشأ تلقائياً.\n",
    "# - الآن يمكنك استخدام النماذج للتنبؤ على بيانات جديدة أو لاكتشاف محاولات تسجيل الدخول المشبوهة.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9680c-dc64-49d5-a04c-96e6c082af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(BASE_DIR, 'scripts/data')\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7169b-595e-4b0f-9a8f-ac904411cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 0️⃣ إعداد المسارات\n",
    "# =========================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import geoip2.database\n",
    "\n",
    "# تحديد مجلد المشروع الأساسي\n",
    "BASE_DIR = os.path.abspath('.')  # شغّل النوتبوك من login-anomaly\n",
    "\n",
    "# مسارات الملفات\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'scripts/data')\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# =========================================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# =========================================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# إنشاء متغير الهدف: result_bin (نجاح=0، فشل=1)\n",
    "df['result_bin'] = df['result'].map({'success': 0, 'failure': 1})\n",
    "\n",
    "# =========================================\n",
    "# 2️⃣ تحميل قاعدة GeoIP\n",
    "# =========================================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        return response.location.latitude, response.location.longitude\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# مثال على إضافة ميزات GeoIP (يمكن دمجها مع df أو feat)\n",
    "feat['geo_lat'], feat['geo_lon'] = zip(*feat['ip'].map(geoip_features))\n",
    "\n",
    "# =========================================\n",
    "# 3️⃣ تحضير X و y\n",
    "# =========================================\n",
    "# إزالة الصفوف التي فيها NaN في result_bin أو في أي ميزة مهمة\n",
    "valid_idx = df['result_bin'].notna()\n",
    "X = feat.loc[valid_idx].select_dtypes(include=[np.number])  # فقط الأعمدة الرقمية\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n",
    "\n",
    "# معالجة أي NaN في X\n",
    "X = X.fillna(0)\n",
    "\n",
    "# =========================================\n",
    "# 4️⃣ StandardScaler\n",
    "# =========================================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# =========================================\n",
    "# 5️⃣ التعامل مع Imbalanced Classes\n",
    "# =========================================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# =========================================\n",
    "# 6️⃣ تقسيم البيانات Train/Test\n",
    "# =========================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 7️⃣ تدريب نموذج Random Forest\n",
    "# =========================================\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# =========================================\n",
    "# 8️⃣ تقييم النموذج\n",
    "# =========================================\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# =========================================\n",
    "# 9️⃣ حفظ النموذج و Scaler\n",
    "# =========================================\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'scripts/models')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "joblib.dump(rf, os.path.join(MODEL_DIR, 'random_forest_model.joblib'))\n",
    "joblib.dump(scaler, os.path.join(MODEL_DIR, 'scaler.joblib'))\n",
    "\n",
    "print(\"✅ تم حفظ النموذج و Scaler بنجاح!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300724ac-8eca-4d4b-809d-02676544f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 0️⃣ إعداد المسارات الديناميكية\n",
    "# =========================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import geoip2.database\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# تحديد مسار الملفات\n",
    "BASE_DIR = os.path.join(os.getcwd(), 'scripts')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# =========================================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# =========================================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# =========================================\n",
    "# 2️⃣ تحميل GeoIP\n",
    "# =========================================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"✅ GeoIP database loaded successfully!\")\n",
    "\n",
    "# استخراج ميزات GeoIP\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        country = response.country.iso_code\n",
    "        city = response.city.name\n",
    "        lat = response.location.latitude\n",
    "        lon = response.location.longitude\n",
    "        return country, city, lat, lon\n",
    "    except:\n",
    "        return 'NA', 'NA', np.nan, np.nan\n",
    "\n",
    "df['country'], df['city'], df['lat'], df['lon'] = zip(*df['ip'].map(geoip_features))\n",
    "\n",
    "# =========================================\n",
    "# 3️⃣ تجهيز الهدف y\n",
    "# =========================================\n",
    "df['result_bin'] = df['result'].map(lambda x: 1 if x.lower() == 'fail' else 0)\n",
    "valid_idx = df['result_bin'].notna()\n",
    "df = df.loc[valid_idx].copy()\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# =========================================\n",
    "# 4️⃣ تجهيز الميزات X\n",
    "# =========================================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns\n",
    "X = feat.loc[valid_idx, numeric_cols]\n",
    "\n",
    "# تحويل اللوغاريتم لبعض الميزات (log1p)\n",
    "for col in numeric_cols:\n",
    "    X[f'log1p_{col}'] = np.log1p(X[col])\n",
    "\n",
    "# التعامل مع NaN\n",
    "X = X.fillna(0)\n",
    "\n",
    "# تطبيع الميزات\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# =========================================\n",
    "# 5️⃣ معالجة البيانات غير المتوازنة\n",
    "# =========================================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# =========================================\n",
    "# 6️⃣ تقسيم البيانات إلى Train/Test\n",
    "# =========================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# 7️⃣ تدريب نموذج RandomForest\n",
    "# =========================================\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# =========================================\n",
    "# 8️⃣ تقييم النموذج\n",
    "# =========================================\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# =========================================\n",
    "# 9️⃣ رسم Precision / Recall حسب Threshold\n",
    "# =========================================\n",
    "y_scores = rf.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(thresholds, precision[:-1], label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision & Recall vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# =========================================\n",
    "# 🔟 حفظ النموذج\n",
    "# =========================================\n",
    "MODEL_FILE = os.path.join(DATA_DIR, 'final_random_forest_model.joblib')\n",
    "joblib.dump(rf, MODEL_FILE)\n",
    "print(f\"✅ النموذج تم حفظه في: {MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ff382-162d-47d7-8000-f228f50745e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.join(os.getcwd(), 'scripts')  # <-- هذه فقط scripts وليس scripts/scripts\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144308a-09fa-4cfe-955d-62c8c67af938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد المسارات\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# المسار الأساسي للمشروع\n",
    "BASE_DIR = os.path.join(os.getcwd(), 'scripts')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# إذا لم يكن هناك عمود 'result_bin' ننشئه من 'result'\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].map({'success': 0, 'failure': 1})\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ التعامل مع القيم المفقودة\n",
    "# ==============================\n",
    "# إزالة الصفوف التي فيها result_bin فارغ\n",
    "valid_idx = df['result_bin'].notna()\n",
    "df = df.loc[valid_idx].reset_index(drop=True)\n",
    "feat = feat.loc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "# ملء أي قيم NaN في الميزات بالقيمة الصفرية (يمكن تغييره حسب نوع البيانات)\n",
    "feat = feat.fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ إعداد X و y\n",
    "# ==============================\n",
    "X = feat.select_dtypes(include=[np.number])  # اختيار الأعمدة الرقمية فقط\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Scaling\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ التعامل مع التوازن باستخدام SMOTE\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ تقسيم البيانات Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ بناء نموذج Random Forest\n",
    "# ==============================\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=None, random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ تقييم النموذج\n",
    "# ==============================\n",
    "train_score = rf_model.score(X_train, y_train)\n",
    "test_score = rf_model.score(X_test, y_test)\n",
    "print(f\"✅ Random Forest Train Accuracy: {train_score:.4f}\")\n",
    "print(f\"✅ Random Forest Test Accuracy: {test_score:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ حفظ النموذج و الـ Scaler\n",
    "# ==============================\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "joblib.dump(rf_model, os.path.join(MODEL_DIR, 'random_forest_model.joblib'))\n",
    "joblib.dump(scaler, os.path.join(MODEL_DIR, 'scaler.joblib'))\n",
    "print(\"✅ النموذج والـ Scaler تم حفظهم بنجاح!\")\n",
    "\n",
    "# ==============================\n",
    "#  🔹 ملاحظات إضافية\n",
    "# ==============================\n",
    "# - يمكنك إضافة XGBoost بنفس الطريقة بعد استدعاء xgboost.XGBClassifier\n",
    "# - أي Feature Engineering إضافي يمكن إضافته قبل Step 3\n",
    "# - إذا أردت استخدام GeoIP، تأكد من تحميل geoip2 واستخدام المسار الصحيح للملف\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af039a67-6f52-4c53-9a68-0eebeeb6a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'data')  # مسار ثابت\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5f079-0c7d-4e64-b063-5b9b2a8a0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 0️⃣ إعداد المسارات الديناميكية\n",
    "# =========================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import geoip2.database\n",
    "\n",
    "# مسار مجلد البيانات\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# الملفات المهمة\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# =========================================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# =========================================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# =========================================\n",
    "# 2️⃣ إعداد GeoIP\n",
    "# =========================================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        country = response.country.iso_code\n",
    "        city = response.city.name\n",
    "        return country, city\n",
    "    except:\n",
    "        return 'NA', 'NA'\n",
    "\n",
    "# =========================================\n",
    "# 3️⃣ إعداد الهدف (target)\n",
    "# =========================================\n",
    "# تحويل result إلى 0/1\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x.lower() == 'failure' else 0)\n",
    "\n",
    "# =========================================\n",
    "# 4️⃣ إعداد X و y\n",
    "# =========================================\n",
    "valid_idx = df['result_bin'].notna()\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n",
    "\n",
    "# اختيار الأعمدة الرقمية فقط\n",
    "X = feat.select_dtypes(include=[np.number])\n",
    "X = X.loc[valid_idx]\n",
    "\n",
    "# -----------------------------------------\n",
    "# قياس وتوحيد البيانات\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# =========================================\n",
    "# 5️⃣ معالجة عدم توازن البيانات\n",
    "# =========================================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# =========================================\n",
    "# 6️⃣ تقسيم البيانات إلى Train / Test\n",
    "# =========================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "print(\"✅ البيانات جاهزة للتدريب!\")\n",
    "\n",
    "# =========================================\n",
    "# يمكن الآن تدريب نموذج مثل RandomForest أو XGBoost\n",
    "# =========================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import dump\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# حفظ النموذج\n",
    "MODEL_PATH = os.path.join(DATA_DIR, 'random_forest_model.joblib')\n",
    "dump(model, MODEL_PATH)\n",
    "print(f\"✅ تم حفظ النموذج: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a81e94-12ee-4b42-8c8a-aa95a50de5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ✅ إعداد المسارات والملفات\n",
    "# =========================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# تأكد أنك فتحت النوتبوك من مجلد المشروع الرئيسي\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "ALERTS_DIR = os.path.join(BASE_DIR, 'alerts')\n",
    "\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# =========================================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# =========================================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# إنشاء العمود الثنائي result_bin بناءً على 'result'\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x.lower() == 'failure' else 0)\n",
    "\n",
    "# =========================================\n",
    "# 2️⃣ GeoIP Features (اختياري)\n",
    "# =========================================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        return response.location.latitude, response.location.longitude\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# مثال إضافة أعمدة latitude و longitude\n",
    "df['latitude'], df['longitude'] = zip(*df['ip'].apply(geoip_features))\n",
    "\n",
    "# =========================================\n",
    "# 3️⃣ اختيار X و y\n",
    "# =========================================\n",
    "# إزالة الصفوف التي فيها result_bin فارغ\n",
    "valid_idx = df['result_bin'].notna()\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n",
    "\n",
    "# اختيار الأعمدة الرقمية فقط من feat\n",
    "X = feat.select_dtypes(include=np.number)\n",
    "X = X.loc[valid_idx]\n",
    "\n",
    "# التأكد من عدم وجود NaN\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# =========================================\n",
    "# 4️⃣ Scale البيانات\n",
    "# =========================================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# =========================================\n",
    "# 5️⃣ التعامل مع Imbalanced Classes\n",
    "# =========================================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# =========================================\n",
    "# 6️⃣ Train/Test Split\n",
    "# =========================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "print(\"✅ البيانات جاهزة للتدريب\")\n",
    "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e71f5f-e1ec-4ffc-9573-9d12042aee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد المسارات الديناميكية\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import geoip2.database\n",
    "\n",
    "# المسار الأساسي (مجلد login-anomaly)\n",
    "BASE_DIR = os.getcwd()  # تشغيل النوتبوك من login-anomaly/\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "ALERTS_DIR = os.path.join(BASE_DIR, 'alerts')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "# ملفات البيانات\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# فتح قاعدة GeoIP\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"✅ قاعدة GeoIP جاهزة!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ تجهيز العمود الهدف\n",
    "# ==============================\n",
    "# تحويل result إلى 0/1\n",
    "df['result_bin'] = df['result'].map({'success': 0, 'failure': 1})\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ اختيار الخصائص\n",
    "# ==============================\n",
    "# اختيار الأعمدة الرقمية فقط\n",
    "X = feat.select_dtypes(include=np.number)\n",
    "\n",
    "# إزالة الصفوف التي فيها NaN في الهدف\n",
    "valid_idx = df['result_bin'].notna()\n",
    "X = X.loc[valid_idx]\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Scaling للبيانات\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ معالجة عدم توازن الفئات\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ تقسيم البيانات Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ تدريب نموذج RandomForest\n",
    "# ==============================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ حفظ النموذج\n",
    "# ==============================\n",
    "import joblib\n",
    "MODEL_FILE = os.path.join(MODELS_DIR, 'random_forest_model.joblib')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "joblib.dump(rf, MODEL_FILE)\n",
    "print(f\"✅ تم حفظ النموذج في: {MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa4289-25f8-4096-9b12-d4995fbffafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 0️⃣ إعداد المسارات والبيئة\n",
    "# =========================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import geoip2.database\n",
    "\n",
    "# مجلد التشغيل الحالي\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "\n",
    "# ملفات البيانات\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# =========================================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# =========================================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# =========================================\n",
    "# 2️⃣ تحميل GeoIP Database\n",
    "# =========================================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"✅ قاعدة GeoIP جاهزة!\")\n",
    "\n",
    "# مثال دالة استخراج بيانات GeoIP (IP -> Country, City)\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        resp = reader.city(ip)\n",
    "        country = resp.country.iso_code if resp.country.iso_code else 'NA'\n",
    "        city = resp.city.name if resp.city.name else 'NA'\n",
    "    except:\n",
    "        country, city = 'NA', 'NA'\n",
    "    return country, city\n",
    "\n",
    "# =========================================\n",
    "# 3️⃣ إعداد الهدف y والميزات X\n",
    "# =========================================\n",
    "# إنشاء عمود ثنائي من العمود 'result'\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x.lower() == 'failure' else 0)\n",
    "\n",
    "# فلترة الصفوف الغير صالحة\n",
    "valid_idx = df['result_bin'].notna()\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n",
    "\n",
    "# اختيار الأعمدة الرقمية فقط من feat\n",
    "X = feat.select_dtypes(include=[np.number])\n",
    "X = X.loc[valid_idx]\n",
    "\n",
    "# =========================================\n",
    "# 4️⃣ Standard Scaling\n",
    "# =========================================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# =========================================\n",
    "# 5️⃣ معالجة البيانات غير المتوازنة باستخدام SMOTE\n",
    "# =========================================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# =========================================\n",
    "# 6️⃣ Train/Test Split\n",
    "# =========================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "print(\"✅ جميع البيانات جاهزة للتدريب!\")\n",
    "print(f\"Training samples: {len(y_train)}, Testing samples: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e740c-2712-469e-b3e1-4618f4606c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد المسارات الثابتة\n",
    "# ==============================\n",
    "AUTH_PARSED_FILE = '/home/bakri/projects/login-anomaly/data/auth_parsed_large.csv'\n",
    "AUTH_FEATURES_FILE = '/home/bakri/projects/login-anomaly/data/auth_features_large.csv'\n",
    "GEOIP_FILE = '/home/bakri/projects/login-anomaly/data/GeoLite2-City.mmdb'\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "import os\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Import Libraries\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# فتح قاعدة GeoIP\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "print(\"✅ GeoIP database loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ إعداد y\n",
    "# ==============================\n",
    "# تحويل العمود 'result' إلى binary (نجاح=0، فشل=1)\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x == 'failure' else 0)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ إعداد X و y\n",
    "# ==============================\n",
    "valid_idx = df['result_bin'].notna()\n",
    "y = df.loc[valid_idx, 'result_bin'].astype(int)\n",
    "\n",
    "# اختيار الأعمدة الرقمية فقط\n",
    "X = feat.select_dtypes(include=np.number).loc[valid_idx]\n",
    "\n",
    "# قياس المقياس\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ التعامل مع عدم توازن البيانات\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ تقسيم البيانات Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ تدريب نموذج Random Forest\n",
    "# ==============================\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ تقييم النموذج\n",
    "# ==============================\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ef8c5-89d2-4c0e-b551-b3c0b01c1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b8a49-3a47-4321-9f17-9cdb78c6a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if len(np.unique(y)) > 1:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "else:\n",
    "    print(\"⚠️ لا توجد إلا فئة واحدة في y، سيتم استخدام البيانات الأصلية بدون SMOTE\")\n",
    "    X_res, y_res = X_scaled, y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res if len(np.unique(y_res)) > 1 else None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45853d7-df20-4cd2-8b2c-d35ea5f44a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد المسارات الديناميكية\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'  # المسار الصحيح للبيانات\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# ------------------------------\n",
    "# التأكد من وجود الملفات\n",
    "# ------------------------------\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ تجهيز y و result_bin\n",
    "# ==============================\n",
    "# إضافة عمود result_bin إذا لم يكن موجودًا\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x == 'success' else 0)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ تجهيز X\n",
    "# ==============================\n",
    "# اختيار الأعمدة الرقمية فقط\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns\n",
    "X = feat[numeric_cols].fillna(0)  # ملء القيم المفقودة بصفر\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Scale البيانات\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ التعامل مع عدم توازن البيانات\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ تم تطبيق SMOTE على البيانات.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ لا توجد إلا فئة واحدة في y، سيتم استخدام البيانات الأصلية بدون SMOTE.\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ تقسيم البيانات Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ تدريب نموذج Random Forest\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ تم تدريب النموذج بنجاح!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ تقييم النموذج\n",
    "# ==============================\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy على بيانات الاختبار: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd9857-77ab-4515-98b0-de18227bb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد المسارات\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# التأكد من وجود الملفات\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "print(\"✅ كل الملفات موجودة!\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ قراءة البيانات\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ إعداد target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ استخراج ميزات GeoIP\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "# دمج الميزات مع df\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ تجهيز X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# إضافة إحداثيات GeoIP\n",
    "X['lat'] = df['lat'].fillna(0)\n",
    "X['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale البيانات\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ التعامل مع عدم توازن البيانات\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ تم تطبيق SMOTE على البيانات.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ لا توجد إلا فئة واحدة في y، سيتم استخدام البيانات الأصلية بدون SMOTE.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ تقسيم البيانات Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ تدريب نموذج Random Forest\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ تم تدريب النموذج بنجاح!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ تقييم النموذج\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 تحليل الفئات والميزات\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"توزيع الفئات في y\")\n",
    "plt.show()\n",
    "\n",
    "# أهم الميزات\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"أهم 10 ميزات للنموذج\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 حفظ النموذج\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump(clf, MODEL_PATH)\n",
    "print(f\"✅ تم حفظ النموذج في: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c03bea-d409-4643-8e83-2f52befe975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "# Convert 'result' column to binary target\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Extract GeoIP Features\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "# Merge GeoIP features with main dataframe\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# Add GeoIP coordinates\n",
    "X['lat'] = df['lat'].fillna(0)\n",
    "X['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10 important features\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump(clf, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb13dd-09b6-480f-b2ed-2f481d08b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Pipeline: Login Anomaly Detection\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check file existence\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All required files found.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x == 'success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (New)\n",
    "# ==============================\n",
    "\n",
    "# ---- Time-based Features ----\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# ---- Failed streaks per IP ----\n",
    "df['failed_streak'] = (\n",
    "    (df['result'] == 'failed')\n",
    "    .groupby(df['ip'])\n",
    "    .cumsum()\n",
    ")\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0)\n",
    "\n",
    "# ---- Unique users attempted by same IP in last 5 minutes ----\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "feat['unique_users_last_5m'] = (\n",
    "    df_sorted\n",
    "    .groupby('ip')['user']\n",
    "    .transform(lambda x: x.rolling('5min', on=df_sorted['timestamp']).apply(lambda s: len(set(s)), raw=False))\n",
    "    .fillna(1)\n",
    ")\n",
    "\n",
    "# ---- Average time gap between attempts per IP ----\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "avg_time = df_sorted.groupby('ip')['time_diff'].transform(lambda x: x.fillna(x.median()))\n",
    "feat['avg_interarrival_seconds'] = avg_time.fillna(0)\n",
    "\n",
    "# ---- GeoIP Features ----\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        return pd.Series([r.country.iso_code, r.city.name, r.location.latitude, r.location.longitude])\n",
    "    except:\n",
    "        return pd.Series(['NA', 'NA', np.nan, np.nan])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "\n",
    "# ---- Add Geo coordinates to features ----\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# ---- Log transformations to reduce skewness ----\n",
    "for col in ['failed_streak', 'unique_users_last_5m', 'avg_interarrival_seconds']:\n",
    "    feat[f'log1p_{col}'] = np.log1p(feat[col])\n",
    "\n",
    "print(\"✅ Feature engineering completed successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols:\n",
    "    numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res)) > 1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n🔹 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ---- Precision-Recall Curve ----\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# ---- Feature Importance ----\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved successfully at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31277bd7-6375-400b-9f61-4568cac6fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# ✅ Full Script: Login Anomaly Detection (with advanced features)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check file existence\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files found.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x == 'success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ GeoIP Features\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        return pd.Series([\n",
    "            r.country.iso_code,\n",
    "            r.city.name,\n",
    "            r.location.latitude,\n",
    "            r.location.longitude\n",
    "        ])\n",
    "    except:\n",
    "        return pd.Series(['NA', 'NA', np.nan, np.nan])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "print(\"✅ GeoIP features extracted.\")\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0, 1, 2, 3, 4, 5, 23]).astype(int)\n",
    "\n",
    "# ---- Unique users attempted by same IP in the last 5 minutes ----\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "unique_users_last_5m = []\n",
    "\n",
    "for ip, group in df_sorted.groupby('ip'):\n",
    "    users_in_window = []\n",
    "    for i in range(len(group)):\n",
    "        current_time = group.iloc[i]['timestamp']\n",
    "        mask = (group['timestamp'] >= current_time - pd.Timedelta(minutes=5)) & (group['timestamp'] <= current_time)\n",
    "        unique_count = group.loc[mask, 'user'].nunique()\n",
    "        users_in_window.append(unique_count)\n",
    "    unique_users_last_5m.extend(users_in_window)\n",
    "\n",
    "feat['unique_users_last_5m'] = unique_users_last_5m\n",
    "print(\"✅ Added feature: unique_users_last_5m\")\n",
    "\n",
    "# ---- Average time gap between attempts per IP ----\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = (\n",
    "    df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    ")\n",
    "\n",
    "# Add GeoIP numeric fields\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Features (X)\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res)) > 1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split completed.\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=None,\n",
    "    random_state=42, class_weight='balanced'\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model training completed!\")\n",
    "\n",
    "# ==============================\n",
    "# 🔟 Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 💾 Save Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07916ee7-530e-4a53-bf0e-b56e1fa1608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# ✅ FULL SCRIPT: LOGIN ANOMALY DETECTION (FINAL)\n",
    "# ===============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "# ===============================================\n",
    "# 0️⃣ SETUP PATHS\n",
    "# ===============================================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if all files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files found successfully!\")\n",
    "\n",
    "# ===============================================\n",
    "# 1️⃣ LOAD DATA\n",
    "# ===============================================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ===============================================\n",
    "# 2️⃣ PREPARE TARGET VARIABLE\n",
    "# ===============================================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x == 'success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ===============================================\n",
    "# 3️⃣ GEOIP FEATURES\n",
    "# ===============================================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        return pd.Series([\n",
    "            r.country.iso_code,\n",
    "            r.city.name,\n",
    "            r.location.latitude,\n",
    "            r.location.longitude\n",
    "        ])\n",
    "    except:\n",
    "        return pd.Series(['NA', 'NA', np.nan, np.nan])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "print(\"✅ GeoIP features extracted successfully!\")\n",
    "\n",
    "# ===============================================\n",
    "# 4️⃣ FEATURE ENGINEERING\n",
    "# ===============================================\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0, 1, 2, 3, 4, 5, 23]).astype(int)\n",
    "\n",
    "# ---- Unique users per IP in the last 5 minutes ----\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "unique_users_last_5m = []\n",
    "\n",
    "for ip, group in df_sorted.groupby('ip'):\n",
    "    users_in_window = []\n",
    "    for i in range(len(group)):\n",
    "        current_time = group.iloc[i]['timestamp']\n",
    "        mask = (group['timestamp'] >= current_time - pd.Timedelta(minutes=5)) & (group['timestamp'] <= current_time)\n",
    "        unique_count = group.loc[mask, 'user'].nunique()\n",
    "        users_in_window.append(unique_count)\n",
    "    unique_users_last_5m.extend(users_in_window)\n",
    "\n",
    "feat['unique_users_last_5m'] = unique_users_last_5m\n",
    "print(\"✅ Added feature: unique_users_last_5m\")\n",
    "\n",
    "# ---- Average time gap between attempts per IP ----\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = (\n",
    "    df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    ")\n",
    "\n",
    "# Add GeoIP numeric fields\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# ===============================================\n",
    "# 5️⃣ EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ===============================================\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=df['result'])\n",
    "plt.title(\"Login Attempts by Result (Success vs Failure)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['timestamp'].dt.hour, bins=24, kde=True)\n",
    "plt.title(\"Distribution of Login Attempts by Hour\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Number of Attempts\")\n",
    "plt.show()\n",
    "\n",
    "top_ips = df['ip'].value_counts().head(10)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=top_ips.values, y=top_ips.index)\n",
    "plt.title(\"Top 10 IPs with Most Login Attempts\")\n",
    "plt.xlabel(\"Number of Attempts\")\n",
    "plt.show()\n",
    "\n",
    "if 'country' in df.columns:\n",
    "    top_countries = df['country'].value_counts().head(10)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=top_countries.values, y=top_countries.index)\n",
    "    plt.title(\"Top Countries by Login Attempts\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ EDA visualizations completed!\")\n",
    "\n",
    "# ===============================================\n",
    "# 6️⃣ PREPARE FEATURES (X)\n",
    "# ===============================================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ===============================================\n",
    "# 7️⃣ SCALE FEATURES\n",
    "# ===============================================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ===============================================\n",
    "# 8️⃣ HANDLE IMBALANCED CLASSES\n",
    "# ===============================================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present. SMOTE skipped.\")\n",
    "\n",
    "# ===============================================\n",
    "# 9️⃣ TRAIN/TEST SPLIT\n",
    "# ===============================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res)) > 1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done.\")\n",
    "\n",
    "# ===============================================\n",
    "# 🔟 TRAIN RANDOM FOREST MODEL\n",
    "# ===============================================\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=None,\n",
    "    random_state=42, class_weight='balanced'\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model training completed!\")\n",
    "\n",
    "# ===============================================\n",
    "# 1️⃣1️⃣ EVALUATE MODEL\n",
    "# ===============================================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}', linewidth=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ===============================================\n",
    "# 1️⃣2️⃣ FEATURE IMPORTANCE\n",
    "# ===============================================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Most Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ===============================================\n",
    "# 1️⃣3️⃣ SAVE MODEL\n",
    "# ===============================================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved successfully at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91109710-403d-45ee-9469-90f632c12f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Login Anomaly Detection Full Pipeline\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check files\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All required files found!\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully.\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x == 'success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ GeoIP Features\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        return pd.Series({\n",
    "            'country': r.country.iso_code,\n",
    "            'city': r.city.name,\n",
    "            'lat': r.location.latitude,\n",
    "            'lon': r.location.longitude\n",
    "        })\n",
    "    except:\n",
    "        return pd.Series({'country': 'NA', 'city': 'NA', 'lat': np.nan, 'lon': np.nan})\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# Select numeric columns only\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Class Imbalance\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in target — SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res)) > 1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split completed!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n🔹 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 10️⃣ Save Model + Scaler\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "\n",
    "joblib.dump({\n",
    "    'model': clf,\n",
    "    'scaler': scaler,\n",
    "    'features': X.columns.tolist()\n",
    "}, MODEL_PATH)\n",
    "\n",
    "print(f\"✅ Model and scaler saved successfully at: {MODEL_PATH}\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 11️⃣ Load Model Example (optional)\n",
    "# ==============================\n",
    "data_loaded = joblib.load(MODEL_PATH)\n",
    "clf_loaded = data_loaded['model']\n",
    "scaler_loaded = data_loaded['scaler']\n",
    "print(\"✅ Model reloaded and ready to use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e842bb-8a74-441d-8eac-8a160647a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 12️⃣ Interactive Geo Map (optional)\n",
    "# ==============================\n",
    "\n",
    "import folium\n",
    "\n",
    "# Filter rows that have valid coordinates\n",
    "df_map = df.dropna(subset=['lat', 'lon'])\n",
    "df_map = df_map[(df_map['lat'] != 0) & (df_map['lon'] != 0)]\n",
    "\n",
    "if len(df_map) == 0:\n",
    "    print(\"⚠️ No valid coordinates to plot on map.\")\n",
    "else:\n",
    "    # Center map around the mean coordinates\n",
    "    avg_lat, avg_lon = df_map['lat'].mean(), df_map['lon'].mean()\n",
    "    m = folium.Map(location=[avg_lat, avg_lon], zoom_start=3)\n",
    "\n",
    "    # Add points to map\n",
    "    for _, row in df_map.iterrows():\n",
    "        color = 'green' if row['result_bin'] == 1 else 'red'\n",
    "        popup_text = f\"User: {row['user']}<br>IP: {row['ip']}<br>Country: {row['country']}\"\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['lon']],\n",
    "            radius=4,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            popup=popup_text\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Save and display map\n",
    "    map_path = os.path.join(BASE_DIR, 'login_geo_map.html')\n",
    "    m.save(map_path)\n",
    "    print(f\"✅ Interactive map saved at: {map_path}\")\n",
    "    m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9508891-abf0-4cc9-98c5-fa22c42afb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 12️⃣ Interactive Geo Map (optional)\n",
    "# ==============================\n",
    "\n",
    "import folium\n",
    "\n",
    "# Filter rows that have valid coordinates\n",
    "df_map = df.dropna(subset=['lat', 'lon'])\n",
    "df_map = df_map[(df_map['lat'] != 0) & (df_map['lon'] != 0)]\n",
    "\n",
    "if len(df_map) == 0:\n",
    "    print(\"⚠️ No valid coordinates to plot on map.\")\n",
    "else:\n",
    "    # Center map around the mean coordinates\n",
    "    avg_lat, avg_lon = df_map['lat'].mean(), df_map['lon'].mean()\n",
    "    m = folium.Map(location=[avg_lat, avg_lon], zoom_start=3)\n",
    "\n",
    "    # Add points to map\n",
    "    for _, row in df_map.iterrows():\n",
    "        color = 'green' if row['result_bin'] == 1 else 'red'\n",
    "        popup_text = f\"User: {row['user']}<br>IP: {row['ip']}<br>Country: {row['country']}\"\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['lon']],\n",
    "            radius=4,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            popup=popup_text\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Save and display map\n",
    "    map_path = os.path.join(BASE_DIR, 'login_geo_map.html')\n",
    "    m.save(map_path)\n",
    "    print(f\"✅ Interactive map saved at: {map_path}\")\n",
    "    m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed2aad-fcf3-445a-a100-eebe82fdc83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['ip', 'lat', 'lon']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7611f06-6456-4681-95af-6b55b4a2ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Login Anomaly Analysis Script\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "# Convert 'result' column to binary target (success=1, failed=0)\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Extract GeoIP Features\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# Hour of day & is_night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average inter-arrival seconds per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df_sorted['failed_streak'] = df_sorted.groupby('user')['result'].apply(lambda x: x.eq('failed').cumsum())\n",
    "feat['failed_streak'] = df_sorted['failed_streak']\n",
    "\n",
    "# Merge GeoIP numeric features\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Feature Importance & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 11️⃣ Save the Model\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8307a-e1ca-4d95-bf1f-86d556429846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Login Anomaly Detection Pipeline\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Extract GeoIP Features\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Time features\n",
    "feat['hour_of_day'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour_of_day'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Sort by IP and timestamp\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df_sorted['failed_streak'] = df_sorted.groupby('user')['result'].transform(lambda x: x.eq('failed').cumsum())\n",
    "feat['failed_streak'] = df_sorted['failed_streak']\n",
    "\n",
    "# Unique users per IP in last 5 minutes (approximate, using rolling 5-row window)\n",
    "feat['unique_users_last_5'] = df_sorted.groupby('ip')['user'].transform(lambda x: x.rolling(5, min_periods=1).apply(lambda s: len(set(s))))\n",
    "\n",
    "# Merge GeoIP numeric features\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# Fill remaining missing numeric values\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns\n",
    "feat[numeric_cols] = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(feat[numeric_cols])\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# PR Curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Feature Analysis\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=numeric_cols).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 11️⃣ Save Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57ae1c-d102-4045-9877-485313b5d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Login Anomaly Analysis Script\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Extract GeoIP Features\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "df_sorted = df.sort_values(['ip', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Average time gap between attempts per IP\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df_sorted['failed_streak'] = df_sorted.groupby('user')['result'].apply(lambda x: x.eq('failed').cumsum())\n",
    "feat['failed_streak'] = df_sorted['failed_streak']\n",
    "\n",
    "# Unique users per IP in last 5 minutes (accurate, using deque)\n",
    "unique_last_5min = []\n",
    "ip_windows = {}\n",
    "for ip, ts, user in zip(df_sorted['ip'], df_sorted['timestamp'], df_sorted['user']):\n",
    "    if ip not in ip_windows:\n",
    "        ip_windows[ip] = deque()\n",
    "    \n",
    "    # Remove entries older than 5 minutes\n",
    "    while ip_windows[ip] and (ts - ip_windows[ip][0][0]).total_seconds() > 300:\n",
    "        ip_windows[ip].popleft()\n",
    "    \n",
    "    # Count unique users in window\n",
    "    unique_users = len(set(u for t,u in ip_windows[ip]))\n",
    "    unique_last_5min.append(unique_users)\n",
    "    \n",
    "    # Append current attempt\n",
    "    ip_windows[ip].append((ts, user))\n",
    "\n",
    "feat['unique_users_last_5m'] = unique_last_5min\n",
    "\n",
    "# Hour of day and is_night feature\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Merge GeoIP numeric features\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 🔟 Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c155d5-165e-450c-8e07-d52ca4a43c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Login Anomaly Analysis Script\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "# Convert 'result' column to binary target: 1=success, 0=failed\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Extract GeoIP Features\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "# Merge GeoIP features with main dataframe\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Sort by IP and timestamp for time-based features\n",
    "df_sorted = df.sort_values(['ip', 'timestamp']).copy()\n",
    "\n",
    "# Average time gap between attempts per IP\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df_sorted['failed_streak'] = df_sorted.groupby('user')['result'].transform(lambda x: x.eq('failed').cumsum())\n",
    "feat['failed_streak'] = df_sorted['failed_streak']\n",
    "\n",
    "# Hour of day and night indicator\n",
    "feat['hour'] = df_sorted['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Merge GeoIP numeric features\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10 important features\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Scaler\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a14d5-4fa9-4534-989e-51c70086c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Improved)\n",
    "# ==============================\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# Copy the original dataframe for feature calculations\n",
    "df_sorted = df.sort_values(['ip', 'timestamp']).reset_index(drop=True)\n",
    "feat = feat.copy()\n",
    "\n",
    "# ---- Hour of day and night indicator ----\n",
    "feat['hour_of_day'] = df_sorted['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour_of_day'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# ---- Failed streak per user ----\n",
    "def failed_streak(series):\n",
    "    streaks = []\n",
    "    current_streak = 0\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            current_streak += 1\n",
    "        else:\n",
    "            current_streak = 0\n",
    "        streaks.append(current_streak)\n",
    "    return streaks\n",
    "\n",
    "feat['failed_streak'] = df_sorted.groupby('user')['result'].transform(failed_streak)\n",
    "\n",
    "# ---- Average inter-arrival time per IP ----\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# ---- Unique users per IP in last 5 minutes (approximate) ----\n",
    "unique_users = []\n",
    "window = deque()\n",
    "time_window = 300  # 5 minutes in seconds\n",
    "\n",
    "for idx, row in df_sorted.iterrows():\n",
    "    # Remove old entries from deque\n",
    "    while window and (row['timestamp'] - window[0][1]).total_seconds() > time_window:\n",
    "        window.popleft()\n",
    "    # Count unique users in current window\n",
    "    window.append((row['user'], row['timestamp']))\n",
    "    unique_users.append(len(set(u for u, t in window)))\n",
    "\n",
    "feat['unique_users_last_5m'] = unique_users\n",
    "\n",
    "# ---- GeoIP country indicator (if available) ----\n",
    "feat['geo_country'] = df_sorted['country']\n",
    "\n",
    "# ---- Log transformation for skewed numeric features ----\n",
    "numeric_cols = ['avg_interarrival_seconds', 'failed_streak', 'unique_users_last_5m']\n",
    "for col in numeric_cols:\n",
    "    feat[col] = np.log1p(feat[col])\n",
    "\n",
    "print(\"✅ Feature Engineering completed successfully!\")\n",
    "print(feat.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c526a-6418-4e99-8fb7-db6ed2546454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis with Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# --- Time-based features ---\n",
    "feat['hour_of_day'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour_of_day'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- GeoIP country ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "def geo_country(ip):\n",
    "    try:\n",
    "        return reader.city(ip).country.iso_code\n",
    "    except:\n",
    "        return 'NA'\n",
    "feat['geo_country'] = df['ip'].apply(geo_country)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival_seconds'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val=='failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return pd.Series(streaks, index=series.index)\n",
    "\n",
    "feat['failed_streak'] = df.groupby('user')['result'].apply(compute_failed_streak)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts (approximate) ---\n",
    "def unique_users_last_5(series):\n",
    "    res = []\n",
    "    window = deque(maxlen=5)\n",
    "    for user in series:\n",
    "        window.append(user)\n",
    "        res.append(len(set(window)))\n",
    "    return pd.Series(res, index=series.index)\n",
    "\n",
    "feat['unique_users_last_5'] = df.groupby('ip')['user'].apply(unique_users_last_5)\n",
    "\n",
    "# Optional: log-transform skewed numeric features\n",
    "for col in ['avg_interarrival_seconds', 'failed_streak', 'unique_users_last_5']:\n",
    "    feat[col] = np.log1p(feat[col])\n",
    "\n",
    "print(\"✅ Feature Engineering complete!\")\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79d1a1-87e7-4d07-8ed3-2294f0177257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Enhanced Features)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return streaks\n",
    "\n",
    "feat['failed_streak'] = df.groupby('user')['result'].transform(lambda x: compute_failed_streak(x))\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts ---\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "feat['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "# Merge GeoIP features\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1ef59-881b-4b83-8954-b999f16680c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5️⃣ Advanced Models + Cross-Validation\n",
    "# ==============================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ------------------------------\n",
    "# Define models & parameter grids\n",
    "# ------------------------------\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=sum(y_train==0)/sum(y_train==1)),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Cross-Validation Setup\n",
    "# ------------------------------\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, mp in models.items():\n",
    "    print(f\"\\n🔹 Tuning {name}...\")\n",
    "    clf = RandomizedSearchCV(mp['model'], mp['params'], n_iter=10, scoring='recall', cv=cv, n_jobs=-1, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Best params for {name}: {clf.best_params_}\")\n",
    "    print(f\"CV Recall Score: {clf.best_score_:.4f}\")\n",
    "    best_models[name] = clf.best_estimator_\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate on Test Set\n",
    "# ------------------------------\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f\"\\n{name} - PR AUC: {pr_auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bc731-e538-4efc-a842-abf62824f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5️⃣ Advanced Models + Cross-Validation\n",
    "# ==============================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ------------------------------\n",
    "# Define models & parameter grids\n",
    "# ------------------------------\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=sum(y_train==0)/sum(y_train==1)),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Cross-Validation Setup\n",
    "# ------------------------------\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, mp in models.items():\n",
    "    print(f\"\\n🔹 Tuning {name}...\")\n",
    "    clf = RandomizedSearchCV(mp['model'], mp['params'], n_iter=10, scoring='recall', cv=cv, n_jobs=-1, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Best params for {name}: {clf.best_params_}\")\n",
    "    print(f\"CV Recall Score: {clf.best_score_:.4f}\")\n",
    "    best_models[name] = clf.best_estimator_\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate on Test Set\n",
    "# ------------------------------\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f\"\\n{name} - PR AUC: {pr_auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb1749-9765-4ac3-bbcd-f41654332dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5️⃣ Advanced Models + Cross-Validation\n",
    "# ==============================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ------------------------------\n",
    "# Define models & parameter grids\n",
    "# ------------------------------\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=sum(y_train==0)/sum(y_train==1)),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Cross-Validation Setup\n",
    "# ------------------------------\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, mp in models.items():\n",
    "    print(f\"\\n🔹 Tuning {name}...\")\n",
    "    clf = RandomizedSearchCV(mp['model'], mp['params'], n_iter=10, scoring='recall', cv=cv, n_jobs=-1, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Best params for {name}: {clf.best_params_}\")\n",
    "    print(f\"CV Recall Score: {clf.best_score_:.4f}\")\n",
    "    best_models[name] = clf.best_estimator_\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate on Test Set\n",
    "# ------------------------------\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f\"\\n{name} - PR AUC: {pr_auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049f58e-7569-460b-a789-cdeee5e6e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5️⃣ تجربة نماذج أقوى وتقييم متقاطع\n",
    "# ==============================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# Define models & parameter grids\n",
    "# ------------------------------\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                               scale_pos_weight=np.sum(y_train==0)/np.sum(y_train==1)),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Cross-Validation Setup\n",
    "# ------------------------------\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, mp in models.items():\n",
    "    print(f\"🔹 Training {name} with RandomizedSearchCV...\")\n",
    "    rs = RandomizedSearchCV(mp['model'], mp['params'], n_iter=10, cv=cv,\n",
    "                            scoring='recall', verbose=1, n_jobs=-1, random_state=42)\n",
    "    rs.fit(X_train, y_train)\n",
    "    \n",
    "    best_models[name] = rs.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = rs.predict(X_test)\n",
    "    y_proba = rs.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Metrics\n",
    "    print(f\"✅ Best params for {name}: {rs.best_params_}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    roc = roc_auc_score(y_test, y_proba)\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(f\"ROC AUC: {roc:.4f}, PR AUC: {pr_auc:.4f}\\n\")\n",
    "\n",
    "# ------------------------------\n",
    "# Save the best model (example: LightGBM)\n",
    "# ------------------------------\n",
    "MODEL_PATH = 'models/best_model_step5.joblib'\n",
    "joblib.dump(best_models['LightGBM'], MODEL_PATH)\n",
    "print(f\"✅ Best model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60211c5a-556d-4571-bc13-ff5f81803cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# افترض أن 'feat' هو DataFrame الخاص بالميزات المحسوبة\n",
    "# وأن 'result' هو العمود الهدف (0=success, 1=failed)\n",
    "X = feat.drop(columns=['result'])  # كل الأعمدة عدا الهدف\n",
    "y = feat['result']\n",
    "\n",
    "# تقسيم البيانات\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Data ready for modeling:\")\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "print(\"y_train distribution:\\n\", y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bec876-62d5-4d4f-9bd9-0146d35d4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1️⃣ تجهيز البيانات\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# قراءة الميزات\n",
    "feat = pd.read_csv('data/auth_features.csv', parse_dates=['timestamp'])\n",
    "df = pd.read_csv('data/auth_parsed.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# تأكد من وجود العمود الهدف\n",
    "feat['result'] = df['result']  # 0=success, 1=failed\n",
    "\n",
    "# X و y\n",
    "X = feat.drop(columns=['result'])\n",
    "y = feat['result']\n",
    "\n",
    "# تقسيم البيانات\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ استدعاء المكتبات للنماذج\n",
    "# ===============================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ تعريف النماذج و Grid\n",
    "# ===============================\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                               scale_pos_weight=np.sum(y_train==0)/np.sum(y_train==1)),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ Cross-Validation Setup\n",
    "# ===============================\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 5️⃣ تدريب كل نموذج باستخدام RandomizedSearchCV\n",
    "# ===============================\n",
    "best_models = {}\n",
    "for name, m in models.items():\n",
    "    print(f\"Training {name} ...\")\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=m['model'],\n",
    "        param_distributions=m['params'],\n",
    "        n_iter=10,\n",
    "        scoring='recall',       # نركز على recall لفئة failed\n",
    "        cv=cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rs.fit(X_train, y_train)\n",
    "    best_models[name] = rs.best_estimator_\n",
    "    print(f\"Best params for {name}: {rs.best_params_}\\n\")\n",
    "\n",
    "# ===============================\n",
    "# 6️⃣ تقييم النماذج على Test Set\n",
    "# ===============================\n",
    "for name, model in best_models.items():\n",
    "    print(f\"Evaluating {name} ...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:,1]\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    print(\"PR AUC:\", auc(rec, prec))\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f3d8d-8aea-40cc-909b-a53b2d009d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0️⃣ استيراد المكتبات\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "import joblib\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ قراءة الملفات الكبيرة بالchunks\n",
    "# ===============================\n",
    "chunksize = 10**6\n",
    "dfs_parsed = []\n",
    "dfs_feat = []\n",
    "\n",
    "print(\"📥 قراءة auth_parsed_large.csv...\")\n",
    "for chunk in pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'], chunksize=chunksize):\n",
    "    dfs_parsed.append(chunk)\n",
    "df = pd.concat(dfs_parsed, ignore_index=True)\n",
    "del dfs_parsed\n",
    "\n",
    "print(\"📥 قراءة auth_features_large.csv...\")\n",
    "for chunk in pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'], chunksize=chunksize):\n",
    "    dfs_feat.append(chunk)\n",
    "feat = pd.concat(dfs_feat, ignore_index=True)\n",
    "del dfs_feat\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ جودة البيانات\n",
    "# ===============================\n",
    "print(\"نسبة القيم الفارغة لكل عمود في auth_parsed:\")\n",
    "print(df.isnull().mean())\n",
    "print(\"\\nتوازن النتائج:\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "# رسم توزع النتائج\n",
    "sns.countplot(x='result', data=df)\n",
    "plt.title(\"Distribution of Result\")\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ===============================\n",
    "\n",
    "# --- ميزات وقتية ---\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- avg_interarrival_seconds لكل IP ---\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- failed_streak لكل مستخدم ---\n",
    "def compute_failed_streak(series):\n",
    "    streaks = []\n",
    "    streak = 0\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return pd.Series(streaks, index=series.index)\n",
    "\n",
    "feat['failed_streak'] = df.groupby('user')['result'].apply(compute_failed_streak).reset_index(drop=True)\n",
    "\n",
    "# --- Unique users per IP في آخر 5 محاولات (تقريبية) ---\n",
    "from collections import deque\n",
    "def unique_users_last_5(series):\n",
    "    result = []\n",
    "    last_users = deque(maxlen=5)\n",
    "    for u in series:\n",
    "        last_users.append(u)\n",
    "        result.append(len(set(last_users)))\n",
    "    return pd.Series(result, index=series.index)\n",
    "\n",
    "feat['unique_users_last_5'] = df.groupby('ip')['user'].apply(unique_users_last_5).reset_index(drop=True)\n",
    "\n",
    "# --- تحويل اللوغاريتم لبعض العدادات ---\n",
    "feat['log_failed_streak'] = np.log1p(feat['failed_streak'])\n",
    "feat['log_unique_users_last_5'] = np.log1p(feat['unique_users_last_5'])\n",
    "\n",
    "# --- العمود الهدف ---\n",
    "feat['result'] = df['result'].apply(lambda x: 1 if x=='failed' else 0)\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ رسم الإحصاءات الأساسية للميزات\n",
    "# ===============================\n",
    "num_cols = ['avg_interarrival', 'failed_streak', 'unique_users_last_5', 'log_failed_streak', 'log_unique_users_last_5']\n",
    "for col in num_cols:\n",
    "    sns.histplot(feat[col], bins=50, kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 5️⃣ تقسيم البيانات\n",
    "# ===============================\n",
    "X = feat.drop(columns=['result', 'user', 'ip', 'timestamp'])\n",
    "y = feat['result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 6️⃣ تعريف النماذج وتهيئة الهايبر باراميتر\n",
    "# ===============================\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                               scale_pos_weight=np.sum(y_train==0)/np.sum(y_train==1)),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# 7️⃣ Cross-Validation وتحسين الهايبر باراميتر\n",
    "# ===============================\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "best_models = {}\n",
    "\n",
    "for name, m in models.items():\n",
    "    print(f\"=== تدريب {name} ===\")\n",
    "    rs = RandomizedSearchCV(\n",
    "        m['model'], m['params'], n_iter=10, cv=cv, scoring='recall', n_jobs=-1, random_state=42\n",
    "    )\n",
    "    rs.fit(X_train, y_train)\n",
    "    best_models[name] = rs.best_estimator_\n",
    "    print(\"أفضل معلمات:\", rs.best_params_)\n",
    "\n",
    "    y_pred = rs.predict(X_test)\n",
    "    y_proba = rs.predict_proba(X_test)[:,1]\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    print(\"PR AUC:\", auc(rec, prec))\n",
    "    \n",
    "    # رسم ROC و PR\n",
    "    plt.figure()\n",
    "    plt.plot(rec, prec, label=f'PR Curve {name} (AUC={auc(rec,prec):.3f})')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve - {name}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 8️⃣ حفظ النماذج\n",
    "# ===============================\n",
    "for name, model in best_models.items():\n",
    "    joblib.dump(model, f'models/{name}_best_model_large.joblib')\n",
    "\n",
    "print(\"✅ انتهى التدريب وحفظ النماذج على الملفات الكبيرة.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285c630-5cc8-4a2f-b6e0-fda2ebe29e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# مشروع كشف الأنشطة المشبوهة - كامل\n",
    "# للعمل مع الملفات الكبيرة\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import joblib\n",
    "\n",
    "# ------------------------------\n",
    "# إعداد مجلد العمل\n",
    "# ------------------------------\n",
    "os.chdir('/home/bakri/projects/login-anomaly')\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "\n",
    "# ------------------------------\n",
    "# قراءة الملفات الكبيرة\n",
    "# ------------------------------\n",
    "chunksize = 100000\n",
    "dfs_parsed = []\n",
    "dfs_feat = []\n",
    "\n",
    "print(\"📥 قراءة auth_parsed_large.csv...\")\n",
    "for chunk in pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'], chunksize=chunksize):\n",
    "    dfs_parsed.append(chunk)\n",
    "df = pd.concat(dfs_parsed, ignore_index=True)\n",
    "\n",
    "print(\"📥 قراءة auth_features_large.csv...\")\n",
    "for chunk in pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'], chunksize=chunksize):\n",
    "    dfs_feat.append(chunk)\n",
    "feat = pd.concat(dfs_feat, ignore_index=True)\n",
    "\n",
    "print(df.shape, feat.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# التحقق من جودة البيانات\n",
    "# ------------------------------\n",
    "print(\"نسبة القيم الفارغة لكل عمود:\")\n",
    "print(df.isnull().mean())\n",
    "print(\"توزيع النتائج:\")\n",
    "print(df['result'].value_counts())\n",
    "print(feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# Feature Engineering\n",
    "# ------------------------------\n",
    "\n",
    "# الوقت\n",
    "feat['hour'] = feat['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# متوسط الفاصل الزمني بين محاولات نفس الـIP\n",
    "df_sorted = df.sort_values(['ip','timestamp']).copy()\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak لكل مستخدم\n",
    "def compute_failed_streak(series):\n",
    "    streaks = []\n",
    "    streak = 0\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return pd.Series(streaks, index=series.index)\n",
    "\n",
    "df_sorted['failed_streak'] = df_sorted.groupby('user')['result'].apply(compute_failed_streak)\n",
    "feat['failed_streak'] = df_sorted['failed_streak']\n",
    "\n",
    "# Unique users per IP في آخر 5 محاولات\n",
    "def unique_users_last_5(series):\n",
    "    result = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for val in series:\n",
    "        dq.append(val)\n",
    "        result.append(len(set(dq)))\n",
    "    return pd.Series(result, index=series.index)\n",
    "\n",
    "feat['unique_users_last_5'] = df_sorted.groupby('ip')['user'].apply(unique_users_last_5)\n",
    "\n",
    "# ------------------------------\n",
    "# Visualization\n",
    "# ------------------------------\n",
    "sns.histplot(df['result'], kde=False)\n",
    "plt.title('توزيع نتائج تسجيل الدخول')\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(feat['avg_interarrival'], bins=50)\n",
    "plt.title('توزيع متوسط الفاصل الزمني بين محاولات IP')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x='is_night', y='failed_streak', data=feat)\n",
    "plt.title('الفشل المتتالي حسب وقت الليل')\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# إعداد البيانات للتدريب\n",
    "# ------------------------------\n",
    "X = feat.drop(columns=['result'])\n",
    "y = feat['result'].map({'success':0, 'failed':1})  # تحويل الهدف إلى 0/1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# تعريف النماذج والبحث العشوائي\n",
    "# ------------------------------\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50,100,200],\n",
    "            'max_depth': [5,10,20,None],\n",
    "            'min_samples_split': [2,5,10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                               scale_pos_weight=np.sum(y_train==0)/np.sum(y_train==1)),\n",
    "        'params': {\n",
    "            'n_estimators': [50,100,200],\n",
    "            'max_depth': [3,5,7],\n",
    "            'learning_rate': [0.01,0.1,0.2],\n",
    "            'subsample': [0.7,0.8,1.0]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50,100,200],\n",
    "            'num_leaves': [31,50,100],\n",
    "            'learning_rate': [0.01,0.05,0.1],\n",
    "            'subsample': [0.7,0.8,1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Cross-validation و Grid Search\n",
    "# ------------------------------\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "best_models = {}\n",
    "\n",
    "for name, mp in models.items():\n",
    "    print(f\"🔹 تدريب {name}...\")\n",
    "    rs = RandomizedSearchCV(mp['model'], mp['params'], n_iter=5, cv=cv, scoring='recall', n_jobs=-1)\n",
    "    rs.fit(X_train, y_train)\n",
    "    best_models[name] = rs.best_estimator_\n",
    "    print(f\"أفضل معاملات {name}:\", rs.best_params_)\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluation\n",
    "# ------------------------------\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    print(f\"\\n📊 تقييم النموذج: {name}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ------------------------------\n",
    "# حفظ النماذج\n",
    "# ------------------------------\n",
    "for name, model in best_models.items():\n",
    "    joblib.dump(model, f'models/{name}_best_model.joblib')\n",
    "    print(f\"✅ تم حفظ النموذج {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b20fa0-9269-4f41-9060-d97f592d6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "import joblib\n",
    "\n",
    "# ------------------------------\n",
    "# قراءة الميزات الكبيرة\n",
    "# ------------------------------\n",
    "feat_file = 'data/auth_features_large.csv'\n",
    "feat = pd.read_csv(feat_file, parse_dates=['timestamp'])\n",
    "\n",
    "# ------------------------------\n",
    "# إعداد X و y\n",
    "# ------------------------------\n",
    "X = feat.drop(columns=['result','timestamp','user','ip'])  # استبعاد الأعمدة غير العددية أو الهدف\n",
    "y = (feat['result'] == 'failed').astype(int)  # تحويل إلى binary 0/1\n",
    "\n",
    "# ------------------------------\n",
    "# تقسيم البيانات\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# تعريف النماذج والـ hyperparameters\n",
    "# ------------------------------\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                               scale_pos_weight=np.sum(y_train==0)/np.sum(y_train==1),\n",
    "                               random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'num_leaves': [31, 50, 100],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Cross-Validation و RandomizedSearch\n",
    "# ------------------------------\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "best_models = {}\n",
    "\n",
    "for name, mp in models.items():\n",
    "    print(f\"\\n🔹 تدريب {name} ...\")\n",
    "    rs = RandomizedSearchCV(mp['model'], mp['params'], n_iter=5, cv=cv, scoring='recall', n_jobs=-1, verbose=1)\n",
    "    rs.fit(X_train, y_train)\n",
    "    best_models[name] = rs.best_estimator_\n",
    "    print(f\"أفضل المعاملات لـ {name}: {rs.best_params_}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # تقييم الأداء\n",
    "    # ------------------------------\n",
    "    y_pred = rs.predict(X_test)\n",
    "    y_prob = rs.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ------------------------------\n",
    "# حفظ أفضل نموذج (مثال: RandomForest)\n",
    "# ------------------------------\n",
    "joblib.dump(best_models['RandomForest'], 'models/random_forest_best.joblib')\n",
    "print(\"✅ النموذج محفوظ في models/random_forest_best.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb111b23-7612-4012-8ee3-f90919cd423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# مشروع كشف محاولات تسجيل الدخول المريبة - كامل\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    lightgbm_installed = True\n",
    "except ImportError:\n",
    "    lightgbm_installed = False\n",
    "    print(\"LightGBM غير مثبت، سيتم تجاوز استخدامه\")\n",
    "\n",
    "# ------------------------------\n",
    "# قراءة الملفات الكبيرة\n",
    "# ------------------------------\n",
    "chunksize = 500_000\n",
    "\n",
    "print(\"📥 قراءة auth_parsed_large.csv...\")\n",
    "dfs_parsed = []\n",
    "for chunk in pd.read_csv('data/auth_parsed_large.csv', parse_dates=['timestamp'], chunksize=chunksize):\n",
    "    dfs_parsed.append(chunk)\n",
    "df = pd.concat(dfs_parsed, ignore_index=True)\n",
    "print(\"تم قراءة auth_parsed_large.csv:\", df.shape)\n",
    "\n",
    "print(\"📥 قراءة auth_features_large.csv...\")\n",
    "dfs_feat = []\n",
    "for chunk in pd.read_csv('data/auth_features_large.csv', parse_dates=['timestamp'], chunksize=chunksize):\n",
    "    dfs_feat.append(chunk)\n",
    "feat = pd.concat(dfs_feat, ignore_index=True)\n",
    "print(\"تم قراءة auth_features_large.csv:\", feat.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# استعراض الأعمدة للتأكد من الميزات المتاحة\n",
    "# ------------------------------\n",
    "print(\"\\nأعمدة الميزات المتاحة:\")\n",
    "print(feat.columns.tolist())\n",
    "\n",
    "# ------------------------------\n",
    "# مثال على رسوميات استكشافية\n",
    "# ------------------------------\n",
    "if 'result' in feat.columns:\n",
    "    sns.countplot(x='result', data=feat)\n",
    "    plt.title(\"توزيع النتائج\")\n",
    "    plt.show()\n",
    "\n",
    "if 'hour' not in feat.columns and 'timestamp' in feat.columns:\n",
    "    feat['hour'] = feat['timestamp'].dt.hour\n",
    "    feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# رسم histogram للساعات\n",
    "sns.histplot(feat['hour'], bins=24, kde=False)\n",
    "plt.title(\"توزيع المحاولات حسب الساعة\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# حساب failed_streak\n",
    "# ------------------------------\n",
    "def compute_failed_streak(series):\n",
    "    streaks = []\n",
    "    streak = 0\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return pd.Series(streaks, index=series.index)\n",
    "\n",
    "if 'user' in df.columns and 'result' in df.columns:\n",
    "    df_sorted = df.sort_values(['user', 'timestamp'])\n",
    "    df_sorted['failed_streak'] = df_sorted.groupby('user')['result'].apply(compute_failed_streak)\n",
    "    feat['failed_streak'] = df_sorted['failed_streak']\n",
    "\n",
    "# ------------------------------\n",
    "# تحضير X و y للنماذج\n",
    "# ------------------------------\n",
    "cols_to_drop = ['result','timestamp','user','ip']\n",
    "cols_to_drop_existing = [c for c in cols_to_drop if c in feat.columns]\n",
    "\n",
    "X = feat.drop(columns=cols_to_drop_existing)\n",
    "y = (feat['result'] == 'failed').astype(int) if 'result' in feat.columns else None\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "if y is not None:\n",
    "    print(\"y distribution:\\n\", y.value_counts())\n",
    "\n",
    "# ------------------------------\n",
    "# تقسيم البيانات\n",
    "# ------------------------------\n",
    "if y is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "# ------------------------------\n",
    "# إعداد النماذج والباراميترات\n",
    "# ------------------------------\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators':[50, 100, 200],\n",
    "            'max_depth':[5, 10, 20, None],\n",
    "            'min_samples_split':[2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            scale_pos_weight=(np.sum(y_train==0)/np.sum(y_train==1)) if y is not None else 1\n",
    "        ),\n",
    "        'params': {\n",
    "            'n_estimators':[50, 100, 200],\n",
    "            'max_depth':[3, 5, 7],\n",
    "            'learning_rate':[0.01, 0.1, 0.2],\n",
    "            'subsample':[0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if lightgbm_installed:\n",
    "    models['LightGBM'] = {\n",
    "        'model': LGBMClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators':[50, 100, 200],\n",
    "            'num_leaves':[31, 50, 100],\n",
    "            'learning_rate':[0.01, 0.05, 0.1],\n",
    "            'subsample':[0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ------------------------------\n",
    "# Cross-validation و RandomizedSearch\n",
    "# ------------------------------\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "for name, mp in models.items():\n",
    "    print(f\"\\n💻 تدريب وتقييم النموذج: {name}\")\n",
    "    rs = RandomizedSearchCV(mp['model'], mp['params'], n_iter=5, cv=cv, scoring='recall', n_jobs=-1, verbose=1)\n",
    "    rs.fit(X_train, y_train)\n",
    "    print(\"أفضل الباراميترات:\", rs.best_params_)\n",
    "    \n",
    "    y_pred = rs.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    if hasattr(rs, \"predict_proba\"):\n",
    "        y_prob = rs.predict_proba(X_test)[:,1]\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "        prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "        print(\"PR AUC:\", auc(rec, prec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786eb79b-b203-4fd9-b87c-2757feb22caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Login Anomaly Detection Script (Robust, Large Files)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'login_anomaly_model_final.joblib')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "print(\"📥 Loading data...\")\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='failed' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\\n\", y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform(lambda x: x.fillna(0).rolling(5,min_periods=1).mean())\n",
    "\n",
    "# --- Failed streak per user ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val=='failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return streaks\n",
    "\n",
    "# Avoid transform mismatch\n",
    "failed_streaks = df.groupby('user')['result'].apply(lambda x: pd.Series(compute_failed_streak(x), index=x.index))\n",
    "feat['failed_streak'] = failed_streaks.sort_index()\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts ---\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "unique_users = df.groupby('ip')['user'].apply(lambda x: pd.Series(unique_users_last_5(x), index=x.index))\n",
    "feat['unique_users_last_5'] = unique_users.sort_index()\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, lat, lon = 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['geo_country','lat','lon']\n",
    "\n",
    "# Merge GeoIP features safely\n",
    "feat['geo_country'] = geo_feat['geo_country']\n",
    "feat['lat'] = geo_feat['lat'].fillna(0)\n",
    "feat['lon'] = geo_feat['lon'].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present, SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models: RandomForest, XGBoost, LightGBM\n",
    "# ==============================\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42,\n",
    "                             scale_pos_weight=sum(y_train==0)/sum(y_train==1)),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔹 Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f\"{name} PR AUC: {pr_auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    best_models[name] = model\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Feature Importances (RandomForest)\n",
    "# ==============================\n",
    "importances = pd.Series(best_models['RandomForest'].feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Save the Best Model & Scaler\n",
    "# ==============================\n",
    "joblib.dump({'models': best_models, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ All models and scaler saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39458d29-fd50-4abd-aec3-719a0a8e50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Final Version)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check files\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val=='failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return streaks\n",
    "\n",
    "# حساب الـ streak لكل user بشكل آمن\n",
    "failed_streaks = df.groupby('user')['result'].apply(lambda x: pd.Series(compute_failed_streak(x), index=x.index))\n",
    "feat = feat.join(failed_streaks.rename('failed_streak'))\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts ---\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "unique_users_ip = df.groupby('ip')['user'].apply(lambda x: pd.Series(unique_users_last_5(x), index=x.index))\n",
    "feat = feat.join(unique_users_ip.rename('unique_users_last_5'))\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff22244-bcf7-4a74-b2a5-da43607d0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Enhanced Features, Fixed Index Issues)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return streaks\n",
    "\n",
    "failed_streaks = df.groupby('user')['result'].apply(lambda x: pd.Series(compute_failed_streak(x), index=x.index))\n",
    "feat['failed_streak'] = failed_streaks.reindex(feat.index).fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts ---\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "unique_users_ip = df.groupby('ip')['user'].apply(lambda x: pd.Series(unique_users_last_5(x), index=x.index))\n",
    "feat['unique_users_last_5'] = unique_users_ip.reindex(feat.index).fillna(0).astype(int)\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "# Merge GeoIP features\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c70aca-2826-4f9b-a436-6b8207915af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Final Version)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return streaks\n",
    "\n",
    "failed_streaks = df.groupby('user')['result'].apply(lambda x: pd.Series(compute_failed_streak(x), index=x.index))\n",
    "feat = feat.merge(failed_streaks.rename('failed_streak'), left_index=True, right_index=True, how='left')\n",
    "feat['failed_streak'] = feat['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts ---\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "unique_users = df.groupby('ip')['user'].apply(lambda x: pd.Series(unique_users_last_5(x), index=x.index))\n",
    "feat = feat.merge(unique_users.rename('unique_users_last_5'), left_index=True, right_index=True, how='left')\n",
    "feat['unique_users_last_5'] = feat['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "# Merge GeoIP features\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e55733-0b9c-4027-b110-95c29e95bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Safe & Complete)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check files\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# --- Hour & Night Features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user (Safe) ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return streaks\n",
    "\n",
    "# ترتيب df حسب index لضمان تطابق مع feat\n",
    "df_sorted_index = df.sort_index()\n",
    "failed_streaks = df_sorted_index.groupby('user')['result'].apply(lambda x: pd.Series(compute_failed_streak(x), index=x.index))\n",
    "feat['failed_streak'] = failed_streaks.reindex(feat.index).fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts ---\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "unique_users = df_sorted_index.groupby('ip')['user'].apply(lambda x: pd.Series(unique_users_last_5(x), index=x.index))\n",
    "feat['unique_users_last_5'] = unique_users.reindex(feat.index).fillna(0).astype(int)\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', 0, 0\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "# Merge GeoIP features safely\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c0df8-39f7-470a-88a0-32f241be1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Fixed & Enhanced)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user (Safe) ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return streaks\n",
    "\n",
    "df_temp = df[['user','result']].reset_index()\n",
    "failed_streaks = df_temp.groupby('user')['result'].apply(lambda x: pd.Series(compute_failed_streak(x), index=x.index))\n",
    "failed_streaks = failed_streaks.sort_index()\n",
    "feat['failed_streak'] = failed_streaks.values[:len(feat)]\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts ---\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "df_temp2 = df[['ip','user']].reset_index()\n",
    "unique_users = df_temp2.groupby('ip')['user'].apply(lambda x: pd.Series(unique_users_last_5(x), index=x.index))\n",
    "unique_users = unique_users.sort_index()\n",
    "feat['unique_users_last_5'] = unique_users.values[:len(feat)]\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "# Merge GeoIP features\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31c843-c7d6-41be-aec8-ff99095c47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Analysis - Optimized for Large Files\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files exist.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data (chunks for large files)\n",
    "# ==============================\n",
    "print(\"📥 Loading auth_parsed_large.csv...\")\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], chunksize=500000):\n",
    "    chunks.append(chunk)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "\n",
    "print(\"📥 Loading auth_features_large.csv...\")\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded.\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# --- Hour & night ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted_ip = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted_ip['time_diff'] = df_sorted_ip.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted_ip.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return streaks\n",
    "\n",
    "failed_streaks = df.groupby('user')['result'].apply(lambda x: pd.Series(compute_failed_streak(x), index=x.index))\n",
    "feat['failed_streak'] = failed_streaks.reindex(feat.index).fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts ---\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "unique_users = df.groupby('ip')['user'].apply(lambda x: pd.Series(unique_users_last_5(x), index=x.index))\n",
    "feat['unique_users_last_5'] = unique_users.reindex(feat.index).fillna(0).astype(int)\n",
    "\n",
    "# --- GeoIP features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adcb06-991a-4130-9bb2-6641c6d61007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Enhanced Features)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'])\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'])\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for val in series:\n",
    "        if val == 'failed':\n",
    "            streak += 1\n",
    "        else:\n",
    "            streak = 0\n",
    "        streaks.append(streak)\n",
    "    return streaks\n",
    "\n",
    "failed_streaks = df.groupby('user')['result'].apply(lambda x: pd.Series(compute_failed_streak(x), index=x.index))\n",
    "df['failed_streak'] = failed_streaks\n",
    "feat = feat.merge(df[['failed_streak']], left_index=True, right_index=True, how='left')\n",
    "feat['failed_streak'] = feat['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts ---\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "unique_users = df.groupby('ip')['user'].apply(lambda x: pd.Series(unique_users_last_5(x), index=x.index))\n",
    "df['unique_users_last_5'] = unique_users\n",
    "feat = feat.merge(df[['unique_users_last_5']], left_index=True, right_index=True, how='left')\n",
    "feat['unique_users_last_5'] = feat['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# --- GeoIP Features ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "def geoip_features(ip):\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    return pd.Series([country, city, lat, lon])\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad968d90-a97b-4544-8384-faf9a98fa104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user (optimized) ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts (optimized) ---\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# --- GeoIP Features (optimized with caching) ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_features(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "geo_feat = df['ip'].apply(geoip_features)\n",
    "geo_feat.columns = ['country', 'city', 'lat', 'lon']\n",
    "df = pd.concat([df, geo_feat], axis=1)\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b251a-15cd-44a3-b9f5-aede3a6d33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized with Parallel GeoIP)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user (optimized) ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts (optimized) ---\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# --- GeoIP Features (Parallel + Cache) ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "# Use parallel processing\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(\n",
    "    delayed(geoip_lookup)(ip) for ip in unique_ips\n",
    ")\n",
    "\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c500d47-e942-4c4f-bd1e-8b8f7ed9bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized with Parallel GeoIP + Data Quality Check)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "print(\"\\n--- Data Quality Check ---\\n\")\n",
    "\n",
    "# Missing values\n",
    "print(\"❌ Missing values in auth_parsed:\")\n",
    "print(df.isnull().mean())\n",
    "print(\"\\n❌ Missing values in auth_features:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# Distribution of result\n",
    "print(\"\\n--- Result Distribution ---\")\n",
    "print(df['result'].value_counts())\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# Numeric statistics\n",
    "print(\"\\n--- Numeric Statistics auth_parsed ---\")\n",
    "print(df.describe())\n",
    "print(\"\\n--- Numeric Statistics auth_features ---\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Check IP and timestamp validity\n",
    "print(\"\\n--- IP and Timestamp Checks ---\")\n",
    "print(\"Rows with missing IP:\", df['ip'].isnull().sum())\n",
    "print(\"Rows with missing timestamp:\", df['timestamp'].isnull().sum())\n",
    "\n",
    "# Drop rows with missing critical values\n",
    "df = df.dropna(subset=['ip', 'timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "print(\"✅ Dropped rows with missing IP or timestamp.\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user (optimized) ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts (optimized) ---\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# --- GeoIP Features (Parallel + Cache) ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "# Use parallel processing\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(\n",
    "    delayed(geoip_lookup)(ip) for ip in unique_ips\n",
    ")\n",
    "\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a2956-a96e-437c-9eaf-4624b969dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized with Parallel GeoIP + Data Quality Check & Visual Report)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "print(\"\\n--- Data Quality Check ---\\n\")\n",
    "\n",
    "# Missing values\n",
    "print(\"❌ Missing values in auth_parsed:\")\n",
    "print(df.isnull().mean())\n",
    "print(\"\\n❌ Missing values in auth_features:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# Distribution of result\n",
    "print(\"\\n--- Result Distribution ---\")\n",
    "print(df['result'].value_counts())\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# Numeric statistics\n",
    "print(\"\\n--- Numeric Statistics auth_parsed ---\")\n",
    "print(df.describe())\n",
    "print(\"\\n--- Numeric Statistics auth_features ---\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Check IP and timestamp validity\n",
    "print(\"\\n--- IP and Timestamp Checks ---\")\n",
    "print(\"Rows with missing IP:\", df['ip'].isnull().sum())\n",
    "print(\"Rows with missing timestamp:\", df['timestamp'].isnull().sum())\n",
    "\n",
    "# Drop rows with missing critical values\n",
    "df = df.dropna(subset=['ip', 'timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "print(\"✅ Dropped rows with missing IP or timestamp.\")\n",
    "\n",
    "# ==============================\n",
    "# 1b️⃣ Visual Data Quality Report\n",
    "# ==============================\n",
    "print(\"\\n--- Visual Data Quality Report ---\\n\")\n",
    "\n",
    "# 1️⃣ Missing values plot\n",
    "missing_percent = df.isnull().mean() * 100\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(x=missing_percent.index, y=missing_percent.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('% Missing')\n",
    "plt.title('Missing Values per Column (auth_parsed)')\n",
    "plt.show()\n",
    "\n",
    "missing_percent_feat = feat.isnull().mean() * 100\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(x=missing_percent_feat.index, y=missing_percent_feat.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('% Missing')\n",
    "plt.title('Missing Values per Column (auth_features)')\n",
    "plt.show()\n",
    "\n",
    "# 2️⃣ Distribution of result\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=df['result'])\n",
    "plt.title('Result Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# 3️⃣ Boxplots for numeric columns in features\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Boxplot of Numeric Features')\n",
    "plt.show()\n",
    "\n",
    "# 4️⃣ Correlation heatmap\n",
    "corr = feat[numeric_cols].corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap (Numeric Features)')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visual Data Quality Report Completed!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user (optimized) ---\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users per IP in last 5 attempts (optimized) ---\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# --- GeoIP Features (Parallel + Cache) ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "# Use parallel processing\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(\n",
    "    delayed(geoip_lookup)(ip) for ip in unique_ips\n",
    ")\n",
    "\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d79ae-1ebd-49ea-92b9-a86cda8671b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis \n",
    "# (Optimized with Parallel GeoIP + Data Quality Check + Visual Report + Model Evaluation)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "print(\"\\n--- Data Quality Check ---\\n\")\n",
    "\n",
    "# Missing values\n",
    "print(\"❌ Missing values in auth_parsed:\")\n",
    "print(df.isnull().mean())\n",
    "print(\"\\n❌ Missing values in auth_features:\")\n",
    "print(feat.isnull().mean())\n",
    "\n",
    "# Distribution of result\n",
    "print(\"\\n--- Result Distribution ---\")\n",
    "print(df['result'].value_counts())\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "\n",
    "# Numeric statistics\n",
    "print(\"\\n--- Numeric Statistics auth_parsed ---\")\n",
    "print(df.describe())\n",
    "print(\"\\n--- Numeric Statistics auth_features ---\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Check IP and timestamp validity\n",
    "print(\"\\n--- IP and Timestamp Checks ---\")\n",
    "print(\"Rows with missing IP:\", df['ip'].isnull().sum())\n",
    "print(\"Rows with missing timestamp:\", df['timestamp'].isnull().sum())\n",
    "\n",
    "# Drop rows with missing critical values\n",
    "df = df.dropna(subset=['ip', 'timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "print(\"✅ Dropped rows with missing IP or timestamp.\")\n",
    "\n",
    "# ==============================\n",
    "# 1b️⃣ Visual Data Quality Report\n",
    "# ==============================\n",
    "print(\"\\n--- Visual Data Quality Report ---\\n\")\n",
    "\n",
    "# Missing values plot\n",
    "missing_percent = df.isnull().mean() * 100\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(x=missing_percent.index, y=missing_percent.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('% Missing')\n",
    "plt.title('Missing Values per Column (auth_parsed)')\n",
    "plt.show()\n",
    "\n",
    "missing_percent_feat = feat.isnull().mean() * 100\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(x=missing_percent_feat.index, y=missing_percent_feat.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('% Missing')\n",
    "plt.title('Missing Values per Column (auth_features)')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of result\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=df['result'])\n",
    "plt.title('Result Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Boxplots for numeric columns in features\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Boxplot of Numeric Features')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "corr = feat[numeric_cols].corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap (Numeric Features)')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visual Data Quality Report Completed!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Model Evaluation (Step 2)\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Classification report\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['failed','success']))\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Precision-Recall Curve & PR-AUC\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Determine threshold for target recall (e.g., 0.8)\n",
    "target_recall = 0.8\n",
    "idx = np.where(recall >= target_recall)[0][0]\n",
    "threshold_for_target = thresholds[idx-1]\n",
    "print(f\"Threshold to achieve recall >= {target_recall}: {threshold_for_target:.4f}\")\n",
    "print(f\"Precision at this threshold: {precision[idx]:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3ddae-c320-47b8-8db9-79e7edb6a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script: Login anomaly analysis\n",
    "# (Data Quality Check + Advanced Feature Engineering + Model Evaluation)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "print(\"\\n--- Data Quality Check ---\\n\")\n",
    "print(\"❌ Missing values in auth_parsed:\")\n",
    "print(df.isnull().mean())\n",
    "print(\"\\n❌ Missing values in auth_features:\")\n",
    "print(feat.isnull().mean())\n",
    "print(\"\\n--- Result Distribution ---\")\n",
    "print(df['result'].value_counts())\n",
    "print(df['result'].value_counts(normalize=True))\n",
    "df = df.dropna(subset=['ip', 'timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "print(\"✅ Dropped rows with missing IP or timestamp.\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Basic + Advanced)\n",
    "# ==============================\n",
    "\n",
    "# --- Basic Features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# --- Advanced Time Features ---\n",
    "feat['minute_of_hour'] = df['timestamp'].dt.minute\n",
    "feat['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# --- Advanced Unique Users Last 5 minutes ---\n",
    "def unique_users_last_5m(df):\n",
    "    df_sorted = df.sort_values(['ip','timestamp'])\n",
    "    result = []\n",
    "    for ip, group in df_sorted.groupby('ip'):\n",
    "        times = group['timestamp']\n",
    "        users = group['user']\n",
    "        dq = deque()\n",
    "        counts = []\n",
    "        for t,u in zip(times,users):\n",
    "            dq.append((t,u))\n",
    "            while dq and (t - dq[0][0]).total_seconds() > 300:\n",
    "                dq.popleft()\n",
    "            counts.append(len(set([x[1] for x in dq])))\n",
    "        result.extend(counts)\n",
    "    return pd.Series(result, index=df_sorted.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = unique_users_last_5m(df)\n",
    "\n",
    "# --- GeoIP Features (Parallel + Cache) ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# --- Encoding categorical features ---\n",
    "feat = pd.get_dummies(feat, columns=['geo_country'], dummy_na=True)\n",
    "\n",
    "# --- Log-transform and clip numeric features ---\n",
    "numeric_cols = ['avg_interarrival','failed_streak','unique_users_last_5','unique_users_last_5m']\n",
    "for col in numeric_cols:\n",
    "    # Clip outliers\n",
    "    q_low = feat[col].quantile(0.01)\n",
    "    q_high = feat[col].quantile(0.99)\n",
    "    feat[col] = feat[col].clip(q_low, q_high)\n",
    "    # Log transform\n",
    "    feat[col+'_log'] = np.log1p(feat[col])\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols_final = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols_final: numeric_cols_final.remove('timestamp')\n",
    "X = feat[numeric_cols_final].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Model Evaluation\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['failed','success']))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Threshold for target recall\n",
    "target_recall = 0.8\n",
    "idx = np.where(recall >= target_recall)[0][0]\n",
    "threshold_for_target = thresholds[idx-1]\n",
    "print(f\"Threshold to achieve recall >= {target_recall}: {threshold_for_target:.4f}\")\n",
    "print(f\"Precision at this threshold: {precision[idx]:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0944f33-e01f-491e-b27e-492ea45d7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script: Login anomaly analysis\n",
    "# (Advanced Feature Engineering + Model Evaluation + Hyperparameter Tuning)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "df = df.dropna(subset=['ip', 'timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Basic + Advanced)\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# Advanced time features\n",
    "feat['minute_of_hour'] = df['timestamp'].dt.minute\n",
    "feat['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# unique_users_last_5m\n",
    "def unique_users_last_5m(df):\n",
    "    df_sorted = df.sort_values(['ip','timestamp'])\n",
    "    result = []\n",
    "    for ip, group in df_sorted.groupby('ip'):\n",
    "        times = group['timestamp']\n",
    "        users = group['user']\n",
    "        dq = deque()\n",
    "        counts = []\n",
    "        for t,u in zip(times,users):\n",
    "            dq.append((t,u))\n",
    "            while dq and (t - dq[0][0]).total_seconds() > 300:\n",
    "                dq.popleft()\n",
    "            counts.append(len(set([x[1] for x in dq])))\n",
    "        result.extend(counts)\n",
    "    return pd.Series(result, index=df_sorted.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = unique_users_last_5m(df)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# Encoding categorical\n",
    "feat = pd.get_dummies(feat, columns=['geo_country'], dummy_na=True)\n",
    "\n",
    "# Log-transform + clip numeric features\n",
    "numeric_cols = ['avg_interarrival','failed_streak','unique_users_last_5','unique_users_last_5m']\n",
    "for col in numeric_cols:\n",
    "    q_low = feat[col].quantile(0.01)\n",
    "    q_high = feat[col].quantile(0.99)\n",
    "    feat[col] = feat[col].clip(q_low, q_high)\n",
    "    feat[col+'_log'] = np.log1p(feat[col])\n",
    "\n",
    "# Feature matrix\n",
    "numeric_cols_final = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols_final: numeric_cols_final.remove('timestamp')\n",
    "X = feat[numeric_cols_final].fillna(0)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ RandomForest with RandomizedSearchCV\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "rs.fit(X_train, y_train)\n",
    "best_rf = rs.best_estimator_\n",
    "print(\"✅ Best RandomForest Params:\", rs.best_params_)\n",
    "\n",
    "# Evaluate RandomForest\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_proba = best_rf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred, target_names=['failed','success']))\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}, PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ XGBoost\n",
    "# ==============================\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight = (len(y_res) - sum(y_res)) / sum(y_res),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test)[:,1]\n",
    "print(\"📊 XGBoost Report\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['failed','success']))\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ LightGBM\n",
    "# ==============================\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_clf.predict(X_test)\n",
    "y_proba_lgb = lgb_clf.predict_proba(X_test)[:,1]\n",
    "print(\"📊 LightGBM Report\")\n",
    "print(classification_report(y_test, y_pred_lgb, target_names=['failed','success']))\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Save Best Model (RandomForest example)\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_tuned.joblib')\n",
    "joblib.dump({'model': best_rf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Best RandomForest model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024ece0-4800-4784-a89c-8a45e683a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script: Login anomaly analysis\n",
    "# (Advanced Feature Engineering + Model Evaluation + Hyperparameter Tuning)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "df = df.dropna(subset=['ip', 'timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Basic + Advanced)\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# Advanced time features\n",
    "feat['minute_of_hour'] = df['timestamp'].dt.minute\n",
    "feat['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# unique_users_last_5m\n",
    "def unique_users_last_5m(df):\n",
    "    df_sorted = df.sort_values(['ip','timestamp'])\n",
    "    result = []\n",
    "    for ip, group in df_sorted.groupby('ip'):\n",
    "        times = group['timestamp']\n",
    "        users = group['user']\n",
    "        dq = deque()\n",
    "        counts = []\n",
    "        for t,u in zip(times,users):\n",
    "            dq.append((t,u))\n",
    "            while dq and (t - dq[0][0]).total_seconds() > 300:\n",
    "                dq.popleft()\n",
    "            counts.append(len(set([x[1] for x in dq])))\n",
    "        result.extend(counts)\n",
    "    return pd.Series(result, index=df_sorted.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = unique_users_last_5m(df)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# Encoding categorical\n",
    "feat = pd.get_dummies(feat, columns=['geo_country'], dummy_na=True)\n",
    "\n",
    "# Log-transform + clip numeric features\n",
    "numeric_cols = ['avg_interarrival','failed_streak','unique_users_last_5','unique_users_last_5m']\n",
    "for col in numeric_cols:\n",
    "    q_low = feat[col].quantile(0.01)\n",
    "    q_high = feat[col].quantile(0.99)\n",
    "    feat[col] = feat[col].clip(q_low, q_high)\n",
    "    feat[col+'_log'] = np.log1p(feat[col])\n",
    "\n",
    "# Feature matrix\n",
    "numeric_cols_final = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols_final: numeric_cols_final.remove('timestamp')\n",
    "X = feat[numeric_cols_final].fillna(0)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ RandomForest with RandomizedSearchCV\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "rs.fit(X_train, y_train)\n",
    "best_rf = rs.best_estimator_\n",
    "print(\"✅ Best RandomForest Params:\", rs.best_params_)\n",
    "\n",
    "# Evaluate RandomForest\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_proba = best_rf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred, target_names=['failed','success']))\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}, PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ XGBoost\n",
    "# ==============================\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight = (len(y_res) - sum(y_res)) / sum(y_res),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test)[:,1]\n",
    "print(\"📊 XGBoost Report\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['failed','success']))\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ LightGBM\n",
    "# ==============================\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_clf.predict(X_test)\n",
    "y_proba_lgb = lgb_clf.predict_proba(X_test)[:,1]\n",
    "print(\"📊 LightGBM Report\")\n",
    "print(classification_report(y_test, y_pred_lgb, target_names=['failed','success']))\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Save Best Model (RandomForest example)\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_tuned.joblib')\n",
    "joblib.dump({'model': best_rf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Best RandomForest model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0281dd-099a-4f7a-bd48-4f2d641a882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script: Login anomaly analysis\n",
    "# (Final version: Feature Engineering + Hyperparameter Tuning + Safe Predictions)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "df = df.dropna(subset=['ip', 'timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Basic + Advanced)\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# Advanced time features\n",
    "feat['minute_of_hour'] = df['timestamp'].dt.minute\n",
    "feat['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# unique_users_last_5m\n",
    "def unique_users_last_5m(df):\n",
    "    df_sorted = df.sort_values(['ip','timestamp'])\n",
    "    result = []\n",
    "    for ip, group in df_sorted.groupby('ip'):\n",
    "        times = group['timestamp']\n",
    "        users = group['user']\n",
    "        dq = deque()\n",
    "        counts = []\n",
    "        for t,u in zip(times,users):\n",
    "            dq.append((t,u))\n",
    "            while dq and (t - dq[0][0]).total_seconds() > 300:\n",
    "                dq.popleft()\n",
    "            counts.append(len(set([x[1] for x in dq])))\n",
    "        result.extend(counts)\n",
    "    return pd.Series(result, index=df_sorted.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = unique_users_last_5m(df)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# Encoding categorical\n",
    "feat = pd.get_dummies(feat, columns=['geo_country'], dummy_na=True)\n",
    "\n",
    "# Log-transform + clip numeric features\n",
    "numeric_cols = ['avg_interarrival','failed_streak','unique_users_last_5','unique_users_last_5m']\n",
    "for col in numeric_cols:\n",
    "    q_low = feat[col].quantile(0.01)\n",
    "    q_high = feat[col].quantile(0.99)\n",
    "    feat[col] = feat[col].clip(q_low, q_high)\n",
    "    feat[col+'_log'] = np.log1p(feat[col])\n",
    "\n",
    "# Feature matrix\n",
    "numeric_cols_final = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols_final: numeric_cols_final.remove('timestamp')\n",
    "X = feat[numeric_cols_final].fillna(0)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# Convert X_test back to DataFrame for safe predictions\n",
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ RandomForest with RandomizedSearchCV\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "rs.fit(X_train_df, y_train)\n",
    "best_rf = rs.best_estimator_\n",
    "print(\"✅ Best RandomForest Params:\", rs.best_params_)\n",
    "\n",
    "y_pred_rf = best_rf.predict(X_test_df)\n",
    "y_proba_rf = best_rf.predict_proba(X_test_df)[:,1]\n",
    "print(\"📊 RandomForest Report\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['failed','success']))\n",
    "roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, y_proba_rf)\n",
    "pr_auc_rf = auc(recall_rf, precision_rf)\n",
    "print(f\"ROC AUC: {roc_auc_rf:.4f}, PR AUC: {pr_auc_rf:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ XGBoost\n",
    "# ==============================\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight = (len(y_res) - sum(y_res)) / sum(y_res),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_clf.fit(X_train_df, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_df)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test_df)[:,1]\n",
    "print(\"📊 XGBoost Report\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['failed','success']))\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ LightGBM\n",
    "# ==============================\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_clf.fit(X_train_df, y_train)\n",
    "y_pred_lgb = lgb_clf.predict(X_test_df)\n",
    "y_proba_lgb = lgb_clf.predict_proba(X_test_df)[:,1]\n",
    "print(\"📊 LightGBM Report\")\n",
    "print(classification_report(y_test, y_pred_lgb, target_names=['failed','success']))\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Save Best Model (RandomForest example)\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_tuned_safe.joblib')\n",
    "joblib.dump({'model': best_rf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Best RandomForest model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf7339-c743-4a68-92dd-4d6e3b2e4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script: Login anomaly analysis\n",
    "# (Final version with Plots + Safe Predictions)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "df = df.dropna(subset=['ip', 'timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# Plot target distribution\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Basic + Advanced)\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "def compute_failed_streak(series):\n",
    "    streak = (series != 'failed').cumsum()\n",
    "    return series.groupby(streak).cumcount().where(series=='failed', 0) + 1*(series=='failed')\n",
    "\n",
    "df['failed_streak'] = df.groupby('user')['result'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# Advanced time features\n",
    "feat['minute_of_hour'] = df['timestamp'].dt.minute\n",
    "feat['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# unique_users_last_5m\n",
    "def unique_users_last_5m(df):\n",
    "    df_sorted = df.sort_values(['ip','timestamp'])\n",
    "    result = []\n",
    "    for ip, group in df_sorted.groupby('ip'):\n",
    "        times = group['timestamp']\n",
    "        users = group['user']\n",
    "        dq = deque()\n",
    "        counts = []\n",
    "        for t,u in zip(times,users):\n",
    "            dq.append((t,u))\n",
    "            while dq and (t - dq[0][0]).total_seconds() > 300:\n",
    "                dq.popleft()\n",
    "            counts.append(len(set([x[1] for x in dq])))\n",
    "        result.extend(counts)\n",
    "    return pd.Series(result, index=df_sorted.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = unique_users_last_5m(df)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# Encoding categorical\n",
    "feat = pd.get_dummies(feat, columns=['geo_country'], dummy_na=True)\n",
    "\n",
    "# Log-transform + clip numeric features\n",
    "numeric_cols = ['avg_interarrival','failed_streak','unique_users_last_5','unique_users_last_5m']\n",
    "for col in numeric_cols:\n",
    "    q_low = feat[col].quantile(0.01)\n",
    "    q_high = feat[col].quantile(0.99)\n",
    "    feat[col] = feat[col].clip(q_low, q_high)\n",
    "    feat[col+'_log'] = np.log1p(feat[col])\n",
    "\n",
    "# Feature matrix\n",
    "numeric_cols_final = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols_final: numeric_cols_final.remove('timestamp')\n",
    "X = feat[numeric_cols_final].fillna(0)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# Convert X_train/test to DataFrame for safe predictions\n",
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ RandomForest with RandomizedSearchCV\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "rs.fit(X_train_df, y_train)\n",
    "best_rf = rs.best_estimator_\n",
    "print(\"✅ Best RandomForest Params:\", rs.best_params_)\n",
    "\n",
    "# Evaluate RF\n",
    "y_pred_rf = best_rf.predict(X_test_df)\n",
    "y_proba_rf = best_rf.predict_proba(X_test_df)[:,1]\n",
    "print(\"📊 RandomForest Report\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['failed','success']))\n",
    "roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, y_proba_rf)\n",
    "pr_auc_rf = auc(recall_rf, precision_rf)\n",
    "print(f\"ROC AUC: {roc_auc_rf:.4f}, PR AUC: {pr_auc_rf:.4f}\")\n",
    "\n",
    "# PR Curve plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall_rf, precision_rf, label=f'PR AUC = {pr_auc_rf:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('RandomForest Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance plot\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features (RandomForest)\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ XGBoost\n",
    "# ==============================\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight = (len(y_res) - sum(y_res)) / sum(y_res),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_clf.fit(X_train_df, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_df)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test_df)[:,1]\n",
    "print(\"📊 XGBoost Report\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['failed','success']))\n",
    "\n",
    "# PR Curve for XGBoost\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, y_proba_xgb)\n",
    "pr_auc_xgb = auc(recall_xgb, precision_xgb)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall_xgb, precision_xgb, label=f'PR AUC = {pr_auc_xgb:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('XGBoost Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ LightGBM\n",
    "# ==============================\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_clf.fit(X_train_df, y_train)\n",
    "y_pred_lgb = lgb_clf.predict(X_test_df)\n",
    "y_proba_lgb = lgb_clf.predict_proba(X_test_df)[:,1]\n",
    "print(\"📊 LightGBM Report\")\n",
    "print(classification_report(y_test, y_pred_lgb, target_names=['failed','success']))\n",
    "\n",
    "# PR Curve for LightGBM\n",
    "precision_lgb, recall_lgb, _ = precision_recall_curve(y_test, y_proba_lgb)\n",
    "pr_auc_lgb = auc(recall_lgb, precision_lgb)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall_lgb, precision_lgb, label=f'PR AUC = {pr_auc_lgb:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('LightGBM Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Save Best Model (RandomForest example)\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_tuned_safe_withplots.joblib')\n",
    "joblib.dump({'model': best_rf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Best RandomForest model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0269e9-3f4b-40bb-9f9a-2f510884e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script: Login anomaly analysis (Optimized + Plots + Cache)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "GEO_CACHE_FILE = os.path.join(BASE_DIR, 'geo_cache.pkl')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "df = df.dropna(subset=['ip','timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# Plot target distribution\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Feature Engineering (Vectorized + Optimized)\n",
    "# ==============================\n",
    "# Time features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "feat['minute_of_hour'] = df['timestamp'].dt.minute\n",
    "feat['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# Average interarrival time\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak vectorized\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].apply(lambda x: x.groupby((x==0).cumsum()).cumsum())\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts (vectorized approximation)\n",
    "feat['unique_users_last_5'] = df.groupby('ip')['user'].rolling(5, min_periods=1).apply(lambda x: len(set(x)), raw=False).reset_index(level=0, drop=True)\n",
    "\n",
    "# Unique users last 5 minutes (approximation using rolling 5min)\n",
    "df['timestamp_round'] = df['timestamp'].dt.floor('min')\n",
    "feat['unique_users_last_5m'] = (\n",
    "    df.set_index('timestamp_round')\n",
    "      .groupby('ip')['user']\n",
    "      .rolling('5min')\n",
    "      .apply(lambda x: len(set(x)))\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ GeoIP Features with Cache\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "if os.path.exists(GEO_CACHE_FILE):\n",
    "    with open(GEO_CACHE_FILE,'rb') as f:\n",
    "        geo_cache = pickle.load(f)\n",
    "else:\n",
    "    geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# Save cache\n",
    "with open(GEO_CACHE_FILE,'wb') as f:\n",
    "    pickle.dump(geo_cache,f)\n",
    "\n",
    "# One-hot encode countries\n",
    "feat = pd.get_dummies(feat, columns=['geo_country'], dummy_na=True)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Log-transform + Clip numeric\n",
    "# ==============================\n",
    "num_cols = ['avg_interarrival','failed_streak','unique_users_last_5','unique_users_last_5m']\n",
    "feat[num_cols] = feat[num_cols].clip(lower=feat[num_cols].quantile(0.01), upper=feat[num_cols].quantile(0.99))\n",
    "feat[[c+'_log' for c in num_cols]] = np.log1p(feat[num_cols])\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols_final = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols_final: numeric_cols_final.remove('timestamp')\n",
    "X = feat[numeric_cols_final].fillna(0)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for safe predictions\n",
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ RandomForest (with RandomizedSearchCV)\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth':[5,10,20,None],\n",
    "    'min_samples_split':[2,5,10],\n",
    "    'min_samples_leaf':[1,2,4],\n",
    "    'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "rs.fit(X_train_df, y_train)\n",
    "best_rf = rs.best_estimator_\n",
    "print(\"✅ Best RandomForest Params:\", rs.best_params_)\n",
    "\n",
    "# Evaluate RF\n",
    "y_pred_rf = best_rf.predict(X_test_df)\n",
    "y_proba_rf = best_rf.predict_proba(X_test_df)[:,1]\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['failed','success']))\n",
    "roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_proba_rf)\n",
    "pr_auc_rf = auc(recall_rf, precision_rf)\n",
    "\n",
    "# PR Curve plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall_rf, precision_rf, label=f'PR AUC = {pr_auc_rf:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('RandomForest Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features (RandomForest)\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ XGBoost\n",
    "# ==============================\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=(len(y_res)-sum(y_res))/sum(y_res),\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_clf.fit(X_train_df, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_df)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test_df)[:,1]\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['failed','success']))\n",
    "\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, y_proba_xgb)\n",
    "pr_auc_xgb = auc(recall_xgb, precision_xgb)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall_xgb, precision_xgb, label=f'PR AUC = {pr_auc_xgb:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('XGBoost Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ LightGBM\n",
    "# ==============================\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_clf.fit(X_train_df, y_train)\n",
    "y_pred_lgb = lgb_clf.predict(X_test_df)\n",
    "y_proba_lgb = lgb_clf.predict_proba(X_test_df)[:,1]\n",
    "print(classification_report(y_test, y_pred_lgb, target_names=['failed','success']))\n",
    "\n",
    "precision_lgb, recall_lgb, _ = precision_recall_curve(y_test, y_proba_lgb)\n",
    "pr_auc_lgb = auc(recall_lgb, precision_lgb)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall_lgb, precision_lgb, label=f'PR AUC = {pr_auc_lgb:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('LightGBM Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Save Best Model (RandomForest example)\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final_optimized.joblib')\n",
    "joblib.dump({'model': best_rf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Best RandomForest model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368594c7-594f-4fb9-99f9-aa0e00d4a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script: Login anomaly analysis (Optimized + Plots + Cache)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "GEO_CACHE_FILE = os.path.join(BASE_DIR, 'geo_cache.pkl')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "df = df.dropna(subset=['ip','timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Feature Engineering (Vectorized + Optimized)\n",
    "# ==============================\n",
    "# Time features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "feat['minute_of_hour'] = df['timestamp'].dt.minute\n",
    "feat['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# Average interarrival time\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak vectorized (تصحيح الخطأ)\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(\n",
    "    lambda x: x.groupby((x==0).cumsum()).cumsum()\n",
    ")\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts (vectorized approximation)\n",
    "feat['unique_users_last_5'] = df.groupby('ip')['user'].rolling(5, min_periods=1).apply(lambda x: len(set(x)), raw=False).reset_index(level=0, drop=True)\n",
    "\n",
    "# Unique users last 5 minutes (approximation using rolling 5min)\n",
    "df['timestamp_round'] = df['timestamp'].dt.floor('min')\n",
    "feat['unique_users_last_5m'] = (\n",
    "    df.set_index('timestamp_round')\n",
    "      .groupby('ip')['user']\n",
    "      .rolling('5min')\n",
    "      .apply(lambda x: len(set(x)))\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ GeoIP Features with Cache\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "\n",
    "if os.path.exists(GEO_CACHE_FILE):\n",
    "    with open(GEO_CACHE_FILE,'rb') as f:\n",
    "        geo_cache = pickle.load(f)\n",
    "else:\n",
    "    geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# Save cache\n",
    "with open(GEO_CACHE_FILE,'wb') as f:\n",
    "    pickle.dump(geo_cache,f)\n",
    "\n",
    "# One-hot encode countries\n",
    "feat = pd.get_dummies(feat, columns=['geo_country'], dummy_na=True)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Log-transform + Clip numeric\n",
    "# ==============================\n",
    "num_cols = ['avg_interarrival','failed_streak','unique_users_last_5','unique_users_last_5m']\n",
    "feat[num_cols] = feat[num_cols].clip(lower=feat[num_cols].quantile(0.01), upper=feat[num_cols].quantile(0.99))\n",
    "feat[[c+'_log' for c in num_cols]] = np.log1p(feat[num_cols])\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols_final = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols_final: numeric_cols_final.remove('timestamp')\n",
    "X = feat[numeric_cols_final].fillna(0)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ RandomForest (with RandomizedSearchCV)\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth':[5,10,20,None],\n",
    "    'min_samples_split':[2,5,10],\n",
    "    'min_samples_leaf':[1,2,4],\n",
    "    'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "rs.fit(X_train_df, y_train)\n",
    "best_rf = rs.best_estimator_\n",
    "print(\"✅ Best RandomForest Params:\", rs.best_params_)\n",
    "\n",
    "# Evaluate RF\n",
    "y_pred_rf = best_rf.predict(X_test_df)\n",
    "y_proba_rf = best_rf.predict_proba(X_test_df)[:,1]\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['failed','success']))\n",
    "roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_proba_rf)\n",
    "pr_auc_rf = auc(recall_rf, precision_rf)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall_rf, precision_rf, label=f'PR AUC = {pr_auc_rf:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('RandomForest Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features (RandomForest)\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Save Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final_vectorized.joblib')\n",
    "joblib.dump({'model': best_rf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Best RandomForest model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210cd57-2dbc-4ffd-a469-5ef0bf413364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script: Login anomaly analysis (Vectorized + GeoIP + Safe)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "GEO_CACHE_FILE = os.path.join(BASE_DIR, 'geo_cache.pkl')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df = df.dropna(subset=['ip','timestamp'])\n",
    "feat = feat.dropna(subset=['timestamp'])\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Time features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "feat['minute_of_hour'] = df['timestamp'].dt.minute\n",
    "feat['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user (vectorized)\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(lambda x: x.groupby((x==0).cumsum()).cumsum())\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts per IP (deque)\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "feat['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5).fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 minutes per IP\n",
    "def unique_users_last_5min(group):\n",
    "    group = group.sort_values('timestamp')\n",
    "    users_list = []\n",
    "    dq_users = deque()\n",
    "    dq_times = deque()\n",
    "    for ts, user in zip(group['timestamp'], group['user']):\n",
    "        # إزالة المستخدمين خارج نافذة 5 دقائق\n",
    "        while dq_times and (ts - dq_times[0]).total_seconds() > 300:\n",
    "            dq_users.popleft()\n",
    "            dq_times.popleft()\n",
    "        dq_users.append(user)\n",
    "        dq_times.append(ts)\n",
    "        users_list.append(len(set(dq_users)))\n",
    "    return pd.Series(users_list, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = df.groupby('ip').apply(unique_users_last_5min).sort_index()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ GeoIP Features with Cache\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "if os.path.exists(GEO_CACHE_FILE):\n",
    "    with open(GEO_CACHE_FILE,'rb') as f:\n",
    "        geo_cache = pickle.load(f)\n",
    "else:\n",
    "    geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# Save cache\n",
    "with open(GEO_CACHE_FILE,'wb') as f:\n",
    "    pickle.dump(geo_cache,f)\n",
    "\n",
    "# One-hot encode countries\n",
    "feat = pd.get_dummies(feat, columns=['geo_country'], dummy_na=True)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Log-transform + Clip numeric\n",
    "# ==============================\n",
    "num_cols = ['avg_interarrival','failed_streak','unique_users_last_5','unique_users_last_5m']\n",
    "feat[num_cols] = feat[num_cols].clip(lower=feat[num_cols].quantile(0.01), upper=feat[num_cols].quantile(0.99))\n",
    "feat[[c+'_log' for c in num_cols]] = np.log1p(feat[num_cols])\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols_final = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols_final: numeric_cols_final.remove('timestamp')\n",
    "X = feat[numeric_cols_final].fillna(0)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Train RandomForest (RandomizedSearchCV)\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth':[5,10,20,None],\n",
    "    'min_samples_split':[2,5,10],\n",
    "    'min_samples_leaf':[1,2,4],\n",
    "    'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='recall',\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "rs.fit(X_train, y_train)\n",
    "best_rf = rs.best_estimator_\n",
    "print(\"✅ Best RandomForest Params:\", rs.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_proba = best_rf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred, target_names=['failed','success']))\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('RandomForest Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features (RandomForest)\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Save Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final_safe.joblib')\n",
    "joblib.dump({'model': best_rf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Best RandomForest model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2adb14-7058-48d0-8343-4327094ec1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized & Fixed)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# --- Hour and night features ---\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# --- Average interarrival time per IP ---\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# --- Failed streak per user ---\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(\n",
    "    lambda x: x.groupby((x==0).cumsum()).cumsum()\n",
    ")\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users last 5 attempts per IP ---\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# --- Unique users in last 5 minutes per IP (sliding window) ---\n",
    "def unique_users_last_5min(group):\n",
    "    group = group.sort_values('timestamp')\n",
    "    users_list = []\n",
    "    dq_users = deque()\n",
    "    dq_times = deque()\n",
    "    for ts, user in zip(group['timestamp'], group['user']):\n",
    "        while dq_times and (ts - dq_times[0]).total_seconds() > 300:\n",
    "            dq_users.popleft()\n",
    "            dq_times.popleft()\n",
    "        dq_users.append(user)\n",
    "        dq_times.append(ts)\n",
    "        users_list.append(len(set(dq_users)))\n",
    "    return pd.Series(users_list, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = pd.concat(\n",
    "    [unique_users_last_5min(g) for _, g in df.groupby('ip')]\n",
    ").sort_index()\n",
    "\n",
    "# --- GeoIP Features (Parallel + Cache) ---\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(\n",
    "    delayed(geoip_lookup)(ip) for ip in unique_ips\n",
    ")\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2445144-3bea-415a-9fcf-66f2e9b27c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# High-Performance Login Anomaly Script\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\\n\", y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (High-Performance)\n",
    "# ==============================\n",
    "\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Avg interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(\n",
    "    lambda x: x.groupby((x==0).cumsum()).cumsum()\n",
    ")\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts per IP (parallel)\n",
    "def unique_last_5(group):\n",
    "    arr = group['user'].tolist()\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for u in arr:\n",
    "        dq.append(u)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5'] = pd.concat(\n",
    "    Parallel(n_jobs=-1)(delayed(unique_last_5)(g) for _, g in df.groupby('ip'))\n",
    ").sort_index()\n",
    "\n",
    "# Unique users in last 5 minutes per IP (parallel sliding window)\n",
    "def unique_last_5min(group):\n",
    "    group = group.sort_values('timestamp')\n",
    "    dq_users, dq_times = deque(), deque()\n",
    "    counts = []\n",
    "    for ts, u in zip(group['timestamp'], group['user']):\n",
    "        while dq_times and (ts - dq_times[0]).total_seconds() > 300:\n",
    "            dq_times.popleft()\n",
    "            dq_users.popleft()\n",
    "        dq_times.append(ts)\n",
    "        dq_users.append(u)\n",
    "        counts.append(len(set(dq_users)))\n",
    "    return pd.Series(counts, index=group.index)\n",
    "\n",
    "feat['unique_users_last_5m'] = pd.concat(\n",
    "    Parallel(n_jobs=-1)(delayed(unique_last_5min)(g) for _, g in df.groupby('ip'))\n",
    ").sort_index()\n",
    "\n",
    "# GeoIP features (parallel + cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country, city, lat, lon = r.country.iso_code, r.city.name, r.location.latitude, r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "if len(np.unique(y))>1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'PR AUC={pr_auc:.4f}')\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.legend(); plt.show()\n",
    "\n",
    "sns.countplot(x=y); plt.title(\"Target distribution\"); plt.show()\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10]); plt.title(\"Top 10 Features\"); plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final_hp.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc85973-842f-4049-b2a2-9cf68025b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized with Parallel GeoIP + Stronger Models)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1a️⃣ Data Quality Check\n",
    "# ==============================\n",
    "print(\"\\n--- Missing Values in auth_parsed ---\")\n",
    "print(df.isnull().mean())\n",
    "print(\"\\n--- Result Distribution ---\")\n",
    "print(df['result'].value_counts())\n",
    "print(\"\\n--- auth_features Statistics ---\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].apply(lambda x: x.groupby((x==0).cumsum()).cumsum())\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts (deque)\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# ==============================\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "# ==============================\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Stronger Models + Hyperparameter Tuning\n",
    "# ==============================\n",
    "# RandomForest\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "param_dist_rf = {'n_estimators':[50,100,200], 'max_depth':[5,10,20,None], 'min_samples_split':[2,5,10]}\n",
    "rs_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=10, cv=3, scoring='recall', n_jobs=-1, random_state=42)\n",
    "rs_rf.fit(X_train, y_train)\n",
    "print(\"✅ RandomForest Best Params:\", rs_rf.best_params_)\n",
    "\n",
    "rf_cv_scores = cross_val_score(rs_rf.best_estimator_, X_res, y_res, cv=5, scoring='recall', n_jobs=-1)\n",
    "print(\"RandomForest CV Recall Scores:\", rf_cv_scores)\n",
    "print(\"RandomForest Mean Recall:\", np.mean(rf_cv_scores))\n",
    "\n",
    "# LightGBM\n",
    "lgbm = LGBMClassifier(class_weight='balanced', random_state=42)\n",
    "param_dist_lgbm = {'n_estimators':[50,100,200], 'max_depth':[5,10,20,-1], 'learning_rate':[0.01,0.05,0.1]}\n",
    "rs_lgbm = RandomizedSearchCV(lgbm, param_distributions=param_dist_lgbm, n_iter=10, cv=3, scoring='recall', n_jobs=-1, random_state=42)\n",
    "rs_lgbm.fit(X_train, y_train)\n",
    "print(\"✅ LightGBM Best Params:\", rs_lgbm.best_params_)\n",
    "\n",
    "lgbm_cv_scores = cross_val_score(rs_lgbm.best_estimator_, X_res, y_res, cv=5, scoring='recall', n_jobs=-1)\n",
    "print(\"LightGBM CV Recall Scores:\", lgbm_cv_scores)\n",
    "print(\"LightGBM Mean Recall:\", np.mean(lgbm_cv_scores))\n",
    "\n",
    "# اختيار النموذج النهائي\n",
    "if np.mean(rf_cv_scores) >= np.mean(lgbm_cv_scores):\n",
    "    final_model = rs_rf.best_estimator_\n",
    "    print(\"✅ Final model selected: RandomForest\")\n",
    "else:\n",
    "    final_model = rs_lgbm.best_estimator_\n",
    "    print(\"✅ Final model selected: LightGBM\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Save the final model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'final_model.joblib')\n",
    "joblib.dump({'model': final_model, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Final model saved at: {MODEL_PATH}\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Evaluate Final Model\n",
    "# ==============================\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_proba = final_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(final_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44b8e7-a982-4be4-8b41-322584bce74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized + Fixed Index Issues)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user (fixed index issue)\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Model trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Model\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6eccb5-9bcc-4c69-9bb2-994f2220c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import geoip2.database\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv('data/auth_parsed.csv', parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv('data/auth_features.csv', parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & Night Features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average Interarrival Time\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed Streak per User\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(\n",
    "    lambda x: x.groupby((x==0).cumsum()).cumcount() + 1*(x==1)\n",
    ")\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique Users Last 5 Attempts\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return counts\n",
    "\n",
    "feat['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = feat['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features with Cache\n",
    "GEOIP_FILE = 'GeoLite2-City.mmdb'  # ضع مسار قاعدة GeoIP\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = [geoip_lookup(ip) for ip in unique_ips]\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# Ensure Numeric Columns\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ RandomForest with GridSearch\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators':[50,100,200],\n",
    "    'max_depth':[5,10,20,None],\n",
    "    'min_samples_split':[2,5,10]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rs = RandomizedSearchCV(rf, param_dist, n_iter=10, cv=3, scoring='recall', n_jobs=-1)\n",
    "rs.fit(X_res, y_res)\n",
    "\n",
    "best_rf = rs.best_estimator_\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Evaluation\n",
    "# ==============================\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_proba = best_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Best RF params:\", rs.best_params_)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ XGBoost (مثال)\n",
    "# ==============================\n",
    "xgb_model = xgb.XGBClassifier(scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(),\n",
    "                              use_label_encoder=False,\n",
    "                              eval_metric='logloss',\n",
    "                              random_state=42)\n",
    "xgb_model.fit(X_res, y_res)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ffe01-d6ba-45be-afb8-12c70b0fe062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized + Stronger Models)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user (fixed index issue)\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Random Forest Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"✅ Random Forest trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Random Forest\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Random Forest Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Train Stronger Models (LightGBM & XGBoost) + Hyperparameter Tuning\n",
    "# ==============================\n",
    "\n",
    "# -------- LightGBM --------\n",
    "lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42, n_jobs=-1)\n",
    "lgb_params = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [-1, 5, 10, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500]\n",
    "}\n",
    "lgb_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=lgb_params,\n",
    "    n_iter=10,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_search.fit(X_train, y_train)\n",
    "best_lgb = lgb_search.best_estimator_\n",
    "\n",
    "y_pred_lgb = best_lgb.predict(X_test)\n",
    "y_proba_lgb = best_lgb.predict_proba(X_test)[:,1]\n",
    "print(\"✅ LightGBM Best Params:\", lgb_search.best_params_)\n",
    "print(\"Classification Report (LightGBM):\")\n",
    "print(classification_report(y_test, y_pred_lgb))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_lgb))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba_lgb)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# -------- XGBoost --------\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42, n_jobs=-1, use_label_encoder=False)\n",
    "xgb_params = {\n",
    "    'max_depth': [3, 5, 10, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'subsample': [0.7, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.9, 1.0]\n",
    "}\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=10,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_search.fit(X_train, y_train)\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "y_proba_xgb = best_xgb.predict_proba(X_test)[:,1]\n",
    "print(\"✅ XGBoost Best Params:\", xgb_search.best_params_)\n",
    "print(\"Classification Report (XGBoost):\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_xgb))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba_xgb)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Best Models\n",
    "# ==============================\n",
    "MODEL_PATH_RF = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "MODEL_PATH_LGB = os.path.join(BASE_DIR, 'lgb_model_final.joblib')\n",
    "MODEL_PATH_XGB = os.path.join(BASE_DIR, 'xgb_model_final.joblib')\n",
    "\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH_RF)\n",
    "joblib.dump({'model': best_lgb, 'scaler': scaler}, MODEL_PATH_LGB)\n",
    "joblib.dump({'model': best_xgb, 'scaler': scaler}, MODEL_PATH_XGB)\n",
    "\n",
    "print(f\"✅ Random Forest model saved at: {MODEL_PATH_RF}\")\n",
    "print(f\"✅ LightGBM model saved at: {MODEL_PATH_LGB}\")\n",
    "print(f\"✅ XGBoost model saved at: {MODEL_PATH_XGB}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ce2e9-f232-4d85-87c6-27dcc02b279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized + Multi-Model Comparison)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user (fixed index issue)\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "best_lgb = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train,lgb_eval], verbose_eval=False)\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "best_xgb = xgb.train(xgb_params, xgb_train, evals=[(xgb_train,'train'),(xgb_test,'test')], verbose_eval=False)\n",
    "\n",
    "print(\"✅ All models trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate & Compare Models\n",
    "# ==============================\n",
    "models = {'Random Forest': clf, 'LightGBM': best_lgb, 'XGBoost': best_xgb}\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve Comparison\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    if name=='Random Forest':\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    elif name=='LightGBM':\n",
    "        y_proba = model.predict(X_test)\n",
    "    else:  # XGBoost\n",
    "        y_proba = model.predict(xgb_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC={roc_auc_score(y_test, y_proba):.3f})')\n",
    "\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# PR Curve Comparison\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    if name=='Random Forest':\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    elif name=='LightGBM':\n",
    "        y_proba = model.predict(X_test)\n",
    "    else:  # XGBoost\n",
    "        y_proba = model.predict(xgb_test)\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    plt.plot(rec, prec, label=f'{name} (PR AUC={auc(rec, prec):.3f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve Comparison')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Best Model (Random Forest)\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44514906-3cfe-45e8-ac8c-1942b7158276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized + Multi-Model Comparison)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user (fixed index issue)\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "best_lgb = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train,lgb_eval], verbose_eval=False)\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "best_xgb = xgb.train(xgb_params, xgb_train, evals=[(xgb_train,'train'),(xgb_test,'test')], verbose_eval=False)\n",
    "\n",
    "print(\"✅ All models trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate & Compare Models\n",
    "# ==============================\n",
    "models = {'Random Forest': clf, 'LightGBM': best_lgb, 'XGBoost': best_xgb}\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve Comparison\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    if name=='Random Forest':\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    elif name=='LightGBM':\n",
    "        y_proba = model.predict(X_test)\n",
    "    else:  # XGBoost\n",
    "        y_proba = model.predict(xgb_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC={roc_auc_score(y_test, y_proba):.3f})')\n",
    "\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# PR Curve Comparison\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    if name=='Random Forest':\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    elif name=='LightGBM':\n",
    "        y_proba = model.predict(X_test)\n",
    "    else:  # XGBoost\n",
    "        y_proba = model.predict(xgb_test)\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    plt.plot(rec, prec, label=f'{name} (PR AUC={auc(rec, prec):.3f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve Comparison')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Best Model (Random Forest)\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c775450-ba81-438b-b3dd-7712a520d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized + Multi-Model Comparison)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user (fixed index issue)\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "best_lgb = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train,lgb_eval], verbose_eval=False)\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "best_xgb = xgb.train(xgb_params, xgb_train, evals=[(xgb_train,'train'),(xgb_test,'test')], verbose_eval=False)\n",
    "\n",
    "print(\"✅ All models trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate & Compare Models\n",
    "# ==============================\n",
    "models = {'Random Forest': clf, 'LightGBM': best_lgb, 'XGBoost': best_xgb}\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve Comparison\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    if name=='Random Forest':\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    elif name=='LightGBM':\n",
    "        y_proba = model.predict(X_test)\n",
    "    else:  # XGBoost\n",
    "        y_proba = model.predict(xgb_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC={roc_auc_score(y_test, y_proba):.3f})')\n",
    "\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# PR Curve Comparison\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    if name=='Random Forest':\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    elif name=='LightGBM':\n",
    "        y_proba = model.predict(X_test)\n",
    "    else:  # XGBoost\n",
    "        y_proba = model.predict(xgb_test)\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    plt.plot(rec, prec, label=f'{name} (PR AUC={auc(rec, prec):.3f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve Comparison')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Best Model (Random Forest)\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133ed74-a0f1-4714-85df-85aa337985d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized + Multi-Model Comparison)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (Optimized)\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user (fixed index issue)\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "best_lgb = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train,lgb_eval], verbose_eval=False)\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "best_xgb = xgb.train(xgb_params, xgb_train, evals=[(xgb_train,'train'),(xgb_test,'test')], verbose_eval=False)\n",
    "\n",
    "print(\"✅ All models trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate & Compare Models\n",
    "# ==============================\n",
    "models = {'Random Forest': clf, 'LightGBM': best_lgb, 'XGBoost': best_xgb}\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC Curve Comparison\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    if name=='Random Forest':\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    elif name=='LightGBM':\n",
    "        y_proba = model.predict(X_test)\n",
    "    else:  # XGBoost\n",
    "        y_proba = model.predict(xgb_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC={roc_auc_score(y_test, y_proba):.3f})')\n",
    "\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# PR Curve Comparison\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    if name=='Random Forest':\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    elif name=='LightGBM':\n",
    "        y_proba = model.predict(X_test)\n",
    "    else:  # XGBoost\n",
    "        y_proba = model.predict(xgb_test)\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    plt.plot(rec, prec, label=f'{name} (PR AUC={auc(rec, prec):.3f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve Comparison')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Analyze Features & Target Distribution\n",
    "# ==============================\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Target class distribution\")\n",
    "plt.show()\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save the Best Model (Random Forest)\n",
    "# ==============================\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH)\n",
    "print(f\"✅ Model saved at: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d3043-f665-42ca-8d68-93ccd0611876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full script for login anomaly analysis (Optimized + Advanced Models)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "best_lgb = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train, lgb_eval], callbacks=[lgb.log_evaluation(period=0)])\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "best_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=100, evals=[(xgb_test,'test')], verbose_eval=False)\n",
    "\n",
    "print(\"✅ All models trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Random Forest\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Models\n",
    "# ==============================\n",
    "MODEL_PATH_RF = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_PATH_RF)\n",
    "print(f\"✅ Random Forest saved at: {MODEL_PATH_RF}\")\n",
    "\n",
    "MODEL_PATH_LGB = os.path.join(BASE_DIR, 'lightgbm_model_final.txt')\n",
    "best_lgb.save_model(MODEL_PATH_LGB)\n",
    "print(f\"✅ LightGBM saved at: {MODEL_PATH_LGB}\")\n",
    "\n",
    "MODEL_PATH_XGB = os.path.join(BASE_DIR, 'xgboost_model_final.json')\n",
    "best_xgb.save_model(MODEL_PATH_XGB)\n",
    "print(f\"✅ XGBoost saved at: {MODEL_PATH_XGB}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11a1a0-9d36-453d-bdf3-72308c1f04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Pipeline: Login Anomaly Analysis with Hyperparameter Tuning\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed, dump\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Random Forest - Hyperparameter Tuning\n",
    "# ==============================\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "rf_random_search = RandomizedSearchCV(rf_clf, rf_param_dist, n_iter=20, scoring='recall',\n",
    "                                      cv=3, verbose=1, random_state=42)\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "best_rf = rf_random_search.best_estimator_\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ LightGBM - Hyperparameter Tuning\n",
    "# ==============================\n",
    "lgb_param_dist = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'min_child_samples': [5, 10, 20]\n",
    "}\n",
    "lgb_clf = lgb.LGBMClassifier(objective='binary', class_weight='balanced', n_jobs=-1)\n",
    "lgb_random_search = RandomizedSearchCV(lgb_clf, lgb_param_dist, n_iter=20, scoring='recall',\n",
    "                                       cv=StratifiedKFold(n_splits=3), verbose=1, random_state=42)\n",
    "lgb_random_search.fit(X_train, y_train)\n",
    "best_lgb = lgb_random_search.best_estimator_\n",
    "\n",
    "# ==============================\n",
    "# 🔟 XGBoost - Hyperparameter Tuning\n",
    "# ==============================\n",
    "xgb_param_dist = {\n",
    "    'max_depth': [3, 5, 10, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'scale_pos_weight': [1, sum(y_train==0)/sum(y_train==1)]\n",
    "}\n",
    "xgb_clf = xgb.XGBClassifier(objective='binary:logistic', eval_metric='auc', use_label_encoder=False, n_jobs=-1)\n",
    "xgb_random_search = RandomizedSearchCV(xgb_clf, xgb_param_dist, n_iter=20, scoring='recall',\n",
    "                                       cv=3, verbose=1, random_state=42)\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "best_xgb = xgb_random_search.best_estimator_\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣1️⃣ Evaluate Models\n",
    "# ==============================\n",
    "def evaluate_model(model, X_test, y_test, name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    print(f\"\\n=== {name} Evaluation ===\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "    print(\"PR AUC:\", auc(rec, prec))\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{name} Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return y_pred, y_proba\n",
    "\n",
    "evaluate_model(best_rf, X_test, y_test, \"Random Forest\")\n",
    "evaluate_model(best_lgb, X_test, y_test, \"LightGBM\")\n",
    "evaluate_model(best_xgb, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣2️⃣ Save Models\n",
    "# ==============================\n",
    "dump({'model': best_rf, 'scaler': scaler}, os.path.join(BASE_DIR, 'random_forest_model_final.joblib'))\n",
    "dump({'model': best_lgb, 'scaler': scaler}, os.path.join(BASE_DIR, 'lightgbm_model_final.joblib'))\n",
    "dump({'model': best_xgb, 'scaler': scaler}, os.path.join(BASE_DIR, 'xgboost_model_final.joblib'))\n",
    "print(\"✅ All models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288791a3-a644-4280-ba3b-db0ee8795c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full pipeline: Login Anomaly Analysis (Optimized + Advanced Models + Hyperparameter Tuning)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models with Hyperparameter Tuning\n",
    "# ==============================\n",
    "\n",
    "# ---------- Random Forest ----------\n",
    "rf_param_dist = {\n",
    "    'n_estimators':[50,100,200],\n",
    "    'max_depth':[5,10,20,None],\n",
    "    'min_samples_split':[2,5,10],\n",
    "    'min_samples_leaf':[1,2,4]\n",
    "}\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "rf_rs = RandomizedSearchCV(rf, rf_param_dist, n_iter=10, cv=3, scoring='recall', n_jobs=-1, random_state=42)\n",
    "rf_rs.fit(X_train, y_train)\n",
    "best_rf = rf_rs.best_estimator_\n",
    "print(\"✅ Random Forest tuned!\")\n",
    "\n",
    "# ---------- LightGBM ----------\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "best_lgb = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train,lgb_eval], callbacks=[lgb.log_evaluation(period=0)])\n",
    "print(\"✅ LightGBM trained!\")\n",
    "\n",
    "# ---------- XGBoost ----------\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "best_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=100, evals=[(xgb_test,'test')], verbose_eval=False)\n",
    "print(\"✅ XGBoost trained!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Random Forest\n",
    "# ==============================\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_proba = best_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance RF\n",
    "# ==============================\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features - Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Models\n",
    "# ==============================\n",
    "joblib.dump({'model': best_rf, 'scaler': scaler}, os.path.join(BASE_DIR, 'random_forest_model_final.joblib'))\n",
    "best_lgb.save_model(os.path.join(BASE_DIR, 'lightgbm_model_final.txt'))\n",
    "best_xgb.save_model(os.path.join(BASE_DIR, 'xgboost_model_final.json'))\n",
    "print(\"✅ All models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55740ca-4aba-4f90-9c17-4615715ad3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full pipeline: Login Anomaly Detection\n",
    "# Random Forest + LightGBM + XGBoost\n",
    "# Feature Engineering + SMOTE + Evaluation\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "\n",
    "# Random Forest\n",
    "best_rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "best_lgb = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train, lgb_eval], callbacks=[lgb.log_evaluation(period=0)])\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "best_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=100, evals=[(xgb_test,'test')], verbose_eval=False)\n",
    "print(\"✅ All models trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate All Models\n",
    "# ==============================\n",
    "def evaluate_model(name, y_true, y_pred, y_proba):\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    roc = roc_auc_score(y_true, y_proba)\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    print(f\"ROC AUC: {roc:.4f} | PR AUC: {pr_auc:.4f}\")\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(rec, prec, label=f'{name} PR AUC={pr_auc:.4f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return rec, prec, pr_auc\n",
    "\n",
    "rec_rf, prec_rf, pr_auc_rf = evaluate_model(\"Random Forest\", y_test, best_rf.predict(X_test), best_rf.predict_proba(X_test)[:,1])\n",
    "rec_lgb, prec_lgb, pr_auc_lgb = evaluate_model(\"LightGBM\", y_test, (best_lgb.predict(X_test)>0.5).astype(int), best_lgb.predict(X_test))\n",
    "rec_xgb, prec_xgb, pr_auc_xgb = evaluate_model(\"XGBoost\", y_test, (best_xgb.predict(xgb_test)>0.5).astype(int), best_xgb.predict(xgb_test))\n",
    "\n",
    "# Combined PR Curves\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(rec_rf, prec_rf, label=f'Random Forest PR AUC={pr_auc_rf:.4f}')\n",
    "plt.plot(rec_lgb, prec_lgb, label=f'LightGBM PR AUC={pr_auc_lgb:.4f}')\n",
    "plt.plot(rec_xgb, prec_xgb, label=f'XGBoost PR AUC={pr_auc_xgb:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves - All Models')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Models\n",
    "# ==============================\n",
    "MODEL_PATH_RF = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': best_rf, 'scaler': scaler}, MODEL_PATH_RF)\n",
    "print(f\"✅ Random Forest saved at: {MODEL_PATH_RF}\")\n",
    "\n",
    "MODEL_PATH_LGB = os.path.join(BASE_DIR, 'lightgbm_model_final.txt')\n",
    "best_lgb.save_model(MODEL_PATH_LGB)\n",
    "print(f\"✅ LightGBM saved at: {MODEL_PATH_LGB}\")\n",
    "\n",
    "MODEL_PATH_XGB = os.path.join(BASE_DIR, 'xgboost_model_final.json')\n",
    "best_xgb.save_model(MODEL_PATH_XGB)\n",
    "print(f\"✅ XGBoost saved at: {MODEL_PATH_XGB}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5542614-839e-4901-b89d-8130643bd695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Optimized Login Anomaly Analysis\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(lambda x: x.groupby((x==0).cumsum()).cumsum())\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts per IP\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for val in series:\n",
    "        dq.append(val)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache: return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',np.nan,np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"✅ Features scaled.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance (SMOTE)\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "print(\"✅ SMOTE applied only on training set.\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "# Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# LightGBM\n",
    "lgb_train = lgb.Dataset(X_train_res, label=y_train_res)\n",
    "lgb_eval = lgb.Dataset(X_test_scaled, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_eval], callbacks=[lgb.log_evaluation(period=0)])\n",
    "\n",
    "# XGBoost\n",
    "xgb_train = xgb.DMatrix(X_train_res, label=y_train_res)\n",
    "xgb_test = xgb.DMatrix(X_test_scaled, label=y_test)\n",
    "xgb_model = xgb.train({'objective':'binary:logistic','eval_metric':'auc'}, xgb_train, num_boost_round=100, evals=[(xgb_test,'eval')], verbose_eval=False)\n",
    "\n",
    "print(\"✅ All models trained without data leakage!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Random Forest\n",
    "# ==============================\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "y_proba = rf_clf.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(rf_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Models\n",
    "# ==============================\n",
    "joblib.dump({'model': rf_clf, 'scaler': scaler}, os.path.join(BASE_DIR, 'rf_model_final.joblib'))\n",
    "lgb_model.save_model(os.path.join(BASE_DIR, 'lgb_model_final.txt'))\n",
    "xgb_model.save_model(os.path.join(BASE_DIR, 'xgb_model_final.json'))\n",
    "print(\"✅ All models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d08e25-57b7-4c36-aa10-e49434324016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Optimized Login Anomaly Analysis\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(lambda x: x.groupby((x==0).cumsum()).cumsum())\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts per IP\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for val in series:\n",
    "        dq.append(val)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache: return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',np.nan,np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"✅ Features scaled.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance (SMOTE)\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "print(\"✅ SMOTE applied only on training set.\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "# Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# LightGBM\n",
    "lgb_train = lgb.Dataset(X_train_res, label=y_train_res)\n",
    "lgb_eval = lgb.Dataset(X_test_scaled, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_eval], callbacks=[lgb.log_evaluation(period=0)])\n",
    "\n",
    "# XGBoost\n",
    "xgb_train = xgb.DMatrix(X_train_res, label=y_train_res)\n",
    "xgb_test = xgb.DMatrix(X_test_scaled, label=y_test)\n",
    "xgb_model = xgb.train({'objective':'binary:logistic','eval_metric':'auc'}, xgb_train, num_boost_round=100, evals=[(xgb_test,'eval')], verbose_eval=False)\n",
    "\n",
    "print(\"✅ All models trained without data leakage!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Random Forest\n",
    "# ==============================\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "y_proba = rf_clf.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(rf_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Models\n",
    "# ==============================\n",
    "joblib.dump({'model': rf_clf, 'scaler': scaler}, os.path.join(BASE_DIR, 'rf_model_final.joblib'))\n",
    "lgb_model.save_model(os.path.join(BASE_DIR, 'lgb_model_final.txt'))\n",
    "xgb_model.save_model(os.path.join(BASE_DIR, 'xgb_model_final.json'))\n",
    "print(\"✅ All models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fe3ed-d371-457d-9e48-40e4d56d3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Project Script: Login Anomaly Analysis (Optimized & Safe)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\\n\", y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Avg interarrival per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42,\n",
    "    stratify=y if len(np.unique(y))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance (SMOTE)\n",
    "# ==============================\n",
    "if len(np.unique(y_train)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "    print(\"✅ SMOTE applied to training set.\")\n",
    "else:\n",
    "    X_train_res, y_train_res = X_train_scaled, y_train\n",
    "    print(\"⚠️ Only one class in training set. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "# Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# LightGBM\n",
    "lgb_train = lgb.Dataset(X_train_res, label=y_train_res)\n",
    "lgb_eval = lgb.Dataset(X_test_scaled, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train,lgb_eval], callbacks=[lgb.log_evaluation(period=0)])\n",
    "\n",
    "# XGBoost\n",
    "xgb_train = xgb.DMatrix(X_train_res, label=y_train_res)\n",
    "xgb_test = xgb.DMatrix(X_test_scaled, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100, evals=[(xgb_test,'test')], verbose_eval=False)\n",
    "\n",
    "print(\"✅ All models trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Random Forest\n",
    "# ==============================\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "y_proba = rf_clf.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(rf_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Models & Scaler\n",
    "# ==============================\n",
    "joblib.dump({'model': rf_clf, 'scaler': scaler}, os.path.join(BASE_DIR, 'random_forest_model_final.joblib'))\n",
    "lgb_model.save_model(os.path.join(BASE_DIR, 'lightgbm_model_final.txt'))\n",
    "xgb_model.save_model(os.path.join(BASE_DIR, 'xgboost_model_final.json'))\n",
    "\n",
    "print(\"✅ All models and scaler saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca208ef2-a1af-4457-935b-249aecd45de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"Target distribution:\\n\", y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',np.nan,np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42,\n",
    "    stratify=y if len(np.unique(y))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance (SMOTE)\n",
    "# ==============================\n",
    "if len(np.unique(y_train)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "else:\n",
    "    X_train_res, y_train_res = X_train_scaled, y_train\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Hyperparameter Tuning with RandomizedSearchCV\n",
    "# ==============================\n",
    "# Random Forest\n",
    "rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf_param_dist = {'n_estimators':[50,100,200], 'max_depth':[5,10,20,None], 'min_samples_split':[2,5,10]}\n",
    "rf_search = RandomizedSearchCV(rf_clf, rf_param_dist, n_iter=10, cv=3, scoring='recall', n_jobs=-1, random_state=42)\n",
    "rf_search.fit(X_train_res, y_train_res)\n",
    "rf_best = rf_search.best_estimator_\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "lgb_param_dist = {'n_estimators':[100,200], 'max_depth':[5,10,20,-1], 'learning_rate':[0.01,0.05,0.1]}\n",
    "lgb_search = RandomizedSearchCV(lgb_model, lgb_param_dist, n_iter=10, cv=3, scoring='recall', n_jobs=-1, random_state=42)\n",
    "lgb_search.fit(X_train_res, y_train_res)\n",
    "lgb_best = lgb_search.best_estimator_\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='auc', use_label_encoder=False, n_jobs=-1, random_state=42)\n",
    "xgb_param_dist = {'n_estimators':[100,200], 'max_depth':[3,6,10], 'learning_rate':[0.01,0.05,0.1]}\n",
    "xgb_search = RandomizedSearchCV(xgb_model, xgb_param_dist, n_iter=10, cv=3, scoring='recall', n_jobs=-1, random_state=42)\n",
    "xgb_search.fit(X_train_res, y_train_res)\n",
    "xgb_best = xgb_search.best_estimator_\n",
    "\n",
    "print(\"✅ Hyperparameter tuning completed for all models.\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Random Forest\n",
    "# ==============================\n",
    "y_pred = rf_best.predict(X_test_scaled)\n",
    "y_proba = rf_best.predict_proba(X_test_scaled)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(rf_best.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Models & Scaler\n",
    "# ==============================\n",
    "joblib.dump({'model': rf_best, 'scaler': scaler}, os.path.join(BASE_DIR, 'random_forest_model_tuned.joblib'))\n",
    "joblib.dump({'model': lgb_best, 'scaler': scaler}, os.path.join(BASE_DIR, 'lightgbm_model_tuned.joblib'))\n",
    "joblib.dump({'model': xgb_best, 'scaler': scaler}, os.path.join(BASE_DIR, 'xgboost_model_tuned.joblib'))\n",
    "\n",
    "print(\"✅ All tuned models and scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44418fb5-fae5-4fbc-9d63-e717ae31824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Login Anomaly Pipeline (Optimized + All Steps)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣a Data Quality Check\n",
    "# ==============================\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "print(df.isnull().mean())\n",
    "print(\"\\n--- Result Counts ---\")\n",
    "print(df['result'].value_counts())\n",
    "print(\"\\n--- Feature Statistics ---\")\n",
    "print(feat.describe())\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour and night features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(lambda x: x.groupby((x==0).cumsum()).cumsum())\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for user in series:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features (Parallel + Cache)\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix X\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features scaled successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print(\"✅ SMOTE applied to balance classes.\")\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print(\"⚠️ Only one class present in y. SMOTE skipped.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "print(\"✅ Train/Test split done!\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "# Random Forest\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "clf_lgb = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train,lgb_eval], callbacks=[lgb.log_evaluation(period=0)])\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "clf_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=100, evals=[(xgb_test,'test')], verbose_eval=False)\n",
    "\n",
    "print(\"✅ All models trained successfully!\")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Random Forest\n",
    "# ==============================\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "y_proba = clf_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'PR AUC = {auc(rec, prec):.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Models\n",
    "# ==============================\n",
    "MODEL_PATH_RF = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "joblib.dump({'model': clf_rf, 'scaler': scaler}, MODEL_PATH_RF)\n",
    "print(f\"✅ Random Forest saved at: {MODEL_PATH_RF}\")\n",
    "\n",
    "MODEL_PATH_LGB = os.path.join(BASE_DIR, 'lightgbm_model_final.txt')\n",
    "clf_lgb.save_model(MODEL_PATH_LGB)\n",
    "print(f\"✅ LightGBM saved at: {MODEL_PATH_LGB}\")\n",
    "\n",
    "MODEL_PATH_XGB = os.path.join(BASE_DIR, 'xgboost_model_final.json')\n",
    "clf_xgb.save_model(MODEL_PATH_XGB)\n",
    "print(f\"✅ XGBoost saved at: {MODEL_PATH_XGB}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7beac37-8d95-44da-9fdf-cb7b7c001346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# score_event.py - Simple Alert System\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Features\n",
    "# ==============================\n",
    "df_feat = pd.read_csv(FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Load Model + Scaler\n",
    "# ==============================\n",
    "model_data = joblib.load(MODEL_FILE)\n",
    "clf = model_data['model']\n",
    "scaler = model_data['scaler']\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = df_feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = df_feat[numeric_cols].fillna(0)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Predict Probabilities\n",
    "# ==============================\n",
    "df_feat['failed_prob'] = 1 - clf.predict_proba(X_scaled)[:,1]  # احتمال الفشل\n",
    "df_feat['alert'] = (df_feat['failed_prob'] >= 0.5).astype(int) # تنبيه إذا الاحتمال >= 0.5\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Save Alerts\n",
    "# ==============================\n",
    "df_alert = df_feat[['timestamp','ip','user','failed_prob','alert']]\n",
    "if os.path.exists(ALERT_FILE):\n",
    "    df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df_alert.to_csv(ALERT_FILE, index=False)\n",
    "\n",
    "print(f\"✅ Alerts saved at {ALERT_FILE} ({len(df_alert)} events)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff5523-6a45-440b-a8ca-107b0f4162a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_PATH_RF = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "\n",
    "# ==============================\n",
    "# Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for u in series:\n",
    "        dq.append(u)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache: return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',np.nan,np.nan\n",
    "    geo_cache[ip] = pd.Series([country,city,lat,lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# Handle Imbalanced Classes\n",
    "# ==============================\n",
    "if len(np.unique(y)) > 1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "\n",
    "# ==============================\n",
    "# Split Train/Test\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42,\n",
    "    stratify=y_res if len(np.unique(y_res))>1 else None\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Train Random Forest\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# Save Model + Scaler + Columns\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler, 'columns': numeric_cols}, MODEL_PATH_RF)\n",
    "print(f\"✅ Random Forest saved at: {MODEL_PATH_RF}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9b197-1601-453e-ae26-32f574509361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "df_feat = pd.read_csv(FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "model_data = joblib.load(MODEL_FILE)\n",
    "clf = model_data['model']\n",
    "scaler = model_data['scaler']\n",
    "train_columns = model_data['columns']\n",
    "\n",
    "# تهيئة الأعمدة لتطابق التدريب\n",
    "X = pd.DataFrame(0, index=df_feat.index, columns=train_columns)\n",
    "for col in train_columns:\n",
    "    if col in df_feat.columns:\n",
    "        X[col] = df_feat[col]\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# تنبؤ الاحتمالات\n",
    "df_feat['failed_prob'] = 1 - clf.predict_proba(X_scaled)[:,1]\n",
    "df_feat['alert'] = (df_feat['failed_prob'] >= 0.5).astype(int)\n",
    "\n",
    "# حفظ التنبيهات\n",
    "df_alert = df_feat[['timestamp','ip','user','failed_prob','alert']]\n",
    "if os.path.exists(ALERT_FILE):\n",
    "    df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df_alert.to_csv(ALERT_FILE, index=False)\n",
    "\n",
    "print(f\"✅ Alerts saved at {ALERT_FILE} ({len(df_alert)} events)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c605a2-a1d5-471b-ae55-75df90d31abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# score_event_safe.py\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Features and Model\n",
    "# ==============================\n",
    "df_feat = pd.read_csv(FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "model_data = joblib.load(MODEL_FILE)\n",
    "clf = model_data['model']\n",
    "scaler = model_data['scaler']\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Ensure 'user' column exists\n",
    "# ==============================\n",
    "if 'user' not in df_feat.columns:\n",
    "    df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "    df_feat = df_feat.merge(df_parsed[['timestamp','ip','user']], on=['timestamp','ip'], how='left')\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = df_feat[numeric_cols].fillna(0)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Predict Probabilities\n",
    "# ==============================\n",
    "df_feat['failed_prob'] = 1 - clf.predict_proba(X_scaled)[:,1]  # احتمال الفشل\n",
    "df_feat['alert'] = (df_feat['failed_prob'] >= 0.5).astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Save Alerts\n",
    "# ==============================\n",
    "cols_to_save = ['timestamp','ip','user','failed_prob','alert']\n",
    "df_alert = df_feat[cols_to_save]\n",
    "\n",
    "if os.path.exists(ALERT_FILE):\n",
    "    df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df_alert.to_csv(ALERT_FILE, index=False)\n",
    "\n",
    "print(f\"✅ Alerts saved to {ALERT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd06dd-9196-4213-b45c-0ecebcfe77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# score_event.py - Robust Prediction Script\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, 'auth_features_new.csv')  # ملف الميزات الجديد\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Model and Scaler\n",
    "# ==============================\n",
    "saved = joblib.load(MODEL_FILE)\n",
    "clf = saved['model']\n",
    "scaler = saved['scaler']\n",
    "\n",
    "# Load feature column order\n",
    "FEATURE_COLS = joblib.load(FEATURE_COLS_FILE)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Load Features\n",
    "# ==============================\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# Ensure all expected features exist\n",
    "for col in FEATURE_COLS:\n",
    "    if col not in df_feat.columns:\n",
    "        df_feat[col] = 0\n",
    "\n",
    "# Keep only the trained feature columns, in the correct order\n",
    "X = df_feat[FEATURE_COLS].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Scale Features\n",
    "# ==============================\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Predict Probabilities\n",
    "# ==============================\n",
    "# احتمال الفشل = 1 - احتمال النجاح\n",
    "df_feat['failed_prob'] = 1 - clf.predict_proba(X_scaled)[:,1]\n",
    "df_feat['alert'] = (df_feat['failed_prob'] >= 0.5).astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Save Alerts\n",
    "# ==============================\n",
    "columns_to_save = ['timestamp','ip','user','failed_prob','alert']\n",
    "\n",
    "# إذا أي عمود مفقود في الملف الجديد، نضيفه بصفر\n",
    "for col in columns_to_save:\n",
    "    if col not in df_feat.columns:\n",
    "        df_feat[col] = np.nan\n",
    "\n",
    "df_alert = df_feat[columns_to_save]\n",
    "\n",
    "# Append to file إذا موجود، أو أنشئ جديد\n",
    "if os.path.exists(ALERT_FILE):\n",
    "    df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df_alert.to_csv(ALERT_FILE, index=False)\n",
    "\n",
    "print(f\"✅ Alerts saved to {ALERT_FILE}\")\n",
    "print(\"✅ Prediction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4642831-c9f8-4a89-b52a-05afe242bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Full Prediction Script: score_event.py\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import deque\n",
    "import geoip2.database\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Model & Scaler\n",
    "# ==============================\n",
    "saved = joblib.load(MODEL_FILE)\n",
    "clf = saved['model']\n",
    "scaler = saved['scaler']\n",
    "\n",
    "# Load feature column order\n",
    "FEATURE_COLS = joblib.load(FEATURE_COLS_FILE)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Load New Feature Data\n",
    "# ==============================\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Ensure All Feature Columns Are Present\n",
    "# ==============================\n",
    "for col in FEATURE_COLS:\n",
    "    if col not in df_feat.columns:\n",
    "        df_feat[col] = 0  # Fill missing columns with 0\n",
    "\n",
    "# Reorder columns to match training\n",
    "X = df_feat[FEATURE_COLS].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Scale Features\n",
    "# ==============================\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Predict Probabilities\n",
    "# ==============================\n",
    "df_feat['failed_prob'] = 1 - clf.predict_proba(X_scaled)[:,1]  # احتمال الفشل\n",
    "df_feat['alert'] = (df_feat['failed_prob'] >= 0.5).astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Save Alerts\n",
    "# ==============================\n",
    "df_alert = df_feat[['timestamp','ip','user','failed_prob','alert']]\n",
    "if os.path.exists(ALERT_FILE):\n",
    "    df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df_alert.to_csv(ALERT_FILE, index=False)\n",
    "print(f\"✅ Alerts saved to {ALERT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450950ac-b414-4bf1-b5b5-dda05285bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed, dump\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# ==============================\n",
    "# Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "MODEL_RF_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "MODEL_LGB_FILE = os.path.join(BASE_DIR, 'lightgbm_model_final.txt')\n",
    "MODEL_XGB_FILE = os.path.join(BASE_DIR, 'xgboost_model_final.json')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Target\n",
    "# ==============================\n",
    "if 'result_bin' not in df.columns:\n",
    "    df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(lambda x: x.groupby((x==0).cumsum()).cumsum())\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    dq = deque(maxlen=5)\n",
    "    counts = []\n",
    "    for u in series:\n",
    "        dq.append(u)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',np.nan,np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "if len(np.unique(y))>1:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res if len(np.unique(y_res))>1 else None)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Models\n",
    "# ==============================\n",
    "# Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "lgb_params = {'objective':'binary','metric':'auc','verbosity':-1}\n",
    "best_lgb = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_train,lgb_eval], callbacks=[lgb.log_evaluation(period=0)])\n",
    "\n",
    "# XGBoost\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "xgb_params = {'objective':'binary:logistic','eval_metric':'auc'}\n",
    "best_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=100, evals=[(xgb_test,'test')], verbose_eval=False)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Random Forest\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Save Models & Feature Order\n",
    "# ==============================\n",
    "dump({'model':clf,'scaler':scaler}, MODEL_RF_FILE)\n",
    "best_lgb.save_model(MODEL_LGB_FILE)\n",
    "best_xgb.save_model(MODEL_XGB_FILE)\n",
    "dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ All models and feature columns saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52426c24-ace8-4431-b678-2b83b10c06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "MODEL_RF_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "# Load model + scaler + feature order\n",
    "saved = load(MODEL_RF_FILE)\n",
    "clf = saved['model']\n",
    "scaler = saved['scaler']\n",
    "FEATURE_COLS = load(FEATURE_COLS_FILE)\n",
    "\n",
    "# Load new features\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# Keep only trained features\n",
    "df_feat = df_feat[FEATURE_COLS].fillna(0)\n",
    "X_scaled = scaler.transform(df_feat)\n",
    "\n",
    "# Predict failed probability\n",
    "df_feat['failed_prob'] = 1 - clf.predict_proba(X_scaled)[:,1]  # احتمال الفشل\n",
    "df_feat['alert'] = (df_feat['failed_prob']>=0.5).astype(int)\n",
    "\n",
    "# حفظ التنبيهات\n",
    "df_alert = df_feat.copy()\n",
    "df_alert['timestamp'] = pd.read_csv(FEATURE_FILE)['timestamp']  # أضف timestamp الأصلي\n",
    "df_alert.to_csv(ALERT_FILE, index=False)\n",
    "print(f\"✅ Alerts saved at {ALERT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63d023-10f0-49da-843d-f3c69e847046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f8788-692e-4daf-9196-7c9b362cfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# score_event_full.py\n",
    "# Full prediction script for login anomaly\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from joblib import load\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import geoip2.database\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, 'auth_features_new.csv')  # بيانات جديدة\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load model & scaler\n",
    "# ==============================\n",
    "saved = load(MODEL_FILE)\n",
    "clf = saved['model']\n",
    "scaler = saved['scaler']\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Load new feature data\n",
    "# ==============================\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering (same as training)\n",
    "# ==============================\n",
    "# Hour & night\n",
    "df_feat['hour'] = df_feat['timestamp'].dt.hour\n",
    "df_feat['is_night'] = df_feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Failed streak\n",
    "df_feat['failed_flag'] = (df_feat['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df_feat['failed_streak'] = df_feat.groupby('user')['failed_flag'].transform(compute_failed_streak).fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df_feat['unique_users_last_5'] = df_feat.groupby('ip')['user'].transform(unique_users_last_5).fillna(0).astype(int)\n",
    "\n",
    "# GeoIP features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df_feat['ip'].unique()\n",
    "geo_results = [geoip_lookup(ip) for ip in unique_ips]\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df_feat = df_feat.join(geo_df, on='ip')\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare feature matrix\n",
    "# ==============================\n",
    "feature_cols = [\n",
    "    'hour','is_night','avg_interarrival','failed_streak',\n",
    "    'unique_users_last_5','lat','lon' # 'geo_country' handled separately if encoded\n",
    "]\n",
    "\n",
    "# إذا كان هناك تصنيف للبلد، يمكن استخدام one-hot أو LabelEncoder كما في التدريب\n",
    "# هنا نستخدم فقط الأعمدة الرقمية\n",
    "X = df_feat[feature_cols].fillna(0)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Predict probabilities\n",
    "# ==============================\n",
    "df_feat['failed_prob'] = 1 - clf.predict_proba(X_scaled)[:,1]  # احتمال الفشل\n",
    "df_feat['alert'] = (df_feat['failed_prob'] >= 0.5).astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Save alerts\n",
    "# ==============================\n",
    "cols_alert = ['timestamp','ip','user','failed_prob','alert']\n",
    "df_alert = df_feat[cols_alert]\n",
    "\n",
    "if os.path.exists(ALERT_FILE):\n",
    "    df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df_alert.to_csv(ALERT_FILE, index=False)\n",
    "\n",
    "print(f\"✅ Alerts saved to {ALERT_FILE}\")\n",
    "print(df_alert.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5d99a-7937-4a15-9f94-14c3b3ae9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Generate Features for New Data\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ------------------------------\n",
    "# Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "# ------------------------------\n",
    "# Load parsed log\n",
    "# ------------------------------\n",
    "df = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# Initialize feature dataframe\n",
    "# ------------------------------\n",
    "feat = pd.DataFrame()\n",
    "\n",
    "# Hour and night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival time per IP\n",
    "df_sorted = df.sort_values(['ip', 'timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak per user\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA', 'NA', np.nan, np.nan\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['country']\n",
    "\n",
    "# ------------------------------\n",
    "# Save new feature file\n",
    "# ------------------------------\n",
    "feat.to_csv(FEATURE_FILE_NEW, index=False)\n",
    "print(f\"✅ New feature file saved: {FEATURE_FILE_NEW}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340a3bc-e05f-41de-b13f-ae44d710f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Predict Login Failures & Generate Alerts\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# ------------------------------\n",
    "# Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "MODEL_PATH_RF = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "# ------------------------------\n",
    "# Load model and scaler\n",
    "# ------------------------------\n",
    "saved = joblib.load(MODEL_PATH_RF)\n",
    "clf = saved['model']\n",
    "scaler = saved['scaler']\n",
    "\n",
    "# ------------------------------\n",
    "# Load new feature data\n",
    "# ------------------------------\n",
    "df_feat = pd.read_csv(FEATURE_FILE_NEW, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# Keep only numeric features used in training\n",
    "# ------------------------------\n",
    "FEATURE_COLS = clf.feature_names_in_  # الأعمدة التي تدرب عليها النموذج\n",
    "df_feat = df_feat[FEATURE_COLS].fillna(0)\n",
    "\n",
    "# ------------------------------\n",
    "# Scale features\n",
    "# ------------------------------\n",
    "X_scaled = scaler.transform(df_feat)\n",
    "\n",
    "# ------------------------------\n",
    "# Predict failed probability\n",
    "# ------------------------------\n",
    "df_feat['failed_prob'] = 1 - clf.predict_proba(X_scaled)[:,1]  # احتمال الفشل\n",
    "\n",
    "# ------------------------------\n",
    "# Generate alerts\n",
    "# ------------------------------\n",
    "df_feat['alert'] = (df_feat['failed_prob'] >= 0.5).astype(int)\n",
    "\n",
    "# إذا كنت تريد حفظ الأعمدة الأساسية مع التنبيه\n",
    "df_alert = pd.DataFrame({\n",
    "    'timestamp': df_feat.index,  # أو استخدم df['timestamp'] من الملف الأصلي\n",
    "    'failed_prob': df_feat['failed_prob'],\n",
    "    'alert': df_feat['alert']\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# Save alerts\n",
    "# ------------------------------\n",
    "df_alert.to_csv(ALERT_FILE, index=False)\n",
    "print(f\"✅ Alerts saved to: {ALERT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20e6a6-e0fb-43a7-a20c-feb4b48cc6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Alert Script (Optimized)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Setup Paths\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Model\n",
    "# ==============================\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    raise FileNotFoundError(f\"Model file not found: {MODEL_FILE}\")\n",
    "\n",
    "saved = joblib.load(MODEL_FILE)\n",
    "clf = saved['model']\n",
    "scaler = saved['scaler']\n",
    "FEATURE_COLS = clf.feature_names_in_  # الأعمدة التي تم تدريب النموذج عليها\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Load New Feature Data\n",
    "# ==============================\n",
    "if not os.path.exists(FEATURE_FILE_NEW):\n",
    "    raise FileNotFoundError(f\"Feature file not found: {FEATURE_FILE_NEW}\")\n",
    "\n",
    "df_feat = pd.read_csv(FEATURE_FILE_NEW, low_memory=False)\n",
    "\n",
    "# إضافة timestamp افتراضي إذا لم يكن موجودًا\n",
    "if 'timestamp' not in df_feat.columns:\n",
    "    df_feat['timestamp'] = pd.Timestamp.now()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Keep Only Trained Features\n",
    "# ==============================\n",
    "missing_cols = [c for c in FEATURE_COLS if c not in df_feat.columns]\n",
    "for c in missing_cols:\n",
    "    df_feat[c] = 0  # ملء الأعمدة المفقودة بالقيمة 0\n",
    "\n",
    "X = df_feat[FEATURE_COLS].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Scale Features\n",
    "# ==============================\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Predict Probabilities\n",
    "# ==============================\n",
    "df_feat['failed_prob'] = 1 - clf.predict_proba(X_scaled)[:,1]  # احتمال الفشل\n",
    "df_feat['alert'] = (df_feat['failed_prob'] >= 0.5).astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Save Alerts\n",
    "# ==============================\n",
    "alert_cols = ['timestamp', 'ip', 'user', 'failed_prob', 'alert']\n",
    "# إضافة الأعمدة الناقصة إذا لم تكن موجودة\n",
    "for col in ['ip', 'user']:\n",
    "    if col not in df_feat.columns:\n",
    "        df_feat[col] = 'NA'\n",
    "\n",
    "df_alert = df_feat[alert_cols]\n",
    "\n",
    "# إذا الملف موجود مسبقًا، أضف الأسطر الجديدة\n",
    "if os.path.exists(ALERT_FILE):\n",
    "    df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df_alert.to_csv(ALERT_FILE, index=False)\n",
    "\n",
    "print(f\"✅ Alerts saved to: {ALERT_FILE}\")\n",
    "print(df_alert.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941515a8-9e40-46dd-bc5d-0e316bf21940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Pipeline (Full, Robust)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "if os.path.exists(FEATURE_FILE_NEW):\n",
    "    df_new = pd.read_csv(FEATURE_FILE_NEW, low_memory=False)\n",
    "    \n",
    "    # Ensure all trained features exist\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= 0.5).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    print(\"✅ Alerts generated for new data.\")\n",
    "else:\n",
    "    print(\"⚠️ No new feature file found. Skipping alert generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954fed2-1e90-414c-8c33-7f15dda43a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "# إعداد المسارات\n",
    "# ===============================\n",
    "PARSED_FILE = \"data/auth_parsed_large.csv\"\n",
    "FEATURE_FILE = \"data/auth_features_large.csv\"\n",
    "\n",
    "PARSED_CLEAN = \"data/auth_parsed_clean.csv\"\n",
    "FEATURE_CLEAN = \"data/auth_features_clean.csv\"\n",
    "\n",
    "# ===============================\n",
    "# دالة فحص جودة البيانات\n",
    "# ===============================\n",
    "def check_quality(df, name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🔍 Checking Data Quality for {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"📊 Shape: {df.shape}\")\n",
    "    print(\"\\nMissing values (%):\")\n",
    "    print(df.isnull().mean() * 100)\n",
    "    print(\"\\nBasic stats:\")\n",
    "    print(df.describe(include='all').T)\n",
    "    print(\"\\nUnique values per column:\")\n",
    "    print(df.nunique())\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# ===============================\n",
    "if not os.path.exists(PARSED_FILE) or not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(\"❌ تأكد من وجود ملفات الإدخال في مجلد data\")\n",
    "\n",
    "print(\"📥 Loading large CSV files ...\")\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ فحص الجودة قبل التنظيف\n",
    "# ===============================\n",
    "check_quality(df_parsed, \"auth_parsed_large.csv\")\n",
    "check_quality(df_feat, \"auth_features_large.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ تنظيف البيانات\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n🧹 Cleaning auth_parsed_large.csv ...\")\n",
    "\n",
    "# إزالة الصفوف التي لا تحتوي على timestamp أو IP\n",
    "df_parsed = df_parsed.dropna(subset=['timestamp', 'src_ip'])\n",
    "\n",
    "# ملء القيم الناقصة في الأعمدة النصية بـ 'unknown'\n",
    "text_cols = df_parsed.select_dtypes(include='object').columns\n",
    "df_parsed[text_cols] = df_parsed[text_cols].fillna('unknown')\n",
    "\n",
    "# تأكد من أن القيم المنطقية سليمة\n",
    "if 'result' in df_parsed.columns:\n",
    "    df_parsed = df_parsed[df_parsed['result'].isin(['success', 'failed'])]\n",
    "\n",
    "# حفظ الملف النظيف\n",
    "df_parsed.to_csv(PARSED_CLEAN, index=False)\n",
    "print(f\"✅ Saved cleaned parsed file → {PARSED_CLEAN}\")\n",
    "print(f\"🧮 Remaining rows: {len(df_parsed)}\")\n",
    "\n",
    "# -------------------------------\n",
    "print(\"\\n🧹 Cleaning auth_features_large.csv ...\")\n",
    "\n",
    "# إزالة الصفوف ذات timestamp مفقود\n",
    "df_feat = df_feat.dropna(subset=['timestamp'])\n",
    "\n",
    "# ملء القيم الناقصة الرقمية بـ 0\n",
    "num_cols = df_feat.select_dtypes(include=[np.number]).columns\n",
    "df_feat[num_cols] = df_feat[num_cols].fillna(0)\n",
    "\n",
    "# ملء النصوص بـ 'unknown'\n",
    "text_cols_feat = df_feat.select_dtypes(include='object').columns\n",
    "df_feat[text_cols_feat] = df_feat[text_cols_feat].fillna('unknown')\n",
    "\n",
    "# إزالة القيم المتطرفة الكبيرة جدًا (3× الانحراف المعياري)\n",
    "for col in num_cols:\n",
    "    mean = df_feat[col].mean()\n",
    "    std = df_feat[col].std()\n",
    "    df_feat = df_feat[(df_feat[col] >= mean - 3*std) & (df_feat[col] <= mean + 3*std)]\n",
    "\n",
    "# حفظ الملف النظيف\n",
    "df_feat.to_csv(FEATURE_CLEAN, index=False)\n",
    "print(f\"✅ Saved cleaned features file → {FEATURE_CLEAN}\")\n",
    "print(f\"🧮 Remaining rows: {len(df_feat)}\")\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ عرض ملخص بعد التنظيف\n",
    "# ===============================\n",
    "print(\"\\n📈 Summary after cleaning:\")\n",
    "check_quality(df_parsed, \"auth_parsed_clean.csv\")\n",
    "check_quality(df_feat, \"auth_features_clean.csv\")\n",
    "\n",
    "print(\"\\n🎯 Data quality check and cleaning completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886366b-1f79-4700-98a0-ee3e98ab2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "# إعداد المسارات\n",
    "# ===============================\n",
    "PARSED_FILE = \"data/auth_parsed_large.csv\"\n",
    "FEATURE_FILE = \"data/auth_features_large.csv\"\n",
    "\n",
    "PARSED_CLEAN = \"data/auth_parsed_clean.csv\"\n",
    "FEATURE_CLEAN = \"data/auth_features_clean.csv\"\n",
    "\n",
    "# ===============================\n",
    "# دالة فحص جودة البيانات\n",
    "# ===============================\n",
    "def check_quality(df, name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🔍 Checking Data Quality for {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"📊 Shape: {df.shape}\")\n",
    "    print(\"\\nMissing values (%):\")\n",
    "    print(df.isnull().mean() * 100)\n",
    "    print(\"\\nBasic stats:\")\n",
    "    print(df.describe(include='all').T)\n",
    "    print(\"\\nUnique values per column:\")\n",
    "    print(df.nunique())\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# ===============================\n",
    "if not os.path.exists(PARSED_FILE) or not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(\"❌ تأكد من وجود ملفات الإدخال في مجلد data\")\n",
    "\n",
    "print(\"📥 Loading large CSV files ...\")\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ فحص الجودة قبل التنظيف\n",
    "# ===============================\n",
    "check_quality(df_parsed, \"auth_parsed_large.csv\")\n",
    "check_quality(df_feat, \"auth_features_large.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ تنظيف البيانات\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n🧹 Cleaning auth_parsed_large.csv ...\")\n",
    "\n",
    "# إزالة الصفوف التي لا تحتوي على timestamp أو IP\n",
    "df_parsed = df_parsed.dropna(subset=['timestamp', 'src_ip'])\n",
    "\n",
    "# ملء القيم الناقصة في الأعمدة النصية بـ 'unknown'\n",
    "text_cols = df_parsed.select_dtypes(include='object').columns\n",
    "df_parsed[text_cols] = df_parsed[text_cols].fillna('unknown')\n",
    "\n",
    "# تأكد من أن القيم المنطقية سليمة\n",
    "if 'result' in df_parsed.columns:\n",
    "    df_parsed = df_parsed[df_parsed['result'].isin(['success', 'failed'])]\n",
    "\n",
    "# حفظ الملف النظيف\n",
    "df_parsed.to_csv(PARSED_CLEAN, index=False)\n",
    "print(f\"✅ Saved cleaned parsed file → {PARSED_CLEAN}\")\n",
    "print(f\"🧮 Remaining rows: {len(df_parsed)}\")\n",
    "\n",
    "# -------------------------------\n",
    "print(\"\\n🧹 Cleaning auth_features_large.csv ...\")\n",
    "\n",
    "# إزالة الصفوف ذات timestamp مفقود\n",
    "df_feat = df_feat.dropna(subset=['timestamp'])\n",
    "\n",
    "# ملء القيم الناقصة الرقمية بـ 0\n",
    "num_cols = df_feat.select_dtypes(include=[np.number]).columns\n",
    "df_feat[num_cols] = df_feat[num_cols].fillna(0)\n",
    "\n",
    "# ملء النصوص بـ 'unknown'\n",
    "text_cols_feat = df_feat.select_dtypes(include='object').columns\n",
    "df_feat[text_cols_feat] = df_feat[text_cols_feat].fillna('unknown')\n",
    "\n",
    "# إزالة القيم المتطرفة الكبيرة جدًا (3× الانحراف المعياري)\n",
    "for col in num_cols:\n",
    "    mean = df_feat[col].mean()\n",
    "    std = df_feat[col].std()\n",
    "    df_feat = df_feat[(df_feat[col] >= mean - 3*std) & (df_feat[col] <= mean + 3*std)]\n",
    "\n",
    "# حفظ الملف النظيف\n",
    "df_feat.to_csv(FEATURE_CLEAN, index=False)\n",
    "print(f\"✅ Saved cleaned features file → {FEATURE_CLEAN}\")\n",
    "print(f\"🧮 Remaining rows: {len(df_feat)}\")\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ عرض ملخص بعد التنظيف\n",
    "# ===============================\n",
    "print(\"\\n📈 Summary after cleaning:\")\n",
    "check_quality(df_parsed, \"auth_parsed_clean.csv\")\n",
    "check_quality(df_feat, \"auth_features_clean.csv\")\n",
    "\n",
    "print(\"\\n🎯 Data quality check and cleaning completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f992be-8e9e-498f-ad6e-a9bdfcead1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ تحديد المسارات\n",
    "# ===============================\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"..\", \"data\")\n",
    "REPORT_DIR = os.path.join(BASE_DIR, \"..\", \"reports\")\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "PARSED_FILE = os.path.join(DATA_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(DATA_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ تحقق من وجود الملفات\n",
    "# ===============================\n",
    "if not os.path.exists(PARSED_FILE) or not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(f\"❌ الملفات غير موجودة:\\n{PARSED_FILE}\\n{FEATURE_FILE}\")\n",
    "\n",
    "print(\"📥 تحميل الملفات الكبيرة...\")\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ فحص جودة البيانات\n",
    "# ===============================\n",
    "print(\"\\n📊 جودة ملف auth_parsed_large.csv:\")\n",
    "missing_parsed = df_parsed.isnull().mean().sort_values(ascending=False)\n",
    "if 'result' in df_parsed.columns:\n",
    "    result_counts = df_parsed['result'].value_counts()\n",
    "else:\n",
    "    result_counts = pd.Series(dtype=int)\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ فحص الميزات\n",
    "# ===============================\n",
    "print(\"\\n📊 جودة ملف auth_features_large.csv:\")\n",
    "missing_feat = df_feat.isnull().mean().sort_values(ascending=False)\n",
    "\n",
    "# ===============================\n",
    "# 5️⃣ إنشاء تقارير\n",
    "# ===============================\n",
    "parsed_report_path = os.path.join(REPORT_DIR, \"parsed_quality_report.html\")\n",
    "feature_report_path = os.path.join(REPORT_DIR, \"feature_quality_report.html\")\n",
    "\n",
    "summary_csv_path = os.path.join(REPORT_DIR, \"data_quality_summary.csv\")\n",
    "\n",
    "# HTML Reports\n",
    "with open(parsed_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_parsed_large.csv</h1>\")\n",
    "    f.write(\"<h3>نسبة القيم المفقودة</h3>\")\n",
    "    f.write(missing_parsed.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    if not result_counts.empty:\n",
    "        f.write(\"<h3>توزيع النتائج</h3>\")\n",
    "        f.write(result_counts.to_frame(\"count\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_parsed.describe().to_html(border=1))\n",
    "\n",
    "with open(feature_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_features_large.csv</h1>\")\n",
    "    f.write(\"<h3>نسبة القيم المفقودة</h3>\")\n",
    "    f.write(missing_feat.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_feat.describe().to_html(border=1))\n",
    "\n",
    "# CSV Summary\n",
    "summary = pd.DataFrame({\n",
    "    \"dataset\": [\"auth_parsed_large.csv\", \"auth_features_large.csv\"],\n",
    "    \"rows\": [len(df_parsed), len(df_feat)],\n",
    "    \"columns\": [len(df_parsed.columns), len(df_feat.columns)],\n",
    "    \"missing_avg\": [missing_parsed.mean(), missing_feat.mean()]\n",
    "})\n",
    "summary.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "# ===============================\n",
    "# 6️⃣ طباعة ملخص نهائي\n",
    "# ===============================\n",
    "print(\"\\n✅ تم إنشاء تقارير جودة البيانات بنجاح!\")\n",
    "print(f\"📄 HTML Reports:\")\n",
    "print(f\"   - {parsed_report_path}\")\n",
    "print(f\"   - {feature_report_path}\")\n",
    "print(f\"📊 CSV Summary:\")\n",
    "print(f\"   - {summary_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c9fdd-c87f-458b-8284-fb95f39b13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ تحديد المسارات (متوافقة مع Jupyter)\n",
    "# ===============================\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "REPORT_DIR = os.path.join(BASE_DIR, \"reports\")\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "PARSED_FILE = os.path.join(DATA_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(DATA_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ===============================\n",
    "if not os.path.exists(PARSED_FILE) or not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(\"❌ تأكد من وجود ملفات الإدخال في مجلد data\")\n",
    "\n",
    "print(\"📥 تحميل الملفات الكبيرة...\")\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ فحص جودة البيانات\n",
    "# ===============================\n",
    "print(\"\\n📊 جودة ملف auth_parsed_large.csv:\")\n",
    "missing_parsed = df_parsed.isnull().mean().sort_values(ascending=False)\n",
    "result_counts = df_parsed['result'].value_counts() if 'result' in df_parsed.columns else pd.Series(dtype=int)\n",
    "\n",
    "print(\"\\n📊 جودة ملف auth_features_large.csv:\")\n",
    "missing_feat = df_feat.isnull().mean().sort_values(ascending=False)\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ عرض ملخص مباشر في Jupyter\n",
    "# ===============================\n",
    "print(\"\\n✅ عدد الصفوف والأعمدة:\")\n",
    "display(pd.DataFrame({\n",
    "    \"Dataset\": [\"auth_parsed_large\", \"auth_features_large\"],\n",
    "    \"Rows\": [len(df_parsed), len(df_feat)],\n",
    "    \"Columns\": [len(df_parsed.columns), len(df_feat.columns)],\n",
    "    \"Missing Ratio\": [missing_parsed.mean(), missing_feat.mean()]\n",
    "}))\n",
    "\n",
    "print(\"\\n📈 توزيع نتائج login:\")\n",
    "display(result_counts.to_frame(\"count\"))\n",
    "\n",
    "print(\"\\n📉 أعلى الأعمدة فيها قيم مفقودة:\")\n",
    "display(missing_feat.head(10).to_frame(\"missing_ratio\"))\n",
    "\n",
    "# ===============================\n",
    "# 5️⃣ إنشاء ملفات تقارير HTML و CSV\n",
    "# ===============================\n",
    "parsed_report_path = os.path.join(REPORT_DIR, \"parsed_quality_report.html\")\n",
    "feature_report_path = os.path.join(REPORT_DIR, \"feature_quality_report.html\")\n",
    "summary_csv_path = os.path.join(REPORT_DIR, \"data_quality_summary.csv\")\n",
    "\n",
    "with open(parsed_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_parsed_large.csv</h1>\")\n",
    "    f.write(missing_parsed.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    if not result_counts.empty:\n",
    "        f.write(\"<h3>توزيع النتائج</h3>\")\n",
    "        f.write(result_counts.to_frame(\"count\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_parsed.describe().to_html(border=1))\n",
    "\n",
    "with open(feature_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_features_large.csv</h1>\")\n",
    "    f.write(missing_feat.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_feat.describe().to_html(border=1))\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"dataset\": [\"auth_parsed_large.csv\", \"auth_features_large.csv\"],\n",
    "    \"rows\": [len(df_parsed), len(df_feat)],\n",
    "    \"columns\": [len(df_parsed.columns), len(df_feat.columns)],\n",
    "    \"missing_avg\": [missing_parsed.mean(), missing_feat.mean()]\n",
    "})\n",
    "summary.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ تم إنشاء التقارير في: {REPORT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d8ebc-3014-48c1-b8e7-0257a547f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 📊 Data Quality Analysis - Advanced Jupyter Version\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ إعداد المسارات\n",
    "# ===============================\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "REPORT_DIR = os.path.join(BASE_DIR, \"reports\")\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "PARSED_FILE = os.path.join(DATA_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(DATA_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ===============================\n",
    "if not os.path.exists(PARSED_FILE) or not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(\"❌ تأكد من وجود ملفات الإدخال في مجلد data\")\n",
    "\n",
    "print(\"📥 تحميل الملفات الكبيرة...\")\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ فحص جودة البيانات الأساسية\n",
    "# ===============================\n",
    "missing_parsed = df_parsed.isnull().mean().sort_values(ascending=False)\n",
    "missing_feat = df_feat.isnull().mean().sort_values(ascending=False)\n",
    "result_counts = df_parsed['result'].value_counts() if 'result' in df_parsed.columns else pd.Series(dtype=int)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Dataset\": [\"auth_parsed_large\", \"auth_features_large\"],\n",
    "    \"Rows\": [len(df_parsed), len(df_feat)],\n",
    "    \"Columns\": [len(df_parsed.columns), len(df_feat.columns)],\n",
    "    \"Missing Ratio\": [missing_parsed.mean(), missing_feat.mean()]\n",
    "})\n",
    "\n",
    "display(summary)\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ عرض الأعمدة ذات القيم المفقودة الأعلى\n",
    "# ===============================\n",
    "print(\"\\n📉 Top 15 Missing Columns (auth_features_large.csv):\")\n",
    "display(missing_feat.head(15).to_frame(\"missing_ratio\"))\n",
    "\n",
    "# ===============================\n",
    "# 5️⃣ الرسوم التفاعلية - Plotly\n",
    "# ===============================\n",
    "\n",
    "# 🔹 توزيع نتائج الدخول (Success / Failed)\n",
    "if not result_counts.empty:\n",
    "    fig1 = px.pie(\n",
    "        result_counts,\n",
    "        names=result_counts.index,\n",
    "        values=result_counts.values,\n",
    "        title=\"🔐 توزيع نتائج تسجيل الدخول (نجاح / فشل)\",\n",
    "        color_discrete_sequence=px.colors.qualitative.Set2\n",
    "    )\n",
    "    fig1.show()\n",
    "\n",
    "# 🔹 نسب القيم المفقودة في auth_features_large.csv\n",
    "fig2 = px.bar(\n",
    "    missing_feat.head(20),\n",
    "    x=missing_feat.head(20).index,\n",
    "    y=missing_feat.head(20).values,\n",
    "    title=\"📉 أعلى الأعمدة من حيث نسبة القيم المفقودة\",\n",
    "    labels={\"x\": \"العمود\", \"y\": \"نسبة القيم المفقودة\"},\n",
    "    color=missing_feat.head(20).values,\n",
    "    color_continuous_scale=\"Reds\"\n",
    ")\n",
    "fig2.update_layout(xaxis_tickangle=-45)\n",
    "fig2.show()\n",
    "\n",
    "# 🔹 تحليل زمني لمحاولات الدخول (إن وجد timestamp)\n",
    "if \"timestamp\" in df_parsed.columns:\n",
    "    df_parsed_time = df_parsed.copy()\n",
    "    df_parsed_time[\"date\"] = df_parsed_time[\"timestamp\"].dt.date\n",
    "    login_trend = df_parsed_time.groupby(\"date\").size().reset_index(name=\"count\")\n",
    "    fig3 = px.line(\n",
    "        login_trend,\n",
    "        x=\"date\",\n",
    "        y=\"count\",\n",
    "        title=\"📈 عدد محاولات تسجيل الدخول اليومية\",\n",
    "        markers=True\n",
    "    )\n",
    "    fig3.show()\n",
    "\n",
    "# 🔹 محاولات الدخول الفاشلة لكل IP (إن وجد)\n",
    "if \"src_ip\" in df_parsed.columns and \"result\" in df_parsed.columns:\n",
    "    failed_by_ip = df_parsed[df_parsed[\"result\"] == \"Failed\"][\"src_ip\"].value_counts().head(15)\n",
    "    fig4 = px.bar(\n",
    "        failed_by_ip,\n",
    "        x=failed_by_ip.index,\n",
    "        y=failed_by_ip.values,\n",
    "        title=\"🚫 أكثر 15 IP لديها محاولات فاشلة\",\n",
    "        labels={\"x\": \"IP\", \"y\": \"عدد المحاولات الفاشلة\"},\n",
    "        color=failed_by_ip.values,\n",
    "        color_continuous_scale=\"Oranges\"\n",
    "    )\n",
    "    fig4.update_layout(xaxis_tickangle=-45)\n",
    "    fig4.show()\n",
    "\n",
    "# ===============================\n",
    "# 6️⃣ حفظ تقارير HTML و CSV\n",
    "# ===============================\n",
    "parsed_report_path = os.path.join(REPORT_DIR, \"parsed_quality_report.html\")\n",
    "feature_report_path = os.path.join(REPORT_DIR, \"feature_quality_report.html\")\n",
    "summary_csv_path = os.path.join(REPORT_DIR, \"data_quality_summary.csv\")\n",
    "\n",
    "# تقارير HTML أساسية\n",
    "with open(parsed_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_parsed_large.csv</h1>\")\n",
    "    f.write(missing_parsed.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    if not result_counts.empty:\n",
    "        f.write(\"<h3>توزيع النتائج</h3>\")\n",
    "        f.write(result_counts.to_frame(\"count\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_parsed.describe().to_html(border=1))\n",
    "\n",
    "with open(feature_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_features_large.csv</h1>\")\n",
    "    f.write(missing_feat.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_feat.describe().to_html(border=1))\n",
    "\n",
    "summary.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ تم إنشاء تقارير HTML وCSV في: {REPORT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16172db3-393d-4926-b40b-ac8ad4a74450",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce7e5a-7888-4b5d-9f08-784c8e8025de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 📊 Data Quality Analysis - Advanced Jupyter Version\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ إعداد المسارات\n",
    "# ===============================\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "REPORT_DIR = os.path.join(BASE_DIR, \"reports\")\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "PARSED_FILE = os.path.join(DATA_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(DATA_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ===============================\n",
    "if not os.path.exists(PARSED_FILE) or not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(\"❌ تأكد من وجود ملفات الإدخال في مجلد data\")\n",
    "\n",
    "print(\"📥 تحميل الملفات الكبيرة...\")\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ فحص جودة البيانات الأساسية\n",
    "# ===============================\n",
    "missing_parsed = df_parsed.isnull().mean().sort_values(ascending=False)\n",
    "missing_feat = df_feat.isnull().mean().sort_values(ascending=False)\n",
    "result_counts = df_parsed['result'].value_counts() if 'result' in df_parsed.columns else pd.Series(dtype=int)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Dataset\": [\"auth_parsed_large\", \"auth_features_large\"],\n",
    "    \"Rows\": [len(df_parsed), len(df_feat)],\n",
    "    \"Columns\": [len(df_parsed.columns), len(df_feat.columns)],\n",
    "    \"Missing Ratio\": [missing_parsed.mean(), missing_feat.mean()]\n",
    "})\n",
    "\n",
    "display(summary)\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ عرض الأعمدة ذات القيم المفقودة الأعلى\n",
    "# ===============================\n",
    "print(\"\\n📉 Top 15 Missing Columns (auth_features_large.csv):\")\n",
    "display(missing_feat.head(15).to_frame(\"missing_ratio\"))\n",
    "\n",
    "# ===============================\n",
    "# 5️⃣ الرسوم التفاعلية - Plotly\n",
    "# ===============================\n",
    "\n",
    "# 🔹 توزيع نتائج الدخول (Success / Failed)\n",
    "if not result_counts.empty:\n",
    "    fig1 = px.pie(\n",
    "        result_counts,\n",
    "        names=result_counts.index,\n",
    "        values=result_counts.values,\n",
    "        title=\"🔐 توزيع نتائج تسجيل الدخول (نجاح / فشل)\",\n",
    "        color_discrete_sequence=px.colors.qualitative.Set2\n",
    "    )\n",
    "    fig1.show()\n",
    "\n",
    "# 🔹 نسب القيم المفقودة في auth_features_large.csv\n",
    "fig2 = px.bar(\n",
    "    missing_feat.head(20),\n",
    "    x=missing_feat.head(20).index,\n",
    "    y=missing_feat.head(20).values,\n",
    "    title=\"📉 أعلى الأعمدة من حيث نسبة القيم المفقودة\",\n",
    "    labels={\"x\": \"العمود\", \"y\": \"نسبة القيم المفقودة\"},\n",
    "    color=missing_feat.head(20).values,\n",
    "    color_continuous_scale=\"Reds\"\n",
    ")\n",
    "fig2.update_layout(xaxis_tickangle=-45)\n",
    "fig2.show()\n",
    "\n",
    "# 🔹 تحليل زمني لمحاولات الدخول (إن وجد timestamp)\n",
    "if \"timestamp\" in df_parsed.columns:\n",
    "    df_parsed_time = df_parsed.copy()\n",
    "    df_parsed_time[\"date\"] = df_parsed_time[\"timestamp\"].dt.date\n",
    "    login_trend = df_parsed_time.groupby(\"date\").size().reset_index(name=\"count\")\n",
    "    fig3 = px.line(\n",
    "        login_trend,\n",
    "        x=\"date\",\n",
    "        y=\"count\",\n",
    "        title=\"📈 عدد محاولات تسجيل الدخول اليومية\",\n",
    "        markers=True\n",
    "    )\n",
    "    fig3.show()\n",
    "\n",
    "# 🔹 محاولات الدخول الفاشلة لكل IP (إن وجد)\n",
    "if \"src_ip\" in df_parsed.columns and \"result\" in df_parsed.columns:\n",
    "    failed_by_ip = df_parsed[df_parsed[\"result\"] == \"Failed\"][\"src_ip\"].value_counts().head(15)\n",
    "    fig4 = px.bar(\n",
    "        failed_by_ip,\n",
    "        x=failed_by_ip.index,\n",
    "        y=failed_by_ip.values,\n",
    "        title=\"🚫 أكثر 15 IP لديها محاولات فاشلة\",\n",
    "        labels={\"x\": \"IP\", \"y\": \"عدد المحاولات الفاشلة\"},\n",
    "        color=failed_by_ip.values,\n",
    "        color_continuous_scale=\"Oranges\"\n",
    "    )\n",
    "    fig4.update_layout(xaxis_tickangle=-45)\n",
    "    fig4.show()\n",
    "\n",
    "# ===============================\n",
    "# 6️⃣ حفظ تقارير HTML و CSV\n",
    "# ===============================\n",
    "parsed_report_path = os.path.join(REPORT_DIR, \"parsed_quality_report.html\")\n",
    "feature_report_path = os.path.join(REPORT_DIR, \"feature_quality_report.html\")\n",
    "summary_csv_path = os.path.join(REPORT_DIR, \"data_quality_summary.csv\")\n",
    "\n",
    "# تقارير HTML أساسية\n",
    "with open(parsed_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_parsed_large.csv</h1>\")\n",
    "    f.write(missing_parsed.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    if not result_counts.empty:\n",
    "        f.write(\"<h3>توزيع النتائج</h3>\")\n",
    "        f.write(result_counts.to_frame(\"count\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_parsed.describe().to_html(border=1))\n",
    "\n",
    "with open(feature_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_features_large.csv</h1>\")\n",
    "    f.write(missing_feat.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_feat.describe().to_html(border=1))\n",
    "\n",
    "summary.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ تم إنشاء تقارير HTML وCSV في: {REPORT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f32767-bd22-4748-b419-28a8d89b84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 📊 Data Quality Analysis - Advanced Jupyter Version\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ إعداد المسارات\n",
    "# ===============================\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "REPORT_DIR = os.path.join(BASE_DIR, \"reports\")\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "PARSED_FILE = os.path.join(DATA_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(DATA_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ===============================\n",
    "if not os.path.exists(PARSED_FILE) or not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(\"❌ تأكد من وجود ملفات الإدخال في مجلد data\")\n",
    "\n",
    "print(\"📥 تحميل الملفات الكبيرة...\")\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ فحص جودة البيانات الأساسية\n",
    "# ===============================\n",
    "missing_parsed = df_parsed.isnull().mean().sort_values(ascending=False)\n",
    "missing_feat = df_feat.isnull().mean().sort_values(ascending=False)\n",
    "result_counts = df_parsed['result'].value_counts() if 'result' in df_parsed.columns else pd.Series(dtype=int)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Dataset\": [\"auth_parsed_large\", \"auth_features_large\"],\n",
    "    \"Rows\": [len(df_parsed), len(df_feat)],\n",
    "    \"Columns\": [len(df_parsed.columns), len(df_feat.columns)],\n",
    "    \"Missing Ratio\": [missing_parsed.mean(), missing_feat.mean()]\n",
    "})\n",
    "\n",
    "display(summary)\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ عرض الأعمدة ذات القيم المفقودة الأعلى\n",
    "# ===============================\n",
    "print(\"\\n📉 Top 15 Missing Columns (auth_features_large.csv):\")\n",
    "display(missing_feat.head(15).to_frame(\"missing_ratio\"))\n",
    "\n",
    "# ===============================\n",
    "# 5️⃣ الرسوم التفاعلية - Plotly\n",
    "# ===============================\n",
    "\n",
    "# 🔹 توزيع نتائج الدخول (Success / Failed)\n",
    "if not result_counts.empty:\n",
    "    fig1 = px.pie(\n",
    "        result_counts,\n",
    "        names=result_counts.index,\n",
    "        values=result_counts.values,\n",
    "        title=\"🔐 توزيع نتائج تسجيل الدخول (نجاح / فشل)\",\n",
    "        color_discrete_sequence=px.colors.qualitative.Set2\n",
    "    )\n",
    "    fig1.show()\n",
    "\n",
    "# 🔹 نسب القيم المفقودة في auth_features_large.csv\n",
    "fig2 = px.bar(\n",
    "    missing_feat.head(20),\n",
    "    x=missing_feat.head(20).index,\n",
    "    y=missing_feat.head(20).values,\n",
    "    title=\"📉 أعلى الأعمدة من حيث نسبة القيم المفقودة\",\n",
    "    labels={\"x\": \"العمود\", \"y\": \"نسبة القيم المفقودة\"},\n",
    "    color=missing_feat.head(20).values,\n",
    "    color_continuous_scale=\"Reds\"\n",
    ")\n",
    "fig2.update_layout(xaxis_tickangle=-45)\n",
    "fig2.show()\n",
    "\n",
    "# 🔹 تحليل زمني لمحاولات الدخول (إن وجد timestamp)\n",
    "if \"timestamp\" in df_parsed.columns:\n",
    "    df_parsed_time = df_parsed.copy()\n",
    "    df_parsed_time[\"date\"] = df_parsed_time[\"timestamp\"].dt.date\n",
    "    login_trend = df_parsed_time.groupby(\"date\").size().reset_index(name=\"count\")\n",
    "    fig3 = px.line(\n",
    "        login_trend,\n",
    "        x=\"date\",\n",
    "        y=\"count\",\n",
    "        title=\"📈 عدد محاولات تسجيل الدخول اليومية\",\n",
    "        markers=True\n",
    "    )\n",
    "    fig3.show()\n",
    "\n",
    "# 🔹 محاولات الدخول الفاشلة لكل IP (إن وجد)\n",
    "if \"src_ip\" in df_parsed.columns and \"result\" in df_parsed.columns:\n",
    "    failed_by_ip = df_parsed[df_parsed[\"result\"] == \"Failed\"][\"src_ip\"].value_counts().head(15)\n",
    "    fig4 = px.bar(\n",
    "        failed_by_ip,\n",
    "        x=failed_by_ip.index,\n",
    "        y=failed_by_ip.values,\n",
    "        title=\"🚫 أكثر 15 IP لديها محاولات فاشلة\",\n",
    "        labels={\"x\": \"IP\", \"y\": \"عدد المحاولات الفاشلة\"},\n",
    "        color=failed_by_ip.values,\n",
    "        color_continuous_scale=\"Oranges\"\n",
    "    )\n",
    "    fig4.update_layout(xaxis_tickangle=-45)\n",
    "    fig4.show()\n",
    "\n",
    "# ===============================\n",
    "# 6️⃣ حفظ تقارير HTML و CSV\n",
    "# ===============================\n",
    "parsed_report_path = os.path.join(REPORT_DIR, \"parsed_quality_report.html\")\n",
    "feature_report_path = os.path.join(REPORT_DIR, \"feature_quality_report.html\")\n",
    "summary_csv_path = os.path.join(REPORT_DIR, \"data_quality_summary.csv\")\n",
    "\n",
    "# تقارير HTML أساسية\n",
    "with open(parsed_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_parsed_large.csv</h1>\")\n",
    "    f.write(missing_parsed.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    if not result_counts.empty:\n",
    "        f.write(\"<h3>توزيع النتائج</h3>\")\n",
    "        f.write(result_counts.to_frame(\"count\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_parsed.describe().to_html(border=1))\n",
    "\n",
    "with open(feature_report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"<h1>تقرير جودة البيانات - auth_features_large.csv</h1>\")\n",
    "    f.write(missing_feat.to_frame(\"missing_ratio\").to_html(border=1))\n",
    "    f.write(\"<h3>وصف إحصائي</h3>\")\n",
    "    f.write(df_feat.describe().to_html(border=1))\n",
    "\n",
    "summary.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ تم إنشاء تقارير HTML وCSV في: {REPORT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7a46d-1fae-4908-bbb3-5b1ccbda2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 🔹 Data Quality Check (Interactive with Plotly)\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ Configure Plotly Renderer\n",
    "# -------------------------------\n",
    "pio.renderers.default = \"notebook_connected\"  # أفضل عرض تفاعلي داخل Jupyter\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ File Paths\n",
    "# -------------------------------\n",
    "DATA_DIR = \"data\"\n",
    "PARSED_FILE = os.path.join(DATA_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(DATA_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Load Data\n",
    "# -------------------------------\n",
    "if not os.path.exists(PARSED_FILE) or not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(\"❌ تأكد من وجود الملفات auth_parsed_large.csv و auth_features_large.csv في مجلد data\")\n",
    "\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "print(\"✅ تم تحميل البيانات بنجاح!\")\n",
    "print(f\"auth_parsed shape: {df_parsed.shape}\")\n",
    "print(f\"auth_features shape: {df_feat.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Missing Values Check\n",
    "# -------------------------------\n",
    "print(\"\\n--- نسبة القيم الفارغة لكل عمود ---\")\n",
    "missing_parsed = df_parsed.isnull().mean().sort_values(ascending=False)\n",
    "print(missing_parsed)\n",
    "\n",
    "missing_feat = df_feat.isnull().mean().sort_values(ascending=False)\n",
    "print(\"\\nfeature missing values:\")\n",
    "print(missing_feat)\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ Target Distribution\n",
    "# -------------------------------\n",
    "if 'result' in df_parsed.columns:\n",
    "    target_counts = df_parsed['result'].value_counts()\n",
    "    print(\"\\n--- توزيع النتائج (success vs failed) ---\")\n",
    "    print(target_counts)\n",
    "    \n",
    "    fig = px.pie(names=target_counts.index, values=target_counts.values,\n",
    "                 title=\"Distribution of login results\")\n",
    "    fig.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6️⃣ Numerical Features Summary\n",
    "# -------------------------------\n",
    "num_cols = df_feat.select_dtypes(include=np.number).columns.tolist()\n",
    "print(\"\\n--- إحصاءات عددية للميزات ---\")\n",
    "print(df_feat[num_cols].describe())\n",
    "\n",
    "fig = px.histogram(df_feat[num_cols], marginal=\"box\", nbins=50)\n",
    "fig.update_layout(title=\"Distribution of numeric features\")\n",
    "fig.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 7️⃣ Categorical Features Summary\n",
    "# -------------------------------\n",
    "cat_cols = df_feat.select_dtypes(include='object').columns.tolist()\n",
    "print(\"\\n--- توزيع الفئات للميزات الفئوية ---\")\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df_feat[col].value_counts().head(10))  # عرض أهم 10 فئات\n",
    "    \n",
    "    fig = px.bar(df_feat[col].value_counts().head(10),\n",
    "                 labels={'index': col, 'value':'Count'},\n",
    "                 title=f\"Top 10 categories for {col}\")\n",
    "    fig.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 8️⃣ Top IPs and Users\n",
    "# -------------------------------\n",
    "if 'ip' in df_parsed.columns:\n",
    "    top_ips = df_parsed['ip'].value_counts().head(20)\n",
    "    fig = px.bar(top_ips, labels={'index':'IP','value':'Count'}, title=\"Top 20 IPs by login attempts\")\n",
    "    fig.show()\n",
    "\n",
    "if 'user' in df_parsed.columns:\n",
    "    top_users = df_parsed['user'].value_counts().head(20)\n",
    "    fig = px.bar(top_users, labels={'index':'User','value':'Count'}, title=\"Top 20 Users by login attempts\")\n",
    "    fig.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ✅ Ready for next steps\n",
    "# -------------------------------\n",
    "print(\"\\n✅ فحص جودة البيانات تم بنجاح! يمكنك الآن متابعة Feature Engineering.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef20c3-f7f9-4ae3-a302-60847859596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1️⃣ عرض مسار العمل الحالي\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# 2️⃣ عرض الملفات الموجودة في مجلد data\n",
    "print(\"Files in data folder:\", os.listdir(\"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe9483-190c-49e9-b74c-4005ec59d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد المسارات وبيئة العمل\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# تغيير مجلد العمل إلى جذر المشروع\n",
    "os.chdir(\"/home/bakri/projects/login-anomaly\")\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "PARSED_FILE = \"data/auth_parsed_large.csv\"\n",
    "FEATURE_FILE = \"data/auth_features_large.csv\"\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ تحميل البيانات\n",
    "# ==============================\n",
    "if not os.path.exists(PARSED_FILE) or not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(\"❌ تأكد من وجود الملفات auth_parsed_large.csv و auth_features_large.csv في مجلد data\")\n",
    "\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "print(\"Parsed CSV shape:\", df_parsed.shape)\n",
    "print(\"Features CSV shape:\", df_feat.shape)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ فحص القيم المفقودة\n",
    "# ==============================\n",
    "print(\"\\n--- Missing Values in Parsed Data ---\")\n",
    "print(df_parsed.isnull().mean())\n",
    "\n",
    "print(\"\\n--- Missing Values in Features Data ---\")\n",
    "print(df_feat.isnull().mean())\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ فحص توزيع الفئات\n",
    "# ==============================\n",
    "if 'result' in df_parsed.columns:\n",
    "    print(\"\\n--- Result Counts ---\")\n",
    "    print(df_parsed['result'].value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ إحصاءات عددية Features\n",
    "# ==============================\n",
    "print(\"\\n--- Features Numeric Summary ---\")\n",
    "print(df_feat.describe())\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ رسوم بيانية أساسية\n",
    "# ==============================\n",
    "# توزيع نتائج تسجيل الدخول\n",
    "if 'result' in df_parsed.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x='result', data=df_parsed)\n",
    "    plt.title(\"Distribution of Login Results\")\n",
    "    plt.show()\n",
    "\n",
    "# القيم المفقودة في Features\n",
    "plt.figure(figsize=(8,4))\n",
    "missing_percent = df_feat.isnull().mean() * 100\n",
    "missing_percent = missing_percent[missing_percent > 0]\n",
    "missing_percent.sort_values(inplace=True)\n",
    "missing_percent.plot(kind='barh')\n",
    "plt.title(\"Missing Values (%) in Features\")\n",
    "plt.xlabel(\"Percentage (%)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13b098-dd7d-4215-88f1-b294f9f15b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 0️⃣ إعداد المسارات وبيئة العمل\n",
    "# ==============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# نتحقق من المجلد الحالي ونضع مسار البيانات بالنسبة له\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# قم بتعديل المسارات حسب مكان وجود الملفات داخل جهازك\n",
    "PARSED_FILE = \"../data/auth_parsed_large.csv\"      # إذا كنت في مجلد scripts\n",
    "FEATURE_FILE = \"../data/auth_features_large.csv\"\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ التأكد من وجود الملفات\n",
    "# ==============================\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "print(\"✅ جميع الملفات موجودة.\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ==============================\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "print(\"✅ تم تحميل البيانات\")\n",
    "print(\"Parsed CSV shape:\", df_parsed.shape)\n",
    "print(\"Features CSV shape:\", df_feat.shape)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ فحص القيم المفقودة\n",
    "# ==============================\n",
    "print(\"\\n--- Missing Values in Parsed Data ---\")\n",
    "print(df_parsed.isnull().mean())\n",
    "\n",
    "print(\"\\n--- Missing Values in Features Data ---\")\n",
    "print(df_feat.isnull().mean())\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ فحص توزيع النتائج\n",
    "# ==============================\n",
    "if 'result' in df_parsed.columns:\n",
    "    print(\"\\n--- Result Counts ---\")\n",
    "    print(df_parsed['result'].value_counts())\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ إحصاءات عددية Features\n",
    "# ==============================\n",
    "print(\"\\n--- Features Numeric Summary ---\")\n",
    "print(df_feat.describe())\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ رسم بياني أساسي للقيم المفقودة\n",
    "# ==============================\n",
    "missing_percent = df_feat.isnull().mean() * 100\n",
    "missing_percent = missing_percent[missing_percent > 0]\n",
    "if len(missing_percent) > 0:\n",
    "    missing_percent.sort_values(inplace=True)\n",
    "    missing_percent.plot(kind='barh', figsize=(8,4), title=\"Missing Values (%) in Features\")\n",
    "    plt.xlabel(\"Percentage (%)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"لا توجد قيم مفقودة في Features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbef96c-e59e-47cf-8c58-1c2afc249bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Check Data Quality - auth_parsed_large & auth_features_large\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ إعداد المسارات\n",
    "# ------------------------------\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly/data\"\n",
    "PARSED_FILE = os.path.join(BASE_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ الملف غير موجود: {f}\")\n",
    "print(\"✅ جميع الملفات موجودة.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ تحميل البيانات\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ فحص القيم المفقودة\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values -----\")\n",
    "print(df_parsed.isnull().mean())\n",
    "\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values -----\")\n",
    "print(df_feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ توزيع النتائج في auth_parsed\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Result Distribution -----\")\n",
    "print(df_parsed['result'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df_parsed, x='result')\n",
    "plt.title(\"Distribution of Result\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ إحصاءات عددية للميزات\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_features_large.csv: Numerical Summary -----\")\n",
    "print(df_feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ فحص القيم المتطرفة (اختياري)\n",
    "# ------------------------------\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=df_feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ فحص التوزيعات الأساسية (histogram)\n",
    "# ------------------------------\n",
    "df_feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ خطوة جودة البيانات اكتملت.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6854d1-42d0-4e34-b862-25fded9a07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Check Data Quality - auth_parsed_large & auth_features_large\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Setup Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly/data\"\n",
    "PARSED_FILE = os.path.join(BASE_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Check Missing Values\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values -----\")\n",
    "print(df_parsed.isnull().mean())  # Percentage of missing values per column\n",
    "\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values -----\")\n",
    "print(df_feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Check Result Distribution in auth_parsed\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Result Distribution -----\")\n",
    "print(df_parsed['result'].value_counts())\n",
    "\n",
    "# Plot count of each result class\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df_parsed, x='result')\n",
    "plt.title(\"Distribution of Result\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Numerical Summary of Features\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_features_large.csv: Numerical Summary -----\")\n",
    "print(df_feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Check for Outliers (optional)\n",
    "# ------------------------------\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=df_feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Check Basic Distributions (histograms)\n",
    "# ------------------------------\n",
    "df_feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Data quality check completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53ae01-a660-43ca-bf85-c8695661409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Interactive Data Quality Check for Login Anomaly Project\n",
    "# ==============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Setup file paths\n",
    "# ------------------------------\n",
    "BASE_DIR = '../data'\n",
    "PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All data files are present.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(f\"✅ Loaded parsed data: {df_parsed.shape}\")\n",
    "print(f\"✅ Loaded feature data: {df_feat.shape}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Check Missing Values\n",
    "# ------------------------------\n",
    "missing_parsed = df_parsed.isnull().mean().sort_values(ascending=False)\n",
    "missing_feat = df_feat.isnull().mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"🔹 Missing values in parsed data:\")\n",
    "print(missing_parsed[missing_parsed>0])\n",
    "print(\"\\n🔹 Missing values in feature data:\")\n",
    "print(missing_feat[missing_feat>0])\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Check result distribution\n",
    "# ------------------------------\n",
    "if 'result' in df_parsed.columns:\n",
    "    fig = px.histogram(df_parsed, x='result', title='Result Distribution', text_auto=True)\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Quick numeric stats for features\n",
    "# ------------------------------\n",
    "numeric_cols = df_feat.select_dtypes(include=np.number).columns\n",
    "desc = df_feat[numeric_cols].describe().T\n",
    "print(\"🔹 Feature statistics:\")\n",
    "print(desc)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Boxplots for outliers (interactive)\n",
    "# ------------------------------\n",
    "for col in numeric_cols:\n",
    "    fig = px.box(df_feat, y=col, title=f\"Boxplot of {col}\")\n",
    "    fig.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Histograms for distributions\n",
    "# ------------------------------\n",
    "for col in numeric_cols:\n",
    "    fig = px.histogram(df_feat, x=col, nbins=50, title=f\"Histogram of {col}\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c242f-3364-4778-ab11-ac996e7ffb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Data Quality Check for Login Anomaly Project\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------\n",
    "# 0️⃣ Set data file paths (flexible)\n",
    "# ------------------------------\n",
    "BASE_DIR = os.path.join(os.getcwd(), '..', 'data')  # Works if running from scripts/\n",
    "PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All data files are present.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Load CSV files\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Check missing values\n",
    "# ------------------------------\n",
    "print(\"\\n--- Missing Values in auth_parsed_large.csv ---\")\n",
    "print(df_parsed.isnull().mean().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n--- Missing Values in auth_features_large.csv ---\")\n",
    "print(df_feat.isnull().mean().sort_values(ascending=False))\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Basic distribution checks\n",
    "# ------------------------------\n",
    "print(\"\\n--- Result column distribution ---\")\n",
    "if 'result' in df_parsed.columns:\n",
    "    print(df_parsed['result'].value_counts())\n",
    "else:\n",
    "    print(\"Column 'result' not found in auth_parsed_large.csv\")\n",
    "\n",
    "print(\"\\n--- Feature dataset summary ---\")\n",
    "print(df_feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Optional: Preview top rows\n",
    "# ------------------------------\n",
    "print(\"\\n--- Preview auth_parsed_large.csv ---\")\n",
    "print(df_parsed.head())\n",
    "\n",
    "print(\"\\n--- Preview auth_features_large.csv ---\")\n",
    "print(df_feat.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a343e37-dee9-4730-95b7-9a3caee0f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Data Quality Check - Login Anomaly Project\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ Set absolute paths for data\n",
    "# ===============================\n",
    "PARSED_FILE = '/home/bakri/projects/login-anomaly/data/auth_parsed_large.csv'\n",
    "FEATURE_FILE = '/home/bakri/projects/login-anomaly/data/auth_features_large.csv'\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All data files are present.\")\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ Load CSV files\n",
    "# ===============================\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "print(\"✅ CSV files loaded successfully.\")\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ Basic Data Overview\n",
    "# ===============================\n",
    "print(\"\\n--- Parsed Data Info ---\")\n",
    "print(df_parsed.info())\n",
    "print(\"\\nMissing values percentage in parsed data:\")\n",
    "print(df_parsed.isnull().mean()*100)\n",
    "\n",
    "print(\"\\nResult distribution:\")\n",
    "print(df_parsed['result'].value_counts())\n",
    "\n",
    "print(\"\\n--- Features Data Info ---\")\n",
    "print(df_feat.info())\n",
    "print(\"\\nMissing values percentage in features data:\")\n",
    "print(df_feat.isnull().mean()*100)\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(df_feat.describe())\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ Visualize missing values\n",
    "# ===============================\n",
    "missing_parsed = df_parsed.isnull().sum()\n",
    "missing_feat = df_feat.isnull().sum()\n",
    "\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Bar(x=missing_parsed.index, y=missing_parsed.values, marker_color='red'))\n",
    "fig1.update_layout(title=\"Missing Values in Parsed Data\", xaxis_title=\"Columns\", yaxis_title=\"Count\")\n",
    "fig1.show()\n",
    "\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Bar(x=missing_feat.index, y=missing_feat.values, marker_color='blue'))\n",
    "fig2.update_layout(title=\"Missing Values in Features Data\", xaxis_title=\"Columns\", yaxis_title=\"Count\")\n",
    "fig2.show()\n",
    "\n",
    "# ===============================\n",
    "# 5️⃣ Visualize class balance\n",
    "# ===============================\n",
    "fig3 = px.histogram(df_parsed, x='result', title=\"Login Result Distribution\")\n",
    "fig3.show()\n",
    "\n",
    "# ===============================\n",
    "# 6️⃣ Check outliers in numeric features\n",
    "# ===============================\n",
    "numeric_cols = df_feat.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    fig = px.box(df_feat, y=col, points=\"outliers\", title=f\"Outliers in {col}\")\n",
    "    fig.show()\n",
    "\n",
    "# ===============================\n",
    "# ✅ Summary\n",
    "# - All files loaded successfully\n",
    "# - Missing values and distributions visualized\n",
    "# - Numeric features outliers inspected\n",
    "# ===============================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd2117-c2ad-4669-8315-d1c27806c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Check Data Quality - auth_parsed_large & auth_features_large\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Setup Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly/data\"\n",
    "PARSED_FILE = os.path.join(BASE_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Check Missing Values\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values -----\")\n",
    "print(df_parsed.isnull().mean())  # Percentage of missing values per column\n",
    "\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values -----\")\n",
    "print(df_feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Check Result Distribution in auth_parsed\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Result Distribution -----\")\n",
    "print(df_parsed['result'].value_counts())\n",
    "\n",
    "# Plot count of each result class\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df_parsed, x='result')\n",
    "plt.title(\"Distribution of Result\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Numerical Summary of Features\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_features_large.csv: Numerical Summary -----\")\n",
    "print(df_feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Check for Outliers (optional)\n",
    "# ------------------------------\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=df_feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Check Basic Distributions (histograms)\n",
    "# ------------------------------\n",
    "df_feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Data quality check completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9089399-2194-4df1-ac01-dc384cae59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Interactive Data Quality Check - auth_parsed_large & auth_features_large\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Setup Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly/data\"\n",
    "PARSED_FILE = os.path.join(BASE_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Check Missing Values\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values -----\")\n",
    "print(df_parsed.isnull().mean())\n",
    "\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values -----\")\n",
    "print(df_feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Check Result Distribution in auth_parsed\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Result Distribution -----\")\n",
    "print(df_parsed['result'].value_counts())\n",
    "\n",
    "fig_result = px.histogram(df_parsed, x='result', title=\"Distribution of Result\", \n",
    "                          color='result', text_auto=True)\n",
    "fig_result.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Numerical Summary of Features\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_features_large.csv: Numerical Summary -----\")\n",
    "print(df_feat.describe())\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Check for Outliers (Boxplots)\n",
    "# ------------------------------\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    fig_box = px.box(df_feat, y=col, points=\"all\", title=f\"Boxplot of {col}\")\n",
    "    fig_box.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Check Basic Distributions (Histograms)\n",
    "# ------------------------------\n",
    "for col in numeric_cols:\n",
    "    fig_hist = px.histogram(df_feat, x=col, nbins=30, title=f\"Histogram of {col}\")\n",
    "    fig_hist.show()\n",
    "\n",
    "print(\"✅ Interactive data quality check completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8606b9-591d-4a2c-a653-5cd013810202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Dashboard: Interactive Data Quality Check\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Setup Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly/data\"\n",
    "PARSED_FILE = os.path.join(BASE_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Missing Values\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values -----\")\n",
    "print(df_parsed.isnull().mean())\n",
    "\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values -----\")\n",
    "print(df_feat.isnull().mean())\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Result Distribution\n",
    "# ------------------------------\n",
    "fig_result = px.histogram(df_parsed, x='result', color='result', text_auto=True,\n",
    "                          title=\"Distribution of Result in auth_parsed_large.csv\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Numerical Features Summary\n",
    "# ------------------------------\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Create Subplots: Boxplots + Histograms\n",
    "# ------------------------------\n",
    "n_cols = 2  # number of plots per row\n",
    "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=n_rows*2, cols=n_cols,\n",
    "    subplot_titles=[f\"Boxplot of {c}\" for c in numeric_cols] +\n",
    "                   [f\"Histogram of {c}\" for c in numeric_cols]\n",
    ")\n",
    "\n",
    "# Add boxplots\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    row = i // n_cols + 1\n",
    "    col_pos = i % n_cols + 1\n",
    "    fig.add_trace(\n",
    "        go.Box(y=df_feat[col], name=col, boxpoints='all'),\n",
    "        row=row, col=col_pos\n",
    "    )\n",
    "\n",
    "# Add histograms\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    row = n_rows + i // n_cols + 1\n",
    "    col_pos = i % n_cols + 1\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df_feat[col], nbinsx=30, name=col),\n",
    "        row=row, col=col_pos\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=300*n_rows*2, width=1000, title_text=\"Boxplots & Histograms of Numeric Features\")\n",
    "fig.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Show Result Distribution\n",
    "# ------------------------------\n",
    "fig_result.show()\n",
    "\n",
    "print(\"✅ Interactive dashboard completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04a977-ec15-4513-ba2f-3b7f1a60f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Full Interactive Data Quality Dashboard\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Setup Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly/data\"\n",
    "PARSED_FILE = os.path.join(BASE_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Missing Values Summary\n",
    "# ------------------------------\n",
    "missing_parsed = df_parsed.isnull().mean() * 100\n",
    "missing_feat = df_feat.isnull().mean() * 100\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Interactive Result Distribution\n",
    "# ------------------------------\n",
    "fig_result = px.histogram(df_parsed, x='result', color='result', text_auto=True,\n",
    "                          title=\"Distribution of Result in auth_parsed_large.csv\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Numeric Columns\n",
    "# ------------------------------\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Create Interactive Dropdown Dashboard\n",
    "# ------------------------------\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Boxplots & Histograms for each numeric feature\n",
    "for col in numeric_cols:\n",
    "    fig.add_trace(go.Box(y=df_feat[col], name=f\"{col} - Boxplot\", visible=False))\n",
    "    fig.add_trace(go.Histogram(x=df_feat[col], nbinsx=30, name=f\"{col} - Histogram\", visible=False))\n",
    "\n",
    "# Make the first feature visible by default\n",
    "fig.data[0].visible = True\n",
    "fig.data[1].visible = True\n",
    "\n",
    "# Dropdown buttons\n",
    "buttons = []\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    visibility = [False] * len(fig.data)\n",
    "    visibility[2*i] = True   # Boxplot\n",
    "    visibility[2*i+1] = True # Histogram\n",
    "    buttons.append(dict(label=col,\n",
    "                        method=\"update\",\n",
    "                        args=[{\"visible\": visibility},\n",
    "                              {\"title\": f\"Data Quality Dashboard: {col}\"}]))\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(active=0, buttons=buttons, x=1.1, y=0.8)],\n",
    "    title=f\"Data Quality Dashboard: {numeric_cols[0]}\",\n",
    "    height=700,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Show Result Distribution Separately\n",
    "# ------------------------------\n",
    "fig_result.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Display Missing Values\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values (%) -----\")\n",
    "print(missing_parsed)\n",
    "\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values (%) -----\")\n",
    "print(missing_feat)\n",
    "\n",
    "print(\"✅ Full interactive data quality dashboard completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e8d0a9-4b0e-4f20-941f-d7583bf4e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Advanced Full Data Quality Dashboard\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Setup Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly/data\"\n",
    "PARSED_FILE = os.path.join(BASE_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Missing Values Summary\n",
    "# ------------------------------\n",
    "missing_parsed = df_parsed.isnull().mean() * 100\n",
    "missing_feat = df_feat.isnull().mean() * 100\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Heatmap of Missing Values\n",
    "# ------------------------------\n",
    "fig_missing = px.imshow(df_feat.isnull(),\n",
    "                        color_continuous_scale='Viridis',\n",
    "                        title=\"Heatmap of Missing Values (auth_features_large.csv)\")\n",
    "fig_missing.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Result Distribution\n",
    "# ------------------------------\n",
    "fig_result = px.histogram(df_parsed, x='result', color='result', text_auto=True,\n",
    "                          title=\"Distribution of Result in auth_parsed_large.csv\")\n",
    "fig_result.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Numeric Columns\n",
    "# ------------------------------\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Interactive Boxplots + Histograms\n",
    "# ------------------------------\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each numeric feature\n",
    "for col in numeric_cols:\n",
    "    fig.add_trace(go.Box(y=df_feat[col], name=f\"{col} - Boxplot\", visible=False))\n",
    "    fig.add_trace(go.Histogram(x=df_feat[col], nbinsx=30, name=f\"{col} - Histogram\", visible=False))\n",
    "\n",
    "# Show first feature by default\n",
    "fig.data[0].visible = True\n",
    "fig.data[1].visible = True\n",
    "\n",
    "# Dropdown buttons\n",
    "buttons = []\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    visibility = [False] * len(fig.data)\n",
    "    visibility[2*i] = True\n",
    "    visibility[2*i+1] = True\n",
    "    buttons.append(dict(label=col,\n",
    "                        method=\"update\",\n",
    "                        args=[{\"visible\": visibility},\n",
    "                              {\"title\": f\"Data Quality Dashboard: {col}\"}]))\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(active=0, buttons=buttons, x=1.1, y=0.8)],\n",
    "    title=f\"Data Quality Dashboard: {numeric_cols[0]}\",\n",
    "    height=700,\n",
    "    width=1100\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Display Missing Values in Console\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values (%) -----\")\n",
    "print(missing_parsed)\n",
    "\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values (%) -----\")\n",
    "print(missing_feat)\n",
    "\n",
    "print(\"✅ Full advanced interactive data quality dashboard completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f80baf-2062-4915-bf25-b2fc6d99e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Full Integrated Data Quality Dashboard\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Setup Paths\n",
    "# ------------------------------\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly/data\"\n",
    "PARSED_FILE = os.path.join(BASE_DIR, \"auth_parsed_large.csv\")\n",
    "FEATURE_FILE = os.path.join(BASE_DIR, \"auth_features_large.csv\")\n",
    "\n",
    "# Check if files exist\n",
    "for f in [PARSED_FILE, FEATURE_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All files are present.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Load Data\n",
    "# ------------------------------\n",
    "df_parsed = pd.read_csv(PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(FEATURE_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Compute Missing Values\n",
    "# ------------------------------\n",
    "missing_parsed = df_parsed.isnull().mean() * 100\n",
    "missing_feat = df_feat.isnull().mean() * 100\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Create Subplots\n",
    "# ------------------------------\n",
    "num_numeric = len(df_feat.select_dtypes(include='number').columns)\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\"Missing Values Heatmap\", \"Result Distribution\",\n",
    "                    \"Numeric Features Boxplots\", \"Numeric Features Histograms\")\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Missing Values Heatmap\n",
    "# ------------------------------\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=df_feat.isnull().astype(int).T.values,\n",
    "        x=df_feat.index,\n",
    "        y=df_feat.columns,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title=\"Missing\")\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ Result Distribution\n",
    "# ------------------------------\n",
    "result_counts = df_parsed['result'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=result_counts.index,\n",
    "        y=result_counts.values,\n",
    "        text=result_counts.values,\n",
    "        textposition='auto'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 7️⃣ Numeric Features Boxplots\n",
    "# ------------------------------\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns\n",
    "for col in numeric_cols:\n",
    "    fig.add_trace(go.Box(y=df_feat[col], name=col, boxpoints='outliers', marker_color='blue'), row=2, col=1)\n",
    "\n",
    "# ------------------------------\n",
    "# 8️⃣ Numeric Features Histograms\n",
    "# ------------------------------\n",
    "for col in numeric_cols:\n",
    "    fig.add_trace(go.Histogram(x=df_feat[col], nbinsx=30, name=col, opacity=0.7), row=2, col=2)\n",
    "\n",
    "# ------------------------------\n",
    "# 9️⃣ Layout and Show\n",
    "# ------------------------------\n",
    "fig.update_layout(\n",
    "    height=900, width=1200,\n",
    "    title_text=\"Integrated Data Quality Dashboard\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 10️⃣ Print Missing Values\n",
    "# ------------------------------\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values (%) -----\")\n",
    "print(missing_parsed)\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values (%) -----\")\n",
    "print(missing_feat)\n",
    "\n",
    "print(\"✅ Integrated data quality dashboard completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51708d97-5822-416b-90c4-443b433914af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Pipeline (Full + Data Quality Dashboard)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Plotly for interactive dashboard\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = \"/home/bakri/projects/login-anomaly/data\"\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, \"auth_parsed_large.csv\")\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, \"auth_features_large.csv\")\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, \"auth_features_new.csv\")\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, \"GeoLite2-City.mmdb\")\n",
    "MODEL_FILE = os.path.join(BASE_DIR, \"random_forest_model_final.joblib\")\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, \"feature_columns.joblib\")\n",
    "ALERT_FILE = os.path.join(BASE_DIR, \"alerts.csv\")\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Data Quality Dashboard\n",
    "# ==============================\n",
    "def run_data_quality_dashboard(df_parsed, df_feat):\n",
    "    # Missing Values\n",
    "    missing_parsed = df_parsed.isnull().mean() * 100\n",
    "    missing_feat = df_feat.isnull().mean() * 100\n",
    "\n",
    "    # Result distribution\n",
    "    result_counts = df_parsed['result'].value_counts()\n",
    "\n",
    "    # Numeric features\n",
    "    numeric_cols = df_feat.select_dtypes(include='number').columns\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\"Missing Values Heatmap\", \"Result Distribution\",\n",
    "                        \"Numeric Features Boxplots\", \"Numeric Features Histograms\")\n",
    "    )\n",
    "\n",
    "    # Missing Values Heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=df_feat.isnull().astype(int).T.values,\n",
    "            x=df_feat.index,\n",
    "            y=df_feat.columns,\n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(title=\"Missing\")\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Result Distribution\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=result_counts.index,\n",
    "            y=result_counts.values,\n",
    "            text=result_counts.values,\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Numeric Features Boxplots\n",
    "    for col in numeric_cols:\n",
    "        fig.add_trace(go.Box(y=df_feat[col], name=col, boxpoints='outliers', marker_color='blue'), row=2, col=1)\n",
    "\n",
    "    # Numeric Features Histograms\n",
    "    for col in numeric_cols:\n",
    "        fig.add_trace(go.Histogram(x=df_feat[col], nbinsx=30, name=col, opacity=0.7), row=2, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=900, width=1200,\n",
    "        title_text=\"Integrated Data Quality Dashboard\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    print(\"\\n----- auth_parsed_large.csv: Missing Values (%) -----\")\n",
    "    print(missing_parsed)\n",
    "    print(\"\\n----- auth_features_large.csv: Missing Values (%) -----\")\n",
    "    print(missing_feat)\n",
    "    print(\"✅ Data quality dashboard completed.\\n\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Load Data\n",
    "# ==============================\n",
    "df_parsed = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# Run dashboard\n",
    "run_data_quality_dashboard(df_parsed, df_feat)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "df_parsed['failed_flag'] = (df_parsed['result']=='failed').astype(int)\n",
    "\n",
    "# Hour & Night\n",
    "df_feat['hour'] = df_parsed['timestamp'].dt.hour\n",
    "df_feat['is_night'] = df_feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df_parsed.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "df_feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df_parsed['failed_streak'] = df_parsed.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "df_feat['failed_streak'] = df_parsed['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df_parsed['unique_users_last_5'] = df_parsed.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "df_feat['unique_users_last_5'] = df_parsed['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "unique_ips = df_parsed['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df_parsed = df_parsed.join(geo_df, on='ip')\n",
    "df_feat['lat'] = df_parsed['lat'].fillna(0)\n",
    "df_feat['lon'] = df_parsed['lon'].fillna(0)\n",
    "df_feat['geo_country'] = df_parsed['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = df_feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = df_feat[numeric_cols].fillna(0)\n",
    "y = (df_parsed['result']=='success').astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale & Handle Imbalance\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Feature Importance\n",
    "# ==============================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔟 Save Model & Features\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣1️⃣ Predict New Data & Generate Alerts\n",
    "# ==============================\n",
    "if os.path.exists(FEATURE_FILE_NEW):\n",
    "    df_new = pd.read_csv(FEATURE_FILE_NEW, low_memory=False)\n",
    "    \n",
    "    # Ensure all trained features exist\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= 0.5).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    print(\"✅ Alerts generated for new data.\")\n",
    "else:\n",
    "    print(\"⚠️ No new feature file found. Skipping alert generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc090e8-5cbb-4b11-8bda-a8da8d7aa196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Pipeline (Full, Robust) with Hyperparameter Tuning\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_tuned.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Hyperparameter Tuning (RandomForest)\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [5, 10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='recall',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best Hyperparameters:\")\n",
    "print(rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "if os.path.exists(FEATURE_FILE_NEW):\n",
    "    df_new = pd.read_csv(FEATURE_FILE_NEW, low_memory=False)\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= 0.5).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    print(\"✅ Alerts generated for new data.\")\n",
    "else:\n",
    "    print(\"⚠️ No new feature file found. Skipping alert generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370ef09-8d9a-4eb5-9aa5-5933cd3d437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Pipeline (Full, Robust) with Hyperparameter Tuning\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_tuned.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Hyperparameter Tuning (RandomForest)\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [5, 10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='recall',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best Hyperparameters:\")\n",
    "print(rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "if os.path.exists(FEATURE_FILE_NEW):\n",
    "    df_new = pd.read_csv(FEATURE_FILE_NEW, low_memory=False)\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= 0.5).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    print(\"✅ Alerts generated for new data.\")\n",
    "else:\n",
    "    print(\"⚠️ No new feature file found. Skipping alert generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cccfc49-a66e-4fc0-abba-7815b5d61a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Pipeline (Full, Robust) with Hyperparameter Tuning\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_tuned.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Hyperparameter Tuning (RandomForest)\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [5, 10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='recall',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best Hyperparameters:\")\n",
    "print(rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "if os.path.exists(FEATURE_FILE_NEW):\n",
    "    df_new = pd.read_csv(FEATURE_FILE_NEW, low_memory=False)\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= 0.5).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    print(\"✅ Alerts generated for new data.\")\n",
    "else:\n",
    "    print(\"⚠️ No new feature file found. Skipping alert generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dd95b-394d-4d3b-b77e-4d95dd9a32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Pipeline (Full, Robust, Step 5 Enhanced)\n",
    "# ==============================\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Train Stronger Model + Cross-validation (RandomizedSearchCV)\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "rf_base = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rs = RandomizedSearchCV(rf_base, param_dist, n_iter=10, cv=3, scoring='recall', n_jobs=-1, verbose=1)\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "best_clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = best_clf.predict(X_test)\n",
    "y_proba = best_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n----- Classification Report -----\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(best_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': best_clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb3c6d-bb4f-4cbd-b51c-51c1cca37451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Full Login Anomaly Pipeline with Data Quality Dashboard + Step 5\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Data Quality Dashboard\n",
    "# ==============================\n",
    "df_parsed = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "df_feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# Missing Values\n",
    "missing_parsed = df_parsed.isnull().mean() * 100\n",
    "missing_feat = df_feat.isnull().mean() * 100\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values (%) -----\")\n",
    "print(missing_parsed)\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values (%) -----\")\n",
    "print(missing_feat)\n",
    "\n",
    "# Result Distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df_parsed, x='result')\n",
    "plt.title(\"Result Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Numeric Boxplots\n",
    "numeric_cols = df_feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=df_feat[numeric_cols])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Numeric Features Boxplots\")\n",
    "plt.show()\n",
    "\n",
    "# Numeric Histograms\n",
    "df_feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Numeric Features Histograms\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Data quality dashboard displayed successfully.\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "df_parsed['result_bin'] = df_parsed['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df_parsed['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "df_feat['hour'] = df_parsed['timestamp'].dt.hour\n",
    "df_feat['is_night'] = df_feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Avg interarrival per IP\n",
    "df_sorted = df_parsed.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "df_feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df_parsed['failed_flag'] = (df_parsed['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df_parsed['failed_streak'] = df_parsed.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "df_feat['failed_streak'] = df_parsed['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df_parsed['unique_users_last_5'] = df_parsed.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "df_feat['unique_users_last_5'] = df_parsed['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df_parsed['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df_parsed = df_parsed.join(geo_df, on='ip')\n",
    "df_feat['lat'] = df_parsed['lat'].fillna(0)\n",
    "df_feat['lon'] = df_parsed['lon'].fillna(0)\n",
    "df_feat['geo_country'] = df_parsed['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix & Scaling\n",
    "# ==============================\n",
    "numeric_cols = df_feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = df_feat[numeric_cols].fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ RandomizedSearchCV (Step 5)\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='recall',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "rs.fit(X_train, y_train)\n",
    "best_clf = rs.best_estimator_\n",
    "print(\"✅ Best RandomForest parameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Evaluate Best Model\n",
    "# ==============================\n",
    "y_pred = best_clf.predict(X_test)\n",
    "y_proba = best_clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(best_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Save Model & Features\n",
    "# ==============================\n",
    "joblib.dump({'model': best_clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5ba0c-0cf4-4112-a16f-8afada585d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Pipeline with Full Data Quality Dashboard\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Data Quality Dashboard (Step 1)\n",
    "# ==============================\n",
    "# Missing Values\n",
    "missing_parsed = df.isnull().mean() * 100\n",
    "missing_feat = feat.isnull().mean() * 100\n",
    "\n",
    "# Result Distribution\n",
    "result_counts = df['result'].value_counts()\n",
    "\n",
    "# Numeric Features\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\"Missing Values Heatmap\", \"Result Distribution\",\n",
    "                    \"Numeric Features Boxplots\", \"Numeric Features Histograms\")\n",
    ")\n",
    "\n",
    "# Missing Values Heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=feat.isnull().astype(int).T.values,\n",
    "        x=feat.index,\n",
    "        y=feat.columns,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title=\"Missing\")\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Result Distribution\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=result_counts.index,\n",
    "        y=result_counts.values,\n",
    "        text=result_counts.values,\n",
    "        textposition='auto'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Numeric Features Boxplots\n",
    "for col in numeric_cols:\n",
    "    fig.add_trace(go.Box(y=feat[col], name=col, boxpoints='outliers', marker_color='blue'), row=2, col=1)\n",
    "\n",
    "# Numeric Features Histograms\n",
    "for col in numeric_cols:\n",
    "    fig.add_trace(go.Histogram(x=feat[col], nbinsx=30, name=col, opacity=0.7), row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900, width=1200,\n",
    "    title_text=\"Integrated Data Quality Dashboard\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Print missing values\n",
    "print(\"\\n----- auth_parsed_large.csv: Missing Values (%) -----\")\n",
    "print(missing_parsed)\n",
    "print(\"\\n----- auth_features_large.csv: Missing Values (%) -----\")\n",
    "print(missing_feat)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Hyperparameter Tuning for RandomForest (Step 5)\n",
    "# ==============================\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rs = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=20, cv=3, scoring='recall', random_state=42, n_jobs=-1)\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best RandomForest parameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance Plot\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "if os.path.exists(FEATURE_FILE_NEW):\n",
    "    df_new = pd.read_csv(FEATURE_FILE_NEW, low_memory=False)\n",
    "    \n",
    "    # Ensure all trained features exist\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= 0.5).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    print(\"✅ Alerts generated for new data.\")\n",
    "else:\n",
    "    print(\"⚠️ No new feature file found. Skipping alert generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc675a6-d38f-4a18-b9d9-eead51fd04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 🔹 Full Login Anomaly Detection Pipeline + Data Quality Dashboard\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "print(\"✅ All required files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Data Quality Checks\n",
    "# ==============================\n",
    "# Missing Values\n",
    "missing_parsed = df.isnull().mean() * 100\n",
    "missing_feat = feat.isnull().mean() * 100\n",
    "print(\"\\n----- Missing Values (%) -----\")\n",
    "print(\"auth_parsed_large.csv:\\n\", missing_parsed)\n",
    "print(\"\\nauth_features_large.csv:\\n\", missing_feat)\n",
    "\n",
    "# Result Distribution\n",
    "print(\"\\n----- Result Distribution -----\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "# Original matplotlib/seaborn plots\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x='result')\n",
    "plt.title(\"Distribution of Result\")\n",
    "plt.show()\n",
    "\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# Plotly interactive dashboard\n",
    "num_numeric = len(numeric_cols)\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\"Missing Values Heatmap\", \"Result Distribution\",\n",
    "                    \"Numeric Features Boxplots\", \"Numeric Features Histograms\")\n",
    ")\n",
    "\n",
    "# Missing Values Heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=feat.isnull().astype(int).T.values,\n",
    "               x=feat.index, y=feat.columns,\n",
    "               colorscale='Viridis', colorbar=dict(title=\"Missing\")),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Result Distribution\n",
    "result_counts = df['result'].value_counts()\n",
    "fig.add_trace(go.Bar(x=result_counts.index, y=result_counts.values,\n",
    "                     text=result_counts.values, textposition='auto'),\n",
    "              row=1, col=2)\n",
    "\n",
    "# Boxplots\n",
    "for col in numeric_cols:\n",
    "    fig.add_trace(go.Box(y=feat[col], name=col, boxpoints='outliers', marker_color='blue'), row=2, col=1)\n",
    "\n",
    "# Histograms\n",
    "for col in numeric_cols:\n",
    "    fig.add_trace(go.Histogram(x=feat[col], nbinsx=30, name=col, opacity=0.7), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=900, width=1200,\n",
    "                  title_text=\"Integrated Data Quality Dashboard\",\n",
    "                  showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "print(\"✅ Data quality checks completed.\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Model (RandomForest)\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "if os.path.exists(FEATURE_FILE_NEW):\n",
    "    df_new = pd.read_csv(FEATURE_FILE_NEW, low_memory=False)\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= 0.5).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    print(\"✅ Alerts generated for new data.\")\n",
    "else:\n",
    "    print(\"⚠️ No new feature file found. Skipping alert generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086abd4-25ed-46f6-b89c-687234aa8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Pipeline (Full, Robust) + Step 6\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train Model\n",
    "# ==============================\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Evaluate\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feature Importance\n",
    "# ==============================\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Step 6: Save Model & Feature List (Production Ready)\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model, scaler, and feature list saved for production use.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Utility Function: Predict New Data\n",
    "# ==============================\n",
    "def predict_new_data(new_feat_file, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Load new feature CSV, ensure all trained features exist,\n",
    "    scale, predict probabilities and generate alert column.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(new_feat_file):\n",
    "        print(\"⚠️ File not found:\", new_feat_file)\n",
    "        return None\n",
    "    df_new = pd.read_csv(new_feat_file, low_memory=False)\n",
    "    \n",
    "    # Ensure all trained features exist\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    print(\"✅ Alerts generated for new data.\")\n",
    "    return df_alert\n",
    "\n",
    "# Example usage:\n",
    "# df_alerts = predict_new_data(FEATURE_FILE_NEW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8e27b-f1c3-486d-8303-a3fbeacf0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Full Pipeline (With Deployment & Monitoring)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality (Step 1)\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "\n",
    "print(\"\\n----- Result Distribution: auth_parsed_large.csv -----\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\n----- Numeric Summary: auth_features_large.csv -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering (Step 3)\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest) - Step 5\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List (Step 6)\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts (Step 7)\n",
    "# ==============================\n",
    "def predict_new_data(feature_file, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    \n",
    "    # Ensure all trained features exist\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "# Run prediction on new data automatically\n",
    "alerts = predict_new_data(FEATURE_FILE_NEW)\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ceb546-a2b3-483b-988b-9a3234ba093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Full Pipeline (Steps 1-9)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "FEEDBACK_FILE = os.path.join(BASE_DIR, 'feedback_labels.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality (Step 1)\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "\n",
    "print(\"\\n----- Result Distribution: auth_parsed_large.csv -----\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\n----- Numeric Summary: auth_features_large.csv -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering (Step 3)\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest) - Step 5\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List (Step 6)\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts (Step 7)\n",
    "# ==============================\n",
    "def predict_new_data(feature_file, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    \n",
    "    # Ensure all trained features exist\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "alerts = predict_new_data(FEATURE_FILE_NEW)\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Feedback Loop & Metrics Dashboard (Steps 8 & 9)\n",
    "# ==============================\n",
    "# Step 8: Feedback Loop\n",
    "if os.path.exists(FEEDBACK_FILE):\n",
    "    feedback_df = pd.read_csv(FEEDBACK_FILE)\n",
    "    for col in ['timestamp','ip','user','alert','true_label']:\n",
    "        if col not in feedback_df.columns:\n",
    "            feedback_df[col] = 'NA'\n",
    "    feedback_df['alert'] = feedback_df['alert'].astype(int)\n",
    "    feedback_df['true_label'] = feedback_df['true_label'].astype(int)\n",
    "    \n",
    "    correct_alerts = ((feedback_df['alert']==1) & (feedback_df['true_label']==1)).sum()\n",
    "    false_positives = ((feedback_df['alert']==1) & (feedback_df['true_label']==0)).sum()\n",
    "    false_negatives = ((feedback_df['alert']==0) & (feedback_df['true_label']==1)).sum()\n",
    "    total_alerts = feedback_df['alert'].sum()\n",
    "    \n",
    "    print(\"\\n----- Feedback Loop Metrics -----\")\n",
    "    print(f\"✅ Correct Alerts: {correct_alerts}\")\n",
    "    print(f\"❌ False Positives: {false_positives}\")\n",
    "    print(f\"❌ False Negatives: {false_negatives}\")\n",
    "    print(f\"Total Alerts Sent: {total_alerts}\")\n",
    "else:\n",
    "    print(\"⚠️ No feedback file found, skipping feedback metrics.\")\n",
    "\n",
    "# Step 9: Dashboard / Metrics Trends\n",
    "if os.path.exists(ALERT_FILE):\n",
    "    df_alerts = pd.read_csv(ALERT_FILE, parse_dates=['timestamp'])\n",
    "    df_alerts['date'] = df_alerts['timestamp'].dt.date\n",
    "    alerts_per_day = df_alerts.groupby('date')['alert'].sum()\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    alerts_per_day.plot(kind='bar')\n",
    "    plt.title(\"Daily Alerts Trend\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Number of Alerts\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Alerts trend plotted.\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts file found, skipping dashboard plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49b82e-78e1-4705-941d-c52136043933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Full Pipeline (With Deployment, Monitoring & Feedback)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "FEEDBACK_FILE = os.path.join(BASE_DIR, 'feedback_labels.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "\n",
    "print(\"\\n----- Result Distribution: auth_parsed_large.csv -----\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\n----- Numeric Summary: auth_features_large.csv -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest)\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 10️⃣ Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 11️⃣ Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "def predict_new_data(feature_file, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "alerts = predict_new_data(FEATURE_FILE_NEW)\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n",
    "\n",
    "# ==============================\n",
    "# 12️⃣ Feedback Loop: Create feedback file and compute metrics\n",
    "# ==============================\n",
    "if not os.path.exists(FEEDBACK_FILE):\n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alerts = pd.read_csv(ALERT_FILE, low_memory=False)\n",
    "        df_alerts['true_label'] = df_alerts['alert']  # placeholder\n",
    "        feedback_cols = ['timestamp','ip','user','alert','true_label']\n",
    "        df_feedback = df_alerts[feedback_cols]\n",
    "        df_feedback.to_csv(FEEDBACK_FILE, index=False)\n",
    "        print(f\"✅ Feedback file created at: {FEEDBACK_FILE}\")\n",
    "    else:\n",
    "        print(\"⚠️ No alerts file found, skipping feedback creation.\")\n",
    "else:\n",
    "    print(\"✅ Feedback file already exists.\")\n",
    "\n",
    "# Optional: Compute simple metrics if feedback exists\n",
    "if os.path.exists(FEEDBACK_FILE):\n",
    "    df_feedback = pd.read_csv(FEEDBACK_FILE, low_memory=False)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(df_feedback['true_label'], df_feedback['alert'])\n",
    "    print(\"\\nConfusion Matrix (Feedback):\\n\", cm)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp+fp) if (tp+fp)>0 else 0\n",
    "    recall = tp / (tp+fn) if (tp+fn)>0 else 0\n",
    "    print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5133458-6a92-4332-a684-bf29d42fdd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Full Pipeline (With Monitoring & Dashboard)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "FEEDBACK_FILE = os.path.join(BASE_DIR, 'feedback.csv')  # Optional\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "\n",
    "print(\"\\n----- Result Distribution: auth_parsed_large.csv -----\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\n----- Numeric Summary: auth_features_large.csv -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest)\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "def predict_new_data(feature_file, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    return df_alert\n",
    "\n",
    "alerts = predict_new_data(FEATURE_FILE_NEW)\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Step 10: Production Metrics & Dashboard\n",
    "# ==============================\n",
    "if os.path.exists(ALERT_FILE):\n",
    "    df_alerts = pd.read_csv(ALERT_FILE, parse_dates=['timestamp'])\n",
    "    alerts_by_day = df_alerts.groupby(df_alerts['timestamp'].dt.date)['alert'].sum()\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    sns.lineplot(x=alerts_by_day.index, y=alerts_by_day.values)\n",
    "    plt.title(\"Daily Alerts Trend\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Number of Alerts\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    if os.path.exists(FEEDBACK_FILE):\n",
    "        df_feedback = pd.read_csv(FEEDBACK_FILE, parse_dates=['timestamp'])\n",
    "        df_merged = pd.merge(df_alerts, df_feedback, on=['timestamp','ip','user'], how='left')\n",
    "        \n",
    "        TP = len(df_merged[(df_merged['alert']==1) & (df_merged['label']==1)])\n",
    "        FP = len(df_merged[(df_merged['alert']==1) & (df_merged['label']==0)])\n",
    "        FN = len(df_merged[(df_merged['alert']==0) & (df_merged['label']==1)])\n",
    "        TN = len(df_merged[(df_merged['alert']==0) & (df_merged['label']==0)])\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        \n",
    "        print(\"\\n----- Feedback Metrics -----\")\n",
    "        print(f\"True Positives: {TP}, False Positives: {FP}\")\n",
    "        print(f\"False Negatives: {FN}, True Negatives: {TN}\")\n",
    "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "        \n",
    "        plt.figure(figsize=(10,4))\n",
    "        sns.histplot(df_alerts['failed_prob'], bins=30, kde=True)\n",
    "        plt.title(\"Distribution of Alert Probabilities\")\n",
    "        plt.xlabel(\"Failed Probability\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5067fc-6f4e-4231-bdbd-cfa568220c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Full Pipeline (With Feature Engineering Module)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import Feature Engineering Functions\n",
    "from scripts.feature_engineering import (\n",
    "    compute_failed_streak,\n",
    "    unique_users_last_5,\n",
    "    add_time_features,\n",
    "    avg_interarrival\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "\n",
    "print(\"\\n----- Result Distribution: auth_parsed_large.csv -----\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\n----- Numeric Summary: auth_features_large.csv -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Time-based features\n",
    "df = add_time_features(df, timestamp_col='timestamp')\n",
    "\n",
    "# Average interarrival\n",
    "df = avg_interarrival(df, timestamp_col='timestamp', ip_col='ip')\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest)\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "def predict_new_data(feature_file, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    \n",
    "    # Ensure all trained features exist\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "# Run prediction on new data automatically\n",
    "alerts = predict_new_data(FEATURE_FILE_NEW)\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc18a0-a253-48b6-a69a-ab2171ed98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_pipeline.py\n",
    "# ==============================\n",
    "# Full Login Anomaly Detection Pipeline (Step 1-7)\n",
    "# ==============================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), 'scripts'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import Feature Engineering Functions\n",
    "from feature_engineering import (\n",
    "    compute_failed_streak,\n",
    "    unique_users_last_5,\n",
    "    add_time_features,\n",
    "    avg_interarrival\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality (Step 1)\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "\n",
    "print(\"\\n----- Result Distribution: auth_parsed_large.csv -----\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\n----- Numeric Summary: auth_features_large.csv -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering (Step 3)\n",
    "# ==============================\n",
    "feat = add_time_features(feat)\n",
    "df = avg_interarrival(df)\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest) - Step 5\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List (Step 6)\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts (Step 7)\n",
    "# ==============================\n",
    "def predict_new_data(feature_file, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    \n",
    "    # Ensure all trained features exist\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "# Run prediction on new data automatically\n",
    "alerts = predict_new_data(FEATURE_FILE_NEW)\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f7f51-a67b-47d3-a340-c20530c3230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# run_pipeline.py\n",
    "# Login Anomaly Detection Full Pipeline\n",
    "# Works in Jupyter or as standalone script\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Add scripts to sys.path (Jupyter & .py compatible)\n",
    "# ==============================\n",
    "try:\n",
    "    BASE_PATH = os.path.dirname(__file__)\n",
    "except NameError:\n",
    "    BASE_PATH = os.getcwd()\n",
    "\n",
    "SCRIPTS_PATH = os.path.join(BASE_PATH, 'scripts')\n",
    "if SCRIPTS_PATH not in sys.path:\n",
    "    sys.path.append(SCRIPTS_PATH)\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Import Feature Engineering Functions\n",
    "# ==============================\n",
    "from feature_engineering import (\n",
    "    compute_failed_streak,\n",
    "    unique_users_last_5,\n",
    "    add_time_features,\n",
    "    avg_interarrival\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Paths Setup\n",
    "# ==============================\n",
    "DATA_DIR = os.path.join(BASE_PATH, 'data')\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(DATA_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(DATA_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(DATA_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(DATA_DIR, 'alerts.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Check Data Quality\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "print(\"\\n----- Result Distribution: auth_parsed_large.csv -----\")\n",
    "print(df['result'].value_counts())\n",
    "print(\"\\n----- Numeric Summary: auth_features_large.csv -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Add time features\n",
    "feat = add_time_features(df, feat)\n",
    "\n",
    "# Compute failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Prepare Feature Matrix & Scaling\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Train Model (RandomForest) & Hyperparameter Tuning\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Features\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "def predict_new_data(feature_file, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    \n",
    "    # Ensure all trained features exist\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "alerts = predict_new_data(FEATURE_FILE_NEW)\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a175f4-1135-4330-8808-dbafcb703f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Full Pipeline\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup (compatible with Jupyter & scripts)\n",
    "# ==============================\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(__file__)  # إذا تم التشغيل كسكربت\n",
    "except NameError:\n",
    "    BASE_DIR = os.path.abspath('.')      # إذا تم التشغيل في Jupyter Notebook\n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "\n",
    "AUTH_PARSED_FILE = os.path.join(DATA_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(DATA_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(DATA_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(DATA_DIR, 'GeoLite2-City.mmdb')\n",
    "\n",
    "MODEL_FILE = os.path.join(DATA_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(DATA_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(DATA_DIR, 'alerts.csv')\n",
    "\n",
    "# التحقق من الملفات الأساسية\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "print(\"✅ All required files are present.\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "\n",
    "print(\"\\n----- Result Distribution: auth_parsed_large.csv -----\")\n",
    "print(df['result'].value_counts())\n",
    "\n",
    "print(\"\\n----- Numeric Summary: auth_features_large.csv -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest + Hyperparameter Tuning)\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "def predict_new_data(feature_file, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "alerts = predict_new_data(FEATURE_FILE_NEW)\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e69a6-9243-4548-a695-553ebcc42184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Full Pipeline\n",
    "# Steps 1️⃣ to 11️⃣ integrated\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "FEEDBACK_FILE = os.path.join(BASE_DIR, 'alerts_feedback.csv')\n",
    "METRICS_FILE = os.path.join(BASE_DIR, 'alerts_metrics.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "print(\"\\n----- Result Distribution -----\")\n",
    "print(df['result'].value_counts())\n",
    "print(\"\\n----- Numeric Summary -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest)\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "def predict_new_data(feature_file=FEATURE_FILE_NEW, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "alerts = predict_new_data()\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Step 8: Feedback Loop\n",
    "# ==============================\n",
    "def update_feedback_matrix(alert_file=ALERT_FILE, feedback_file=FEEDBACK_FILE):\n",
    "    if not os.path.exists(feedback_file):\n",
    "        print(\"⚠️ Feedback file not found, skipping feedback matrix.\")\n",
    "        return None\n",
    "    \n",
    "    alerts_df = pd.read_csv(alert_file)\n",
    "    feedback_df = pd.read_csv(feedback_file)\n",
    "    \n",
    "    merged = pd.merge(alerts_df, feedback_df, on=['timestamp','ip','user'], how='left')\n",
    "    \n",
    "    merged['tp'] = ((merged['alert']==1) & (merged['label']==1)).astype(int)\n",
    "    merged['fp'] = ((merged['alert']==1) & (merged['label']==0)).astype(int)\n",
    "    merged['fn'] = ((merged['alert']==0) & (merged['label']==1)).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'TP': merged['tp'].sum(),\n",
    "        'FP': merged['fp'].sum(),\n",
    "        'FN': merged['fn'].sum(),\n",
    "        'Precision': merged['tp'].sum() / max(merged['tp'].sum()+merged['fp'].sum(),1),\n",
    "        'Recall': merged['tp'].sum() / max(merged['tp'].sum()+merged['fn'].sum(),1)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n✅ Feedback Metrics:\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "feedback_metrics = update_feedback_matrix()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Step 9: Production Metrics\n",
    "# ==============================\n",
    "def record_production_metrics(alert_file=ALERT_FILE, metrics_file=METRICS_FILE):\n",
    "    if not os.path.exists(alert_file):\n",
    "        print(\"⚠️ No alerts to record metrics.\")\n",
    "        return None\n",
    "    \n",
    "    alerts_df = pd.read_csv(alert_file)\n",
    "    total_alerts = len(alerts_df)\n",
    "    total_failed_prob = alerts_df['failed_prob'].sum()\n",
    "    \n",
    "    metrics = {\n",
    "        'total_alerts': total_alerts,\n",
    "        'sum_failed_prob': total_failed_prob\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    if os.path.exists(metrics_file):\n",
    "        metrics_df.to_csv(metrics_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        metrics_df.to_csv(metrics_file, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Production metrics recorded in {metrics_file}\")\n",
    "    return metrics_df\n",
    "\n",
    "prod_metrics = record_production_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ed647e-95c9-4498-97e3-9e317fc21d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Missing Values: auth_parsed_large.csv -----\n",
      "timestamp    0.0\n",
      "ip           0.0\n",
      "user         0.0\n",
      "result       0.0\n",
      "dtype: float64\n",
      "\n",
      "----- Missing Values: auth_features_large.csv -----\n",
      "timestamp       0.0\n",
      "ip              0.0\n",
      "cnt_last_1m     0.0\n",
      "cnt_last_5m     0.0\n",
      "cnt_last_15m    0.0\n",
      "succ_count      0.0\n",
      "fail_count      0.0\n",
      "total_count     0.0\n",
      "fail_rate       0.0\n",
      "event_user      0.0\n",
      "event_result    0.0\n",
      "dtype: float64\n",
      "\n",
      "----- Result Distribution -----\n",
      "result\n",
      "success    821\n",
      "failed     179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "----- Numeric Summary -----\n",
      "                           timestamp  cnt_last_1m  cnt_last_5m  cnt_last_15m  \\\n",
      "count                           1000  1000.000000  1000.000000   1000.000000   \n",
      "mean   2025-10-01 08:19:29.999999744     4.373000    23.804000     73.995000   \n",
      "min              2025-10-01 00:00:00     0.000000     0.000000      0.000000   \n",
      "25%              2025-10-01 04:09:45     2.000000    11.000000     38.000000   \n",
      "50%              2025-10-01 08:19:30     4.000000    23.000000     73.000000   \n",
      "75%              2025-10-01 12:29:15     7.000000    37.000000    112.250000   \n",
      "max              2025-10-01 16:39:00     9.000000    49.000000    149.000000   \n",
      "std                              NaN     2.801735    14.682205     43.159352   \n",
      "\n",
      "        succ_count   fail_count  total_count    fail_rate  \n",
      "count  1000.000000  1000.000000   1000.00000  1000.000000  \n",
      "mean      4.494000     2.035000      7.61700     0.500006  \n",
      "min       0.000000     0.000000      1.00000     0.000786  \n",
      "25%       2.000000     1.000000      4.00000     0.246669  \n",
      "50%       4.000000     2.000000      8.00000     0.484637  \n",
      "75%       7.000000     3.000000     11.00000     0.765519  \n",
      "max       9.000000     4.000000     14.00000     0.996880  \n",
      "std       2.911433     1.430322      4.04508     0.291784  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best hyperparameters: {'n_estimators': 50, 'max_depth': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       247\n",
      "           1       1.00      1.00      1.00       246\n",
      "\n",
      "    accuracy                           1.00       493\n",
      "   macro avg       1.00      1.00      1.00       493\n",
      "weighted avg       1.00      1.00      1.00       493\n",
      "\n",
      "ROC AUC: 1.0\n",
      "PR AUC: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and feature list saved.\n",
      "✅ 1000 alerts generated. Check /home/bakri/projects/login-anomaly/data/alerts.csv\n",
      "⚠️ Feedback file not found, skipping feedback matrix.\n",
      "\n",
      "✅ Production metrics recorded in /home/bakri/projects/login-anomaly/data/alerts_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Full Pipeline\n",
    "# Steps 1️⃣ to 11️⃣ integrated\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "FEEDBACK_FILE = os.path.join(BASE_DIR, 'alerts_feedback.csv')\n",
    "METRICS_FILE = os.path.join(BASE_DIR, 'alerts_metrics.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "print(\"\\n----- Result Distribution -----\")\n",
    "print(df['result'].value_counts())\n",
    "print(\"\\n----- Numeric Summary -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest)\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Evaluate Model\n",
    "# ==============================\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "def predict_new_data(feature_file=FEATURE_FILE_NEW, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "alerts = predict_new_data()\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Step 8: Feedback Loop\n",
    "# ==============================\n",
    "def update_feedback_matrix(alert_file=ALERT_FILE, feedback_file=FEEDBACK_FILE):\n",
    "    if not os.path.exists(feedback_file):\n",
    "        print(\"⚠️ Feedback file not found, skipping feedback matrix.\")\n",
    "        return None\n",
    "    \n",
    "    alerts_df = pd.read_csv(alert_file)\n",
    "    feedback_df = pd.read_csv(feedback_file)\n",
    "    \n",
    "    merged = pd.merge(alerts_df, feedback_df, on=['timestamp','ip','user'], how='left')\n",
    "    \n",
    "    merged['tp'] = ((merged['alert']==1) & (merged['label']==1)).astype(int)\n",
    "    merged['fp'] = ((merged['alert']==1) & (merged['label']==0)).astype(int)\n",
    "    merged['fn'] = ((merged['alert']==0) & (merged['label']==1)).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'TP': merged['tp'].sum(),\n",
    "        'FP': merged['fp'].sum(),\n",
    "        'FN': merged['fn'].sum(),\n",
    "        'Precision': merged['tp'].sum() / max(merged['tp'].sum()+merged['fp'].sum(),1),\n",
    "        'Recall': merged['tp'].sum() / max(merged['tp'].sum()+merged['fn'].sum(),1)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n✅ Feedback Metrics:\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "feedback_metrics = update_feedback_matrix()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Step 9: Production Metrics\n",
    "# ==============================\n",
    "def record_production_metrics(alert_file=ALERT_FILE, metrics_file=METRICS_FILE):\n",
    "    if not os.path.exists(alert_file):\n",
    "        print(\"⚠️ No alerts to record metrics.\")\n",
    "        return None\n",
    "    \n",
    "    alerts_df = pd.read_csv(alert_file)\n",
    "    total_alerts = len(alerts_df)\n",
    "    total_failed_prob = alerts_df['failed_prob'].sum()\n",
    "    \n",
    "    metrics = {\n",
    "        'total_alerts': total_alerts,\n",
    "        'sum_failed_prob': total_failed_prob\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    if os.path.exists(metrics_file):\n",
    "        metrics_df.to_csv(metrics_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        metrics_df.to_csv(metrics_file, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Production metrics recorded in {metrics_file}\")\n",
    "    return metrics_df\n",
    "\n",
    "prod_metrics = record_production_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810af1a4-c81c-4c47-a053-a38c9025d8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1000 alerts generated. Check /home/bakri/projects/login-anomaly/data/alerts.csv\n",
      "⚠️ Feedback file not found, skipping feedback matrix.\n",
      "\n",
      "✅ Production metrics recorded in /home/bakri/projects/login-anomaly/data/alerts_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ==============================\n",
    "# run_pipeline.py\n",
    "# Full Login Anomaly Detection Pipeline\n",
    "# Ready for cron automation\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "FEEDBACK_FILE = os.path.join(BASE_DIR, 'alerts_feedback.csv')\n",
    "METRICS_FILE = os.path.join(BASE_DIR, 'alerts_metrics.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# Helper Functions\n",
    "# ==============================\n",
    "\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "\n",
    "def geoip_lookup(ip, reader, geo_cache):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Target & Feature Engineering\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# Time features\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Avg interarrival\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip, reader, geo_cache) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Scale & Handle Imbalance\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Train/Test Split & Train Model\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "\n",
    "# Save model & scaler\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Predict New Data & Generate Alerts\n",
    "# ==============================\n",
    "def predict_new_data(feature_file=FEATURE_FILE_NEW, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    df_new = pd.read_csv(feature_file)\n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    return df_alert\n",
    "\n",
    "alerts = predict_new_data()\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Feedback Loop\n",
    "# ==============================\n",
    "def update_feedback_matrix(alert_file=ALERT_FILE, feedback_file=FEEDBACK_FILE):\n",
    "    if not os.path.exists(feedback_file):\n",
    "        print(\"⚠️ Feedback file not found, skipping feedback matrix.\")\n",
    "        return None\n",
    "    alerts_df = pd.read_csv(alert_file)\n",
    "    feedback_df = pd.read_csv(feedback_file)\n",
    "    merged = pd.merge(alerts_df, feedback_df, on=['timestamp','ip','user'], how='left')\n",
    "    merged['tp'] = ((merged['alert']==1) & (merged['label']==1)).astype(int)\n",
    "    merged['fp'] = ((merged['alert']==1) & (merged['label']==0)).astype(int)\n",
    "    merged['fn'] = ((merged['alert']==0) & (merged['label']==1)).astype(int)\n",
    "    metrics = {\n",
    "        'TP': merged['tp'].sum(),\n",
    "        'FP': merged['fp'].sum(),\n",
    "        'FN': merged['fn'].sum(),\n",
    "        'Precision': merged['tp'].sum() / max(merged['tp'].sum()+merged['fp'].sum(),1),\n",
    "        'Recall': merged['tp'].sum() / max(merged['tp'].sum()+merged['fn'].sum(),1)\n",
    "    }\n",
    "    print(\"\\n✅ Feedback Metrics:\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    return metrics\n",
    "\n",
    "feedback_metrics = update_feedback_matrix()\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Production Metrics\n",
    "# ==============================\n",
    "def record_production_metrics(alert_file=ALERT_FILE, metrics_file=METRICS_FILE):\n",
    "    if not os.path.exists(alert_file):\n",
    "        print(\"⚠️ No alerts to record metrics.\")\n",
    "        return None\n",
    "    alerts_df = pd.read_csv(alert_file)\n",
    "    total_alerts = len(alerts_df)\n",
    "    total_failed_prob = alerts_df['failed_prob'].sum()\n",
    "    metrics = {'total_alerts': total_alerts, 'sum_failed_prob': total_failed_prob}\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    if os.path.exists(metrics_file):\n",
    "        metrics_df.to_csv(metrics_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        metrics_df.to_csv(metrics_file, index=False)\n",
    "    print(f\"\\n✅ Production metrics recorded in {metrics_file}\")\n",
    "    return metrics_df\n",
    "\n",
    "prod_metrics = record_production_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af5e5f4-c63f-4e33-b2be-2f40aaca64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Missing Values: auth_parsed_large.csv -----\n",
      "timestamp    0.0\n",
      "ip           0.0\n",
      "user         0.0\n",
      "result       0.0\n",
      "dtype: float64\n",
      "\n",
      "----- Missing Values: auth_features_large.csv -----\n",
      "timestamp       0.0\n",
      "ip              0.0\n",
      "cnt_last_1m     0.0\n",
      "cnt_last_5m     0.0\n",
      "cnt_last_15m    0.0\n",
      "succ_count      0.0\n",
      "fail_count      0.0\n",
      "total_count     0.0\n",
      "fail_rate       0.0\n",
      "event_user      0.0\n",
      "event_result    0.0\n",
      "dtype: float64\n",
      "\n",
      "----- Result Distribution -----\n",
      "result\n",
      "success    821\n",
      "failed     179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "----- Numeric Summary -----\n",
      "                           timestamp  cnt_last_1m  cnt_last_5m  cnt_last_15m  \\\n",
      "count                           1000  1000.000000  1000.000000   1000.000000   \n",
      "mean   2025-10-01 08:19:29.999999744     4.373000    23.804000     73.995000   \n",
      "min              2025-10-01 00:00:00     0.000000     0.000000      0.000000   \n",
      "25%              2025-10-01 04:09:45     2.000000    11.000000     38.000000   \n",
      "50%              2025-10-01 08:19:30     4.000000    23.000000     73.000000   \n",
      "75%              2025-10-01 12:29:15     7.000000    37.000000    112.250000   \n",
      "max              2025-10-01 16:39:00     9.000000    49.000000    149.000000   \n",
      "std                              NaN     2.801735    14.682205     43.159352   \n",
      "\n",
      "        succ_count   fail_count  total_count    fail_rate  \n",
      "count  1000.000000  1000.000000   1000.00000  1000.000000  \n",
      "mean      4.494000     2.035000      7.61700     0.500006  \n",
      "min       0.000000     0.000000      1.00000     0.000786  \n",
      "25%       2.000000     1.000000      4.00000     0.246669  \n",
      "50%       4.000000     2.000000      8.00000     0.484637  \n",
      "75%       7.000000     3.000000     11.00000     0.765519  \n",
      "max       9.000000     4.000000     14.00000     0.996880  \n",
      "std       2.911433     1.430322      4.04508     0.291784  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best hyperparameters: {'n_estimators': 200, 'max_depth': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       247\n",
      "           1       1.00      1.00      1.00       246\n",
      "\n",
      "    accuracy                           1.00       493\n",
      "   macro avg       1.00      1.00      1.00       493\n",
      "weighted avg       1.00      1.00      1.00       493\n",
      "\n",
      "ROC AUC: 1.0\n",
      "PR AUC: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and feature list saved.\n",
      "✅ 1000 alerts generated. Check /home/bakri/projects/login-anomaly/data/alerts.csv\n",
      "✅ Feedback file updated: /home/bakri/projects/login-anomaly/data/alerts_feedback.csv\n",
      "✅ Production metrics recorded in /home/bakri/projects/login-anomaly/data/alerts_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Login Anomaly Detection Full Pipeline (With Feedback Loop)\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Paths Setup\n",
    "# ==============================\n",
    "BASE_DIR = '/home/bakri/projects/login-anomaly/data'\n",
    "AUTH_PARSED_FILE = os.path.join(BASE_DIR, 'auth_parsed_large.csv')\n",
    "AUTH_FEATURES_FILE = os.path.join(BASE_DIR, 'auth_features_large.csv')\n",
    "FEATURE_FILE_NEW = os.path.join(BASE_DIR, 'auth_features_new.csv')\n",
    "GEOIP_FILE = os.path.join(BASE_DIR, 'GeoLite2-City.mmdb')\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'random_forest_model_final.joblib')\n",
    "FEATURE_COLS_FILE = os.path.join(BASE_DIR, 'feature_columns.joblib')\n",
    "ALERT_FILE = os.path.join(BASE_DIR, 'alerts.csv')\n",
    "FEEDBACK_FILE = os.path.join(BASE_DIR, 'alerts_feedback.csv')\n",
    "\n",
    "for f in [AUTH_PARSED_FILE, AUTH_FEATURES_FILE, GEOIP_FILE]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {f}\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Load Raw Data\n",
    "# ==============================\n",
    "df = pd.read_csv(AUTH_PARSED_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "feat = pd.read_csv(AUTH_FEATURES_FILE, parse_dates=['timestamp'], low_memory=False)\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Check Data Quality\n",
    "# ==============================\n",
    "print(\"\\n----- Missing Values: auth_parsed_large.csv -----\")\n",
    "print(df.isnull().mean() * 100)\n",
    "print(\"\\n----- Missing Values: auth_features_large.csv -----\")\n",
    "print(feat.isnull().mean() * 100)\n",
    "print(\"\\n----- Result Distribution -----\")\n",
    "print(df['result'].value_counts())\n",
    "print(\"\\n----- Numeric Summary -----\")\n",
    "print(feat.describe())\n",
    "\n",
    "# Optional plots\n",
    "numeric_cols = feat.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=feat[numeric_cols])\n",
    "plt.title(\"Boxplot of Numeric Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "feat[numeric_cols].hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Histograms of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Prepare Target\n",
    "# ==============================\n",
    "df['result_bin'] = df['result'].apply(lambda x: 1 if x=='success' else 0)\n",
    "y = df['result_bin'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ==============================\n",
    "# Hour & night\n",
    "feat['hour'] = df['timestamp'].dt.hour\n",
    "feat['is_night'] = feat['hour'].isin([0,1,2,3,4,5,23]).astype(int)\n",
    "\n",
    "# Average interarrival per IP\n",
    "df_sorted = df.sort_values(['ip','timestamp'])\n",
    "df_sorted['time_diff'] = df_sorted.groupby('ip')['timestamp'].diff().dt.total_seconds()\n",
    "feat['avg_interarrival'] = df_sorted.groupby('ip')['time_diff'].transform('mean').fillna(0)\n",
    "\n",
    "# Failed streak\n",
    "df['failed_flag'] = (df['result']=='failed').astype(int)\n",
    "def compute_failed_streak(x):\n",
    "    return x.groupby((x==0).cumsum()).cumsum()\n",
    "df['failed_streak'] = df.groupby('user')['failed_flag'].transform(compute_failed_streak)\n",
    "feat['failed_streak'] = df['failed_streak'].fillna(0).astype(int)\n",
    "\n",
    "# Unique users last 5 attempts\n",
    "def unique_users_last_5(series):\n",
    "    arr = series.tolist()\n",
    "    counts = []\n",
    "    dq = deque(maxlen=5)\n",
    "    for user in arr:\n",
    "        dq.append(user)\n",
    "        counts.append(len(set(dq)))\n",
    "    return pd.Series(counts, index=series.index)\n",
    "df['unique_users_last_5'] = df.groupby('ip')['user'].transform(unique_users_last_5)\n",
    "feat['unique_users_last_5'] = df['unique_users_last_5'].fillna(0).astype(int)\n",
    "\n",
    "# GeoIP Features\n",
    "reader = geoip2.database.Reader(GEOIP_FILE)\n",
    "geo_cache = {}\n",
    "def geoip_lookup(ip):\n",
    "    if ip in geo_cache:\n",
    "        return geo_cache[ip]\n",
    "    try:\n",
    "        r = reader.city(ip)\n",
    "        country = r.country.iso_code\n",
    "        city = r.city.name\n",
    "        lat = r.location.latitude\n",
    "        lon = r.location.longitude\n",
    "    except:\n",
    "        country, city, lat, lon = 'NA','NA',0,0\n",
    "    geo_cache[ip] = pd.Series([country, city, lat, lon])\n",
    "    return geo_cache[ip]\n",
    "\n",
    "unique_ips = df['ip'].unique()\n",
    "geo_results = Parallel(n_jobs=-1)(delayed(geoip_lookup)(ip) for ip in unique_ips)\n",
    "geo_df = pd.DataFrame(geo_results, index=unique_ips, columns=['geo_country','city','lat','lon'])\n",
    "df = df.join(geo_df, on='ip')\n",
    "feat['lat'] = df['lat'].fillna(0)\n",
    "feat['lon'] = df['lon'].fillna(0)\n",
    "feat['geo_country'] = df['geo_country']\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Prepare Feature Matrix\n",
    "# ==============================\n",
    "numeric_cols = feat.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'timestamp' in numeric_cols: numeric_cols.remove('timestamp')\n",
    "X = feat[numeric_cols].fillna(0)\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ Scale Features\n",
    "# ==============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Handle Imbalance\n",
    "# ==============================\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Train/Test Split\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 9️⃣ Train Model (RandomForest)\n",
    "# ==============================\n",
    "param_dist = {'n_estimators':[50,100,200],'max_depth':[5,10,20,None]}\n",
    "rs = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "                        param_distributions=param_dist, n_iter=10, cv=3, scoring='recall')\n",
    "rs.fit(X_train, y_train)\n",
    "clf = rs.best_estimator_\n",
    "print(\"✅ Best hyperparameters:\", rs.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "print(\"PR AUC:\", auc(rec, prec))\n",
    "\n",
    "# Feature Importance\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=importances[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Save Model & Feature List\n",
    "# ==============================\n",
    "joblib.dump({'model': clf, 'scaler': scaler}, MODEL_FILE)\n",
    "joblib.dump(X.columns.tolist(), FEATURE_COLS_FILE)\n",
    "print(\"✅ Model and feature list saved.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Predict on New Data & Generate Alerts\n",
    "# ==============================\n",
    "def predict_new_data(feature_file, threshold=0.5):\n",
    "    if not os.path.exists(feature_file):\n",
    "        print(\"⚠️ Feature file not found, skipping alerts.\")\n",
    "        return None\n",
    "    \n",
    "    df_new = pd.read_csv(feature_file, low_memory=False)\n",
    "    \n",
    "    for c in X.columns:\n",
    "        if c not in df_new.columns:\n",
    "            df_new[c] = 0\n",
    "    \n",
    "    X_new_scaled = scaler.transform(df_new[X.columns])\n",
    "    df_new['failed_prob'] = 1 - clf.predict_proba(X_new_scaled)[:,1]\n",
    "    df_new['alert'] = (df_new['failed_prob'] >= threshold).astype(int)\n",
    "    \n",
    "    for col in ['timestamp','ip','user']:\n",
    "        if col not in df_new.columns:\n",
    "            df_new[col] = 'NA'\n",
    "    \n",
    "    df_alert = df_new[['timestamp','ip','user','failed_prob','alert']]\n",
    "    \n",
    "    if os.path.exists(ALERT_FILE):\n",
    "        df_alert.to_csv(ALERT_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_alert.to_csv(ALERT_FILE, index=False)\n",
    "    \n",
    "    return df_alert\n",
    "\n",
    "alerts = predict_new_data(FEATURE_FILE_NEW)\n",
    "if alerts is not None and not alerts.empty:\n",
    "    print(f\"✅ {len(alerts)} alerts generated. Check {ALERT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts generated.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Update Feedback Loop\n",
    "# ==============================\n",
    "if alerts is not None and not alerts.empty:\n",
    "    if os.path.exists(FEEDBACK_FILE):\n",
    "        df_feedback = pd.read_csv(FEEDBACK_FILE, parse_dates=['timestamp'])\n",
    "        df_to_append = alerts[~alerts.set_index(['timestamp','ip','user']).index.isin(\n",
    "            df_feedback.set_index(['timestamp','ip','user']).index\n",
    "        )]\n",
    "        if not df_to_append.empty:\n",
    "            df_to_append['feedback'] = 'NA'\n",
    "            df_feedback = pd.concat([df_feedback, df_to_append], ignore_index=True)\n",
    "        df_feedback.to_csv(FEEDBACK_FILE, index=False)\n",
    "    else:\n",
    "        alerts['feedback'] = 'NA'\n",
    "        alerts.to_csv(FEEDBACK_FILE, index=False)\n",
    "    print(f\"✅ Feedback file updated: {FEEDBACK_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No alerts to update feedback.\")\n",
    "\n",
    "# ==============================\n",
    "# 🔹 Optional: record production metrics\n",
    "# ==============================\n",
    "metrics_file = os.path.join(BASE_DIR, 'alerts_metrics.csv')\n",
    "metrics = {\n",
    "    'timestamp': pd.Timestamp.now(),\n",
    "    'num_alerts': len(alerts) if alerts is not None else 0\n",
    "}\n",
    "df_metrics = pd.DataFrame([metrics])\n",
    "if os.path.exists(metrics_file):\n",
    "    df_metrics.to_csv(metrics_file, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df_metrics.to_csv(metrics_file, index=False)\n",
    "print(f\"✅ Production metrics recorded in {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b491537-fcc3-424a-b26b-51222f026773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
